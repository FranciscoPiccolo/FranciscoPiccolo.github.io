---
title: ""
date: 2020-02-07
output:
  html_document:
    css: style.css
    df_print: paged
    theme: cerulean
    highlight: zenburn
    fig_width: 8.5
    fig_height: 4
    code_folding: hide
---

```{r,echo=F,include=F,warning=F,message=F}
library(tidyverse)
library(DAAG)
library(Amelia)
library(ggthemes)
library(pander)

theme_set(theme_bw())

theme_graph <- function(){
  theme(
    # background color
    panel.background = element_rect(fill = "white"),
    panel.grid.minor = element_blank(),
    plot.title = element_text(size = 16),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(face = "italic", size = 9),
    axis.text = element_text(size = 9),
    axis.title = element_text(face = "italic", size = 9),
    text = element_text(family = "Times New Roman"),
    strip.background = element_rect(fill = "grey"),
    strip.text = element_text(face = "bold"),
    legend.title = element_blank(),
    legend.position = "bottom"
  )
}

graph_color <- "#006666"
```

# {.tabset .tabset-fade .tabset-pills}

## **Overview**

Este livro faz parte de uma coleção da Universidade de Cambridge - *Cambridge Series in Statistics and Probabilistic Mathematics*, onde além desta públicação, constam outros 19 livros que exploram áreas da estatística. 

O livro foi publicado (2ª ed) em 2006 e talvez seja por isso que os gráficos não façam uso da biblioteca ggplot e a manipulação dos dados não utilize dplyr ou algo do universo tidyverse. 

A didática do livro é muito boa, os autores conseguiram explicar os conceitos estatísticos com clareza e os códigos usados foram bem comentados para facilitar o entendimento. Para a aplicação dos exemplos e resolução dos exercícios, foram usadas bibliotecas que contêm os data sets necessários para cada situação, o que facilitou bastante, pois não foi preciso copiar a tabela do livro e salvar em um arquivo para depois importar para o R.

Apenas senti falta da resolução dos exercícios propostos, ou de pelo menos alguns deles. Por mais que haja exemplos ao longo do capítulo, na hora de resolver o exercício proposto a pessoa pode se deparar com outro cenário não explorado nos exemplos.

Após 2 meses (sem muita pressa), consegui finalizar este livro. Alguns capítulos estavam em um (ou vários) patamares acima do meu nível de conhecimento, por isso não consegui absorver o conteúdo o suficiente para comentá-lo aqui. Por isso, neste post vou falar resumidamente sobre os capítulos que consegui entender e colocar as respostas dos exercícios que resolvi.

```{r pressure, echo=FALSE,fig.align='center',fig.cap="Capa do livro", out.width="200px", out.height="250px"}
knitr::include_graphics("C:/Users/francisco.piccolo/Desktop/R/franciscopiccolo.github.io/images/2020-02-07_data analysis with R.png")
```

## **Cap. 1**

### **A brief introduction to R**

### **Resumo**

Conseguir absorver o conteúdo deste livro usando uma ferramenta estatística de ponta e gratuita, com este objetivo em mente o primeiro capítulo apresenta o R. Uma ferramenta desenvolvida por uma comunidade engajada que é uma mão na roda para a modelagem estatística e análise de dados. 

Neste capítulo, é fornecido uma visão geral do R, visando deixar o leitor familiarizado com suas funcionalidades. Além disso, tem-se a explicação dos tipos de dados usados na ferramenta (e.g. factors, vectors, data.frames, list e strings), formas de se plotar gráficos (incluindo algumas dicas sobre como se apresentar os dados nos gráficos), instalação e requisição de pacotes no ambiente, modos de acesso à ajuda do sistema (documentação dos pacotes), construção de funções (de maneira bem resumida) e importação de arquivos para o sistema.

### **Exercícios**

### **3) Aplique a função str() no data set *possum* do pacote DAAG e verifique se há valores nulos presentes.**

```{r, echo=T, include=T, message=F,warning=F}
DAAG::possum %>% str()
```

```{r, echo=T, include=T, message=F,warning=F}
DAAG::possum %>% Amelia::missmap(main = "Valores nulos vs não nulos")
```

### **4) Verifique no data set *ais* do pacote DAAG se há desequilíbrio entre homens e mulheres nos esportes analisados.**

```{r, echo=T, include=T, message=F,warning=F}
DAAG::ais %>% 
  group_by(sport,sex) %>% 
  summarise(qty = n()) %>% 
  tidyr::spread(sex,qty,fill = 0) %>% 
  mutate(m_proportion = round(m/(f+m),digits = 2)) %>% 
  ungroup() %>% 
  mutate(sport = fct_reorder(sport, m_proportion)) %>% 
  ggplot(aes(x = sport, y = m_proportion))+
  geom_col(fill = graph_color)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = "Sports balance btw men and women participation",
       x = "",
       y = "Male % participation")+
  coord_flip()
```

### **5) Verifique no data set *rainforest* os valores nulos.**

```{r, echo=T, include=T, message=F,warning=F}
DAAG::rainforest %>% 
  Amelia::missmap(main = "Valores nulos vs não nulos")
```

## **2**

### **Styles of data analysis**

### **Resumo**

A análise exploratória de dados é uma atividade do dia a dia de muitos profissionais (a cada dia esta habilidade se torna mais importante) e o capítulo 2 irá abordar esta técnica, apresentando uma conceituação para o termo e mostrando formas de se realizar uma análise exploratória com rigor técnico.

Neste capítulo, são explorados os principais gráficos que são usados na atividade de análise exploratória, sendo eles o histograma (para validação de normalidade), o box plot e o gráfico de dispersão. A ideia é apresentar técnicas que permitam o pesquisador / analista encontrar padrões nos dados para que se possa desenvolver um modelo estatístico em seguida.

Também é tratado sobre outliers e dados distribuídos de maneira não normal (e.g. com caldas alongadas ) e como lidar com estes dados, seja retirando os outliers ou realizando transformação em log para normalizar a distribuição.

### **Exercícios**

### **1) Apresente a distribuição de idade para cada combinação de *site* e *sex* do data set *possum* do pacote DAAG.**

```{r, echo=T, include=T, message=F,warning=F}
DAAG::possum %>%
  ggplot(aes(x = age))+
  geom_density(fill = graph_color, alpha = .9)+
  theme_graph()+
  labs(title = "Age distribution",
       subtitle = "Site and Sex segmentation",
       x = "Age",
       y = "")+
  facet_grid(site~sex,scales = "free")
```

### **3) Plote um histograma para o campo *earchonch* do data set *possum*. A distribuição deverá parecer bimodal (dois picos). Este é um exemplo de clusterização dos dados, neste caso por conta de que o data set contém informação de homens e mulheres. Plote um gráfico de box plot para homens e mulheres para o campo *earconch*.**

```{r, echo=T, include=T, message=F,warning=F}
DAAG::possum %>% 
  ggplot(aes(x = earconch))+
  geom_density(fill = graph_color, alpha = .8)+
  theme_graph()+
  labs(title = "Earconch distribution",
       x = "",
       y = "")

DAAG::possum %>% 
  ggplot(aes(y = earconch, fill = sex))+
  geom_boxplot()+
  theme_graph()+
  labs(title = "Earconch box plot distribution",
       x = "",
       y = "")
```

### **4) Usando o data set *ais* do pacote DAAG, crie gráficos que mostre como os valores de medidas hematológicas (quantidade de células vermelhas, concentração de hemoglobina, hematrócito, células brancas e concentração de plasma) variam por esporte e gênero do atleta.**

```{r, echo=T, include=T, message=F,warning=F}
DAAG::ais %>% 
  mutate(red_cell_count = rcc,
         white_cell_count = wcc,
         hematrocit_percent = hc,
         hemaglobin_concentration = hg,
         plasma_ferritins = ferr,
         body_mass_index = bmi
  ) %>% 
  select(sport,
         sex,
         red_cell_count,
         white_cell_count,
         hematrocit_percent,
         hemaglobin_concentration,
         plasma_ferritins,
         body_mass_index) %>%
  filter(sex == "m") %>% 
  tidyr::gather(key = "campo",value = "valor", 3:8) %>% 
  mutate(sport = fct_reorder(sport, valor, .desc = F)) %>% 
  ggplot(aes(y = valor, fill = sport))+
  geom_boxplot(alpha = .6)+
  theme_graph()+
  labs(
    title = "Metrics variations accross sport and genre",
    subtitle = "Genre = Male",
    x = "",
    y = ""
  )+
  facet_wrap(~campo,scales = "free")
```

```{r, echo=F, include=T, message=F,warning=F}
DAAG::ais %>% 
  mutate(red_cell_count = rcc,
         white_cell_count = wcc,
         hematrocit_percent = hc,
         hemaglobin_concentration = hg,
         plasma_ferritins = ferr,
         body_mass_index = bmi
  ) %>% 
  select(sport,
         sex,
         red_cell_count,
         white_cell_count,
         hematrocit_percent,
         hemaglobin_concentration,
         plasma_ferritins,
         body_mass_index) %>%
  filter(sex == "f") %>% 
  tidyr::gather(key = "campo",value = "valor", 3:8) %>% 
  mutate(sport = fct_reorder(sport, valor, .desc = F)) %>% 
  ggplot(aes(y = valor, fill = sport))+
  geom_boxplot(alpha = .6)+
  theme_graph()+
  labs(
    title = "Metrics variations accross sport and genre",
    subtitle = "Genre = Female",
    x = "",
    y = ""
  )+
  facet_wrap(~campo,scales = "free")
```

## **3**

### **Statistical models**

### **Resumo**

No capítulo 3, ocorre uma breve explicação sobre o que são e para que servem modelos estatísticos. É feita a explicação sobre os *resíduos*, métodos para escolha de modelos, distribuições dos dados (Bernoulli, Binomial, Poisson e Normal), utilização de números aleatórios na construção de modelos, técnicas de amostragem e principais premissas adotadas na construção de modelos (e.g. normalidade dos dados, amostragem aleatória).

Para a premissa de normalidade dos dados, os autores indicam técnicas para validar isso. A principal é o histograma, porém necessitando ser reforçada pelo qq-plot e também por testes formais (e.g. Jarque-Beta, Anderson-Darling e Shapiro-Wilk).

### **Exercícios**

### **2) Use *y <- rnorm(100)* para gerar 100 números aleatórios de uma distribuição normal (mean = 0 e sd = 1). Calcule a média e o desvio padrão de y. Em seguida, crie um loop e repita esta função 25 vezes e depois insira as 25 médias em uma variável e depois calcule a média e o desvio padrão desta variável.**

```{r, echo=T, include=T, message=F,warning=F}
y <- rnorm(100)

tibble(mean = mean(y),
           sd = sd(y))
```

```{r, echo=T, include=T, message=F,warning=F}
# Criando um vetor com 25 entradas vazias
saida <- rep(NA,25)

# Criando um loop para gerar no vetor vazio as 25 amostragens
for(i in 1:25){
  saida[i] <- data.frame(
    mean(rnorm(100))
  )
}

# Criando um data.frame com o resultado acima
df <- data.frame(
  a = matrix(unlist(saida),nrow = 25,byrow = F),stringsAsFactors = F)

tibble(mean = mean(df[,1]),
           sd = sd(df[,1]))
```

### **3) Crie uma função para gerar o resultado do exercício 2. Rode esta função várias vezes e plote o resultado em um gráfico de densidade.**

```{r, echo=T, include=T, message=F,warning=F}
# Criando a função, x = repetições, y = amostras aleatórias
fun <- function(x,y){
  saida <- rep(NA,x)
  for(i in 1:x){
    saida[i] <- data.frame(
      round(mean(rnorm(y)),digits = 2)
    )
  }
  df <- data.frame(
    a = matrix(unlist(saida),nrow = x,byrow = T),stringsAsFactors = F)
}

# Rodando a função, 25 repetições com 100 amostras cada
fun(25,100) %>% 
  ggplot(aes(x = a))+
  geom_density(fill = graph_color, alpha = .4)+
  theme_graph()
```

### **5) Plote uma matriz 3 x 4, apresentando em cada célula um histograma baseado em uma distribuição normal (média = 100 e desvio padrão = 10). Na 1º linha, utilize uma amostra com n = 10, na 2ª n = 100 e na 3ª n = 1000.**

```{r, echo=F, include=T, message=F,warning=F}
g_1000_1 <- 
  ggplot(data = data.frame(a = (rnorm(n = 1000,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_1000_2 <- 
  ggplot(data = data.frame(a = (rnorm(n = 1000,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_1000_3 <- 
  ggplot(data = data.frame(a = (rnorm(n = 1000,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_1000_4 <- 
  ggplot(data = data.frame(a = (rnorm(n = 1000,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_100_1 <- 
  ggplot(data = data.frame(a = (rnorm(n = 100,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_100_2 <- 
  ggplot(data = data.frame(a = (rnorm(n = 100,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_100_3 <- 
  ggplot(data = data.frame(a = (rnorm(n = 100,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_100_4 <- 
  ggplot(data = data.frame(a = (rnorm(n = 100,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_10_1 <- 
  ggplot(data = data.frame(a = (rnorm(n = 10,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_10_2 <- 
  ggplot(data = data.frame(a = (rnorm(n = 10,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_10_3 <- 
  ggplot(data = data.frame(a = (rnorm(n = 10,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

g_10_4 <- 
  ggplot(data = data.frame(a = (rnorm(n = 10,mean = 100,sd = 10))))+
  geom_histogram(aes(x = a), fill = graph_color, alpha = .5)+
  theme_graph()+
  labs(x = "",
       y = "")

gridExtra::grid.arrange(g_1000_1,g_1000_2,g_1000_3,g_1000_4,
                        g_100_1,g_100_2,g_100_3,g_100_4,
                        g_10_1,g_10_2,g_10_3,g_10_4,
                        nrow = 3, ncol = 4)
```

### **7) Use a função de distribuição exponencial (dexp,pexp,qexp,rexp) para calcular a probabilidade de que em 21 dias alguma pessoa sofrerá um acidente de trânsito em uma dado cruzamento, dado que o intervalo entre os acidentes podem ser modelados por uma distribuição exponencial com taxa de 0.05 acidentes por dia.**

```{r, echo=T, include=T, message=F,warning=F}
pexp(q = 21,rate = 0.05) # usando a fórmula pronta do R
```

## **4**

### **An introduction to formal inference**

### **Resumo**

Inferência é um pilar importante da estatística, visto que muitas vezes não se pode acessar os dados de uma população toda para se construir um modelo ou gerar conclusões, por questões de custo e tempo. Desta forma, a capacidade de se fazer inferências com base em uma amostra da população é indispensável no processo científico. 

Com esta necessidade em mente os autores desenvolvem o capítulo 4, buscando mostrar os princípios do processo de inferência estatística, apresentando os conceitos básicos para se realizá-la corretamente.

Dentre as explicações e temas deste capítulo, as que merecem mais destaque são os parâmetros populacionais, erro padrão, teorema do limite central e por fim, dando sentido ao aspecto formal da inferência, é apresentada as ideias centrais dos testes de hipótese e do intervalo de confiança.

Ao explicarem sobre a construção dos intervalos de confiança, os autores comentaram sobre as características da distribuição **t** e fizeram um comparativo com a distribuição **normal**, visto que são distribuições parecidas. Além disso, o capítulo traz também uma explicação sobre o valor-p e alguns comentários adicionais sobre as diferenças entre intervalos de confiança e teste de hipótese.

Os autores indicam que há um movimento na direção de intervalos de confiança em detrimento do teste de hipótese nas atividades de inferência, por conta da simplicidade dos intervalos e do mau uso do teste de hipótese.

### **Exercícios**

### **1) Usando o data set *nsw74demo* do pacote DAAG, determine com 95% de confiança o intervalo dos seguintes parâmetros:**

### **(i) Renda média dos grupos de controle e teste em 1974**

A ideia por traz destas questões é que o data set apresenta uma amostra de dados referente a cada grupo. Com base nestas amostras é preciso inferir qual o valor do parâmetro populacional (no caso a média de cada grupo), com um nível de confiança de 95%. Desta forma, será usada uma amostra para inferir o valor populacional.

Antes de fazer este cálculo, é interessante dar uma olhada na distribuição dos dados. Abaixo encontra-se um histograma com a distribuição de renda dos dois grupos em 1974. É recomendado usar escala logarítma (com base 10) na distribuição de renda, pois geralmente possui cauda alongada.

```{r, echo=T, include=T, message=F,warning=F}
DAAG::nsw74demo %>%
  mutate(group = ifelse(trt == 0,"control","teste")) %>% 
  select(group,
         re74) %>% 
  ggplot(aes(x = re74, fill = group))+
  geom_histogram(alpha = .4)+
  scale_x_log10(label = scales::comma)+
  theme_graph()+
  labs(title = "Income distribution between groups (1974)",
       x = "Income, log10 scale",
       y = "")
```

Além da distribuição, é importante ver o desvio padrão e a média das amostras coletadas. A tabela abaixo resume estes parâmetros amostrais.

```{r, echo=T, include=T, message=F,warning=F}
DAAG::nsw74demo %>%
  select(trt,
         re74) %>% 
  filter(re74 != 0) %>% 
  group_by(trt) %>% 
  summarise(mean = round(mean(re74),digits = 2),
            sd = round(sd(re74),digits = 2))
```

Após entender a distribuição dos dados amostrais e seus parâmetros (média e desvio padrão), pode-se usá-los para inferir sobre a média e desvio padrão populacional, usando a variável *standard error of the mean (SEM)* e a distribuição **t**.

A variável *SEM* será obtida através da fórmula $SEM = \frac{s}{\sqrt{n}}$, onde *s* será o desvio padrão da amostra e *n* será o tamanho da amostra. Seu valor irá indicar uma estimativa da variabilidade da média amostral, ou seja, se caso fossem retiradas várias amostras de uma população e realiza-se o cálculo da média de cada amostra e depois fosse calculado o desvio padrão desta médias, o resultado seria o valor de *SEM*.

Tendo uma estimativa da variabilidade da média amostral, se faz necessário usá-la junto com a distribuição **t** para criar um intervalor de confiança para a média populacional. Porém, é importante saber o motivo de se usar a distribuição **t** ao invés da distribuição **normal**. Isso ocorre por conta de que não foi fornecido o Desvio Padrão da população, mas sim o Desvio Padrão da amostra, em que foi usado no cálculo de **SEM**. Para o exercício em questão, será usado o valor crítico de **t** (obtido na distribuição **t**), dado o nível de confiança (95%) e os graus de liberade de cada grupo (n-1). 

Em posse do valor de **SEM** e do valor crítico **t** (a.k.a *t.value*), pode-se gerar os limites inferiores e superiores através da fórmula: $limites = (\bar{x} +/- SEM) t.value$. A tabela abaixo realiza estes cálculos e gera os limites de cada grupo.

```{r, echo=T, include=T, message=F,warning=F}
DAAG::nsw74demo %>%
  select(trt,
         re74) %>% 
  filter(re74 != 0) %>% 
  group_by(trt) %>% 
  summarise(mean = round(mean(re74),digits = 4),
            sd = round(sd(re74),digits = 4),
            n = n(),
            sqrt_n = n()^0.5,
            sem = round(sd/sqrt_n,digits = 4),
            t_value = qt(0.975,n()-1),
            lower_limit = mean - (sem*t_value),
            upper_limit = mean + (sem*t_value))
```

Com base na tabela acima pode-se inferir, com grau de confiança de 95%, que a renda média populacional do grupo de controle fica entre 6.256 e 10.600 e para o grupo de teste fica entre 5.334 e 9.025. 

Obs: Importante notar que esta confiança de 95% não indica que há 95% de chance ou probabilidade de que a média populacional esteja neste intervalo (até por que não sabemos qual o valor desta média). Porém a confiança é em relação ao processo de geração dos intervalos e esta confiança indica que, se caso forem gerados diversos intervalos (e.g. 100), 95% deles conteriam o verdadeiro parâmetro populacional, que neste caso é a média da população.

### **(ii) Renda média dos grupos de controle e teste em 1975**

Esta questão pede o mesmo cálculo acima, porém para os dados de 1975. A conclusão seguirá a mesma lógica da conclusão anterior.

```{r, echo=T, include=T, message=F,warning=F}
DAAG::nsw74demo %>%
  select(trt,
         re75) %>% 
  filter(re75 != 0) %>% 
  group_by(trt) %>% 
  summarise(mean = round(mean(re75),digits = 2),
            sd = round(sd(re75),digits = 2),
            n = n(),
            sqrt_n = n()^0.5,
            sem = round(sd/sqrt_n,digits = 2),
            t_value = qt(0.975,n()-1),
            lower_limit = mean - (sem*t_value),
            upper_limit = mean + (sem*t_value))
```

### **(iii) Renda média dos grupos de controle e teste em 1978**

Este questão também pede uma análise igual aos itens (i) e (ii), portanto, basta replicar os cálculos acima, mudando apenas de **re75** para **re78**. Veja que para o grupo de teste, a renda média estimada aumentou. Desta forma, o estudo feito parece poder concluir que a atividade realizada com os trabalhadores no grupo de teste é efetiva no aumento salarial após um certo período de tempo. O [artigo](https://www.uh.edu/~adkugler/Dehejia&Wahba_JASA.pdf) que realiza o estudo explica melhor o que foi feito no grupo de teste e corrobora a conclusão.

```{r, echo=T, include=T, message=F,warning=F}
DAAG::nsw74demo %>%
  select(trt,
         re78) %>% 
  filter(re78 != 0) %>% 
  group_by(trt) %>% 
  summarise(mean = round(mean(re78),digits = 2),
            sd = round(sd(re78),digits = 2),
            n = n(),
            sqrt_n = n()^0.5,
            sem = round(sd/sqrt_n,digits = 2),
            t_value = qt(0.975,n()-1),
            lower_limit = mean - (sem*t_value),
            upper_limit = mean + (sem*t_value))
```

Veja que para os dados de 1978 os intervalos se inverteram entre os grupos. Agora o intervalo do grupo de controle está mais baixo que o intervalo do grupo de teste. Talves fosse esta a ideia do exercício, mostrar que o grupo de teste conseguiu aumentar sua renda média ao longo do tratamento realizado.

### **(iv) Diferença de renda entre o grupo de controle e teste para o ano de 1978**

Para esta questão, pede-se uma estimativa com nível de confiança de 95% para a diferença de renda entre os grupos de controle e teste, para o ano de 1978. De acordo com o livro, a fórmula para inferir sobre esta diferença é dada por:

$SED = \sqrt{SEM_1^2 + SEM_2^2}$

Onde:

SED = *standard error of the difference*

$SEM_1$ = *standard error of the mean* para o grupo 1 (controle, trt = 0)

$SEM_2$ = *standard error of the mean* para o grupo 2 (teste, trt = 1)

Usando os valores da tabela anterior, que calculou o SEM para cada grupo, basta inserir na fórmula e analisar se o resultado faz sentido.

$SED = \sqrt{415^2+680^2}$

$SED = \sqrt{172.225+462.400}$

$SED = 796,6$

Conclusão: A diferença de renda estimada entre os grupos, com 95% de confiança, é de 796,6 (dólares provavelmente).

### **2) Plote um gráfico que mostre, para graus de liberdade de 1 até 100, a mudança no valor da estatística t, com nível de confiança de 95%. Compare dois gráficos com esta visualização, onde um apresente os valores dos eixos sem transformação e o outro transforme o eixo x em log(estatística-t) e y em log(graus de liberdade). Verifique qual gráfico apresenta melhor a mudança no valor da estatística t por conta da mudança nos graus de liberdade.**

```{r, echo=T, include=T, message=F,warning=F}
t_value <- data.frame(
  df = seq(1,100,by = 1)
)

graph_1 <- t_value %>% 
  mutate(
    t_value = qt(0.975,df = row_number())
  ) %>% 
  ggplot(aes(x = df,y = t_value,group = 1))+
  geom_line(color = graph_color)+
  scale_x_log10()+
  scale_y_log10()+
  theme_graph()+
  labs(title = "Log transformation applied",
       x = "log_10 of Degrees of freedom",
       y = "log_10 of t-statistics")

graph_2 <- t_value %>% 
  mutate(
    t_value = qt(0.975,df = row_number())
  ) %>% 
  ggplot(aes(x = df,y = t_value,group = 1))+
  geom_line(color = graph_color)+
  theme_graph()+
  labs(title = "Log transformation not applied",
       x = "Degrees of freedom",
       y = "t-statistics")

gridExtra::grid.arrange(graph_1,graph_2, ncol = 2)
```

### **3) Gere uma amostra aleatória de 10 valores com base em uma distribuição normal com média 0 e desvio padrão 2. Use a função *t.test()* para testar a hipótese nula de que a média é 0. Depois gere novamente uma amostra aleatória de 10 valores com média de 1.5 e desvio padrão 2, usando a função *t.test()* para testar a hipótese da média ser 0.**

Conforme visto no capítulo, o teste de hipótese irá utilizar o **valor-p** para decidir se a hipótese nula deve ou não ser rejeitada. Há duas formas de se realizar o teste, ou seja, encontrar o **valor-p**, a primeira é usando a fórmula do R **t.test()** e a outra é fazendo o cálculo na mão (apesar de que este modo também usará uma fórmula pronta do R para encontrar a área sobre a curva de distribuição **t**).

O **valor-p** será a área sobre a curva, também chamada de probabilidade cumulativa, de um determinado parâmetro. Vamos realizar os exercícios para facilitar o entendimento. Primeiro com a utilização do **t.test()**.

```{r, echo=T, include=T, message=F,warning=F}
# Vetor com 10 valores.
a <- rnorm(n = 10, mean = 0, sd = 2)

# Gerando um teste t
pander::pander(t.test(a, mu = 0, conf.level = 0.975))
```

Agora vamos calcular na "mão". Para isso, basta usar a fórmula $t.value = \frac{\bar{x}}{SEM}$ ou $t.value = \frac{\bar{x}}{\frac{s}{\sqrt{n}}}$. Esta fórmula irá fornecer um valor crítico de **t**, que poderá ser inserido na função do R **qt(nível de confiança, graus de liberdade)** que irá fornecer a probabilidade cumulativa até o valor crítico de **t**. Após isso, o **valor-p** será **1-qt(nível de confiança, graus de liberdade)**.

```{r, echo=T, include=T, message=F,warning=F}
mean <- round(abs(mean(a)),digits = 4)
sd <- round(sd(a),digits = 4)
n_sqrt <- round(length(a)^0.5,digits = 4)
sem <- round(sd(a)/length(a)^0.5,digits = 4)
t_value <- round(abs(mean(a))/(sem),digits = 4)
p_value <- (1-pt(t_value,10-1))*2 # Multiplica por dois pq é um teste bicaudal (H0 = 0)

data.frame(mean = mean,
           sd = sd,
           n_sqrt = n_sqrt,
           sem = sem,
           t_value = t_value,
           p_value = p_value)
```

Veja que o **valor-p** obtido é igual nos dois métodos e ficou alto a ponto de não ser possível rejeitar H0. Um **valor-p** elevado indica que o parâmetro encontrado (neste caso a média da amostra dos 10 valores com valor 1.3783) não foi encontrado por conta do acaso, mas sim por outros motivos (que não é o objetivo do teste indicar quais são). Desta forma, não se pode rejeitar H0, que indica que a média populacional é zero. Isto é satisfatório, pois os números aleatórios foram de fato gerados de uma "população" com média 0 e desvio padrão 2.

Na segunda parte do exercício, é pedido para fazer o mesmo processo, porém agora com desvio padrão de 1.5.

```{r, echo=T, include=T, message=F,warning=F}
# Vetor com 10 valores.
a <- rnorm(n = 10, mean = 0, sd = 1.5)

# Gerando um teste t
pander::pander(t.test(a, mu = 0, conf.level = 0.975))
```

### **4) Desenvolva uma função para fazer o cálculo acima do valor-p, com uma amostra de n = 10, média = 0 e sd = 1. Crie um vetor de 50 valores-p com base nesta função e plote em um gráfico este vetor.**

O exercício pede para usar 50 repetições, porém eu preferi fazer com 1.000 para melhorar a visualização. Veja que como o valor-p é uma probabilidade cumulativa, seu valor fica entre 0 e 1.

```{r, echo=T, include=T, message=F,warning=F}
saida <- rep(NA,1000)

for (i in 1:1000){
  a <- rnorm(n = 10, mean = 0, sd = 1)
  mean <- mean(a)
  sd <- sd(a)
  sem <- sd/sqrt(10)
  saida[i] <- data.frame(
    p_value = (1-pt((abs(mean)/sem),9))*2 # Multiplica por dois pq é um teste bicaudal (H0 = 0)
  )
}

df <- data.frame(
  a = matrix(unlist(saida),nrow = 1000, byrow = F),stringsAsFactors = F)

ggplot(data = df, aes(x = a))+
  geom_histogram(fill = graph_color, binwidth = .1)+
  expand_limits(x = 1)+
  theme_graph()
```

## **5**

### **Regression with a single predictor**

### **Resumo**

O foco deste capítulo será em modelos de regressão linear. As premissas dos modelos deste capítulo serão: **(i) modelo com apenas uma variável**, **(ii) valores de y independentes** e **(iii) dado um valor de x, o valor de y se distribuirá normalmente com uma média dada pela função linear de x**.

Neste capítulo, os autores mostram como se controí um modelo de regressão linear simples no R, usando a função **lm(y~x)**. Além da geração do modelo, são apresentadas as formas de se extrair os resultados específicos do modelo, usando a função **predict()** e **resid()**. 

Duas validações básicas sobre os resíduos são apresentadas, uma irá validar se os resíduos se distribuem normalmente e a outra irá validar se existe algum padrão no comportamento dos resíduos (a ideia é que não exista). A primeira validação pode ser obtida através de um **qq-plot** e a segunda através de um gráfico que compare o valor do resíduo com o valor previsto pelo modelo.

Aprofundando mais a explicação do modelo linear, é apresentada a **Análise de Variância** (a.k.a **ANOVA**), que irá gerar informações sobre o nivel de variabilidade de um modelo de regressão e fundamentar o teste de significância. A fórmula básica da **ANOVA** é:

$y_i - \bar{y} = (\hat{y} - \bar{y})+(y_i-\hat{y})$

Onde:

$y_i - \bar{y}$: Variabilidade total do modelo gerado

$\hat{y} - \bar{y}$: Variabilidade explicada pela variável independente (a.k.a variável explicativa), que é o **x**, ou seja, a variabilidade explicada pelo modelo

$y_i-\hat{y}$: Variabilidade não explicada pelo modelo, ou seja, o termo de erro.

A equação acima também pode ser escrita da seguinte forma: 
**SST = SSM + SSE**

Que será lida como: Variabilidade total (SST) é igual a soma da variabilidade do modelo (SSM) com a soma dos erros (SSE), lembrando que é a soma do quadrado das diferenças, gerando a fórmula final:

$\sum{(y_i - \bar{y})^2} = \sum{(\hat{y} - \bar{y})}^2+\sum{(y_i-\hat{y})}^2$

Obs: Primeiro se calcula a diferença, depois eleva ao quadrado e depois realiza o somatório.

Com isso, os autores deixam claro que a **ANOVA** é uma forma de se analisar o poder de explicação do modelo gerado. Ainda no âmbito da **ANOVA**, os autores mostram como se calcular o **R²** e o **Adjusted R²**, através das fórmulas:

$R^2 = \frac{SSM}{SST}$

Asjusted R² = $1-\frac{\frac{SSE}{n-2}}{\frac{SST}{n-1}}$

Os autores indicam que o **Adjsuted R²** é preferível ao **R²** por conta de que ele leva em consideração os graus de liberdade do modelo. Porém ambos são insatisfatórios para gerar conclusões sobre o quanto o modelo poderá prever novos resultados.

Após explicarem sobre **ANOVA** os autores abordam a questão de outliers presentes nos dados e seu impacto no modelo de regressão. Primeiro é apresentado o conceito de **Cook's distance**, que mede a influência de outliers no modelo através da medição da mudança na linha de regressão com a retirada de um ou mais registros (i.e. registros outliers).

Ainda sobre a tratativa de outliers, os autores apresentam uma técnica chamada de **Regressão Robusta**, que atua sobre os outliers não retirando-os do modelo, mas sim reduzindo sua relevância, enquanto outras observações recebem maior peso para geração das estimativas dos parâmetros ($\beta$).

Após explicarem sobre outliers e como tratar este aspecto dos dados, os autores abordam a acuracidade dos parâmetros do modelo.  Esta acuracidade é medida pelo **erro padrão**, que é usada para criar intervalos de confinaça e teste de significância do parâmetro **intercepto** e **slope**. Um teste fundamental é a validação se o intervalo de confiança do intercepto irá conter o valor 0, se caso conter, isso indicará que ele não deve ser incluído no modelo.

Ao finalizarem a explicação sobre acuracidade dos parâmetros, os autores apresentam a técnica de **cross-validation**, usada também para reforçar o resultado do modelo criado. Esta técnica irá segmentar os registros em **teste** e **treino** de forma que o modelo criado nos registros de **treino** serão testados com os registros no **teste**. Junto com esta técnica, há o **boostraping**, que faz repetições do processo de amostragem (com reposição ou sem), sendo usada junto com o **cross-validation** para repetir várias versões do modelo para analisar variações da acuracidade e aumentar a confiança da conclusão gerada.

### **Exercícios**

### **1) Usando os dados abaixo, referente à duas amostras de dados sobre elasticidades de elásticos, plote um gráfico que correlacione a distância e a elasticidade de cada grupo.**

```{r, echo=T, include=T, message=F,warning=F}
data_set_1 <- data.frame(stretch = c(46,54,48,50,44,42,52),
                         distance = c(183,217,189,208,178,150,249))

data_set_2 <- data.frame(stretch = c(25,45,35,40,55,50,30,50,60),
                         distance = c(71,196,127,187,248,217,114,228,291))

# Plotando os dados
ggplot2::ggplot()+
  geom_point(data = data_set_1, aes(x = stretch, y = distance), shape = 2, color = "blue")+
  geom_point(data = data_set_2, aes(x = stretch, y = distance), shape = 3, color = "red")+
  theme_graph()
```

### **2) Crie um modelo de regressão linear para stretch ~ distance. Para cada modelo, determine (i) os valores previstos e os erros padrão, (ii) o R² e (ii) indique as principais diferenças entre os data sets. Em seguida, use uma regressão robusta através da fórmula rlm() para ver a diferença com a regressão lienar tradicional.**

```{r, echo=T, include=T, message=F,warning=F}
# Modelo linear para o data_set_1
model_1 <- lm(formula = stretch ~ distance, data = data_set_1)

# Modelo linear para o data_set_2
model_2 <- lm(formula = stretch ~ distance, data = data_set_2)

# Trazendo os campos de fitted values e resíduos para o data_set_1
data_set_1 %>% 
  mutate(fitted_values = predict(model_1),
         residuals = residuals(model_1))

# Trazendo os campos de fitted values e resíduos para o data_set_2
data_set_2 %>% 
  mutate(fitted_values = predict(model_2),
         residuals = residuals(model_2))

model_1 %>% summary() %>% pander()
model_2 %>% summary() %>% pander()

# Modelo linear robusto para o data_set_1
summary(MASS::rlm(formula = stretch ~ distance, data = data_set_1)) %>% pander()
# Modelo linear robusto para o data_set_2
summary(MASS::rlm(formula = stretch ~ distance, data = data_set_2)) %>% pander()

```

### **3) Usando o data set cars, plote a distância (para frenagem) versus a velocidade. Crie uma linha de regressão para esta correlação. Em seguida, crie uma linha quadrática para ver se o modelo irá melhorar.**

```{r, echo=T, include=T, message=F,warning=F}
cars %>% 
  ggplot()+
  geom_point(aes(x = speed, y = dist))+
  geom_smooth(aes(x = speed, y = dist, color = "linear"), method = "lm", formula = y ~ x, se = F)+
  geom_smooth(aes(x = speed, y = dist, color = "quadratic"), method = "lm", formula = y ~ x + I(x^2), se = F)+
  scale_color_manual(values = c("linear" = "blue",
                                "quadratic" = "red"),
                     labs(color = ""))+
  theme_graph()+
  labs(title = "Modelo linear e quadrático para as variáveis",
       subtitle = "Linha quadrática com melhor performance",
       x = "Speed",
       y = "Distância de frenagem")

# Validando os dois modelos
summary(lm(formula = dist ~ speed, data = cars)) %>% pander()
summary(lm(formula = dist ~ speed + I(speed^2), data = cars)) %>% pander()
```

### **4) Use o data set oddbooks do pacote DAAG para as alternativas abaixo:** 
### **a) Plote log(weight) versus log(volume) e estabeleça uma linha de regressão.**

```{r, echo=T, include=T, message=F,warning=F}
DAAG::oddbooks %>% 
  mutate(volume = round(height * thick * breadth, digits = 1),
         area = round(height * thick, digits = 1)) %>% 
  ggplot()+
  geom_point(aes(x = log(weight), y = log(volume)))+
  geom_smooth(aes(x = log(weight), y = log(volume)), method = "lm", formula = y ~ x, se = F)+
  theme_graph()
```

### **b) Plote log(weight) versus log(area) e estabeleça uma linha de regressão.**

```{r, echo=T, include=T, message=F,warning=F}
DAAG::oddbooks %>% 
  mutate(volume = round(height * thick * breadth, digits = 1),
         area = round(height * thick, digits = 1)) %>% 
  ggplot()+
  geom_point(aes(x = log(weight), y = log(area)))+
  geom_smooth(aes(x = log(weight), y = log(area)), method = "lm", formula = y ~ x, se = F)+
  theme_graph()
```

