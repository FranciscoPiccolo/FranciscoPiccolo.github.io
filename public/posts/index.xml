<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Glass Frog</title>
        <link>/posts/</link>
        <description>Recent content in Posts on Glass Frog</description>
        <generator>Hugo -- gohugo.io</generator>
        <lastBuildDate>Thu, 13 Feb 2020 00:00:00 +0000</lastBuildDate>
        <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Exploratory Data Analysis Project I</title>
            <link>/posts/2020/02/exploratory-data-analysis-project-i/</link>
            <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
            
            <guid>/posts/2020/02/exploratory-data-analysis-project-i/</guid>
            <description>Exploratory Data Analysis (EDA) is a technique developed by John Tukey in 1960 that aims the following goals: formulate hipothesis, generate graphs e understand patterns on the data. All these goals envision to make people aware of insights that they hadn’t thought before. With this technique being created, also became alive the program language S, the predecessor of R.
The EDA is an activity frequently made by many different professionals, that have to handle with increasingly data volumes (structured and unstructured) to answer business questions in a fastly pace.</description>
            <content type="html"><![CDATA[


<p>Exploratory Data Analysis (EDA) is a technique developed by John Tukey in 1960 that aims the following goals: formulate hipothesis, generate graphs e understand patterns on the data. All these goals envision to make people aware of insights that they hadn’t thought before. With this technique being created, also became alive the program language S, the predecessor of R.</p>
<p>The EDA is an activity frequently made by many different professionals, that have to handle with increasingly data volumes (structured and unstructured) to answer business questions in a fastly pace. In this post I’ll use a dataset that I’ve got in a hiring process to perform an exploratory data analysis task. The goal will be the generation of insights from this dataset to make recommendations for the alleged directors of this company.</p>
<p>Before we start, lets call the necessary packages for this project.</p>
<pre class="r"><code># Pacotes para importar
library(tidyverse)
library(gridExtra)
library(grid)
library(zoo)
library(knitr)
library(scales)
library(ggthemes)
library(zoo)
library(readxl)
library(Amelia)

theme_graph &lt;- function(){
  theme(
    plot.title = element_text(size = 16),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(face = &quot;italic&quot;, size = 9),
    axis.text = element_text(size = 9),
    axis.title = element_text(face = &quot;italic&quot;, size = 9),
    text = element_text(family = &quot;Times New Roman&quot;),
    strip.background = element_rect(fill = &quot;grey&quot;),
    strip.text = element_text(face = &quot;bold&quot;),
    legend.title = element_blank(),
    legend.position = &quot;bottom&quot;
  )
}
  
graph_color &lt;- &quot;#006666&quot;</code></pre>
<p>The dataset is already on my Desktop, thus I’ll just import it to R Studio using “readxl” package.</p>
<pre class="r"><code>df &lt;- readxl::read_xls(&quot;C:/Users/francisco.piccolo/Desktop/R/02.Data_bases/Global Superstore.xls&quot;)</code></pre>
<p>Lets see the data structure using a sample of 10 rows of the dataset. For this, I’ll use the <strong>dim</strong> function that inform the dimension of my dataset in terms of rows and columns. The <strong>sample</strong> function will be used to generate the 10 samples from the dataset.</p>
<pre class="r"><code>dim(df)</code></pre>
<pre><code>## [1] 51290    24</code></pre>
<pre class="r"><code>df[sample(nrow(df),10), ]</code></pre>
<pre><code>## # A tibble: 10 x 24
##    `Row ID` `Order ID` `Order Date`        `Ship Date`         `Ship Mode`
##       &lt;dbl&gt; &lt;chr&gt;      &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;      
##  1    13716 ES-2012-4~ 2012-12-05 00:00:00 2012-12-09 00:00:00 Standard C~
##  2     6473 US-2012-1~ 2012-02-06 00:00:00 2012-02-08 00:00:00 First Class
##  3    17140 ES-2012-5~ 2012-10-12 00:00:00 2012-10-14 00:00:00 Second Cla~
##  4    38463 CA-2011-1~ 2011-07-22 00:00:00 2011-07-24 00:00:00 Second Cla~
##  5     9869 US-2014-1~ 2014-06-03 00:00:00 2014-06-07 00:00:00 Standard C~
##  6     5384 MX-2013-1~ 2013-11-11 00:00:00 2013-11-17 00:00:00 Standard C~
##  7    15545 ES-2011-5~ 2011-04-07 00:00:00 2011-04-11 00:00:00 Second Cla~
##  8     6752 MX-2011-1~ 2011-03-25 00:00:00 2011-03-31 00:00:00 Standard C~
##  9     3046 MX-2013-1~ 2013-09-06 00:00:00 2013-09-06 00:00:00 Same Day   
## 10      259 MX-2011-1~ 2011-06-21 00:00:00 2011-06-23 00:00:00 Second Cla~
## # ... with 19 more variables: `Customer ID` &lt;chr&gt;, `Customer Name` &lt;chr&gt;,
## #   Segment &lt;chr&gt;, City &lt;chr&gt;, State &lt;chr&gt;, Country &lt;chr&gt;, `Postal Code` &lt;chr&gt;,
## #   Market &lt;chr&gt;, Region &lt;chr&gt;, `Product ID` &lt;chr&gt;, Category &lt;chr&gt;,
## #   `Sub-Category` &lt;chr&gt;, `Product Name` &lt;chr&gt;, Sales &lt;dbl&gt;, Quantity &lt;dbl&gt;,
## #   Discount &lt;dbl&gt;, Profit &lt;dbl&gt;, `Shipping Cost` &lt;dbl&gt;, `Order Priority` &lt;chr&gt;</code></pre>
<p>Also, lets see if there are null values on this dataset through the <strong>Amelia</strong> package, that has the <strong>missmap</strong> function that shows in a graph the null values of the hole dataset.</p>
<pre class="r"><code>Amelia::missmap(df)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>We can see that some fields are incorrectly named (i.e. with spaced separating compound names), thus I’ll adjust these fields to simplify future data manipulation.</p>
<pre class="r"><code>df_adj &lt;- df %&gt;% 
  transmute(
    order_date = `Order Date`,
    customer = `Customer ID`,
    segment = Segment,
    category = Category,
    sub_category = `Sub-Category`,
    city = City,
    ship_date = `Ship Date`,
    ship_mode = `Ship Mode`,
    product_id = `Product ID`,
    order_priority = `Order Priority`,
    product_name = `Product Name`,
    market = Market,
    region = Region,
    qty = Quantity,
    sales = Sales,
    profit = Profit,
    shipping_cost = `Shipping Cost`,
    discount = Discount
  )</code></pre>
<p>Now with the fields adjusted, we can start the EDA task, remembering that the goal will be generate recommendation for the supposed directors of this company. Before we start, it’s interesting to establish a reasoning line to approach the dataset. Another important point to have in mid is that if the analyst overly “torture” the data, it will confess anything even if it isn’t the truth. John Tukey mention this on his conferencies about exploratory data analysis and in his books, trying to explain that too many data manipulations can lead to incorrect conclusions if they’re incorrectly made.</p>
<p>The reasoning that I’ll follow to approach this task will be (i) visualize the financial data through time series plots in order to verify trendings and sazonality on financial indicators of the company; (ii) analyse market sales to identify areas to improve; (iii) analyse the product portfolio to see if there are products undermining the profit margin; (iv) analyse the operational aspect of the business to identify strategies that must change in order to recover profit margin.</p>
<p>In this dataset, there’re commercial informations, where the company sells for the whole world office products and technologic products. The dataset presents revenue, profit, discount and delivery cost, that I’ll call financial data. As well as this information, the dataset contain customer, product and operational data.</p>
<div id="time-series-of-financial-data" class="section level2">
<h2><strong>Time series of financial data</strong></h2>
<p>To begin, lets dive in on the financial indicators of the company. The following graph presents revenue throughout 2011-2015 period. As the revenue variation is too high I opted to smooth the line with a moving average of 30 days to ease de visualization.</p>
<pre class="r"><code>df_adj %&gt;% 
  mutate(year = as.numeric(substr(order_date,1,4))) %&gt;% 
  group_by(order_date) %&gt;% 
  summarise(sales = sum(sales)) %&gt;% 
  mutate(roll_avg_sales = zoo::rollmean(sales, 30, na.pad = T, align = &quot;right&quot;)) %&gt;% 
  ggplot()+
  geom_line(aes(x = order_date, y = roll_avg_sales, color = &quot;roll_avg_sales&quot;))+
  geom_line(aes(x = order_date, y = sales, color = &quot;sales&quot;), alpha = .1)+
  geom_smooth(aes(x = order_date, y = roll_avg_sales, color = &quot;trend&quot;), method = &quot;lm&quot;)+
  scale_y_continuous(labels = scales::comma)+ 
  scale_color_manual(values = c(&quot;sales&quot; = &quot;black&quot;,
                                &quot;roll_avg_sales&quot; = graph_color,
                                &quot;trend&quot; = &quot;red&quot;),
                     labs(color = &quot;&quot;))+
  theme_graph()+
  labs(title = &quot;Daily revenue time series&quot;,
       subtitle = &quot;Growth treding and sazonality&quot;,
       x = &quot;Sales Date&quot;,
       y = &quot;Revenue&quot;,
       caption = &quot;30 days moving average&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>At the above graph, the sales present sazonality and also it shows that the company is expanding. Lets see on the next graph if the profit’s followed the same pattern.</p>
<pre class="r"><code>df_adj %&gt;% 
  mutate(year = as.numeric(substr(order_date,1,4))) %&gt;% 
  group_by(order_date) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit)) %&gt;% 
  mutate(roll_avg_profit = zoo::rollmean(profit, 30, na.pad = T, align = &quot;right&quot;)) %&gt;% 
  ggplot()+
  geom_line(aes(x = order_date, y = roll_avg_profit, color = &quot;roll_avg_profit&quot;))+
  geom_line(aes(x = order_date, y = profit, color = &quot;profit&quot;), alpha = .1)+
  geom_smooth(aes(x = order_date, y = roll_avg_profit, color = &quot;trend&quot;), method = &quot;lm&quot;)+
  scale_y_continuous(labels = scales::comma)+
  scale_color_manual(values = c(&quot;profit&quot; = &quot;black&quot;,
                                &quot;roll_avg_profit&quot; = graph_color,
                                &quot;trend&quot; = &quot;red&quot;),
                     labs(color = &quot;&quot;))+
  theme_graph()+
  labs(title = &quot;Daily profit time series&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;,
       caption = &quot;30 days moving average&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The profit is following the same pattern of the sales, however the visualization has gotten a little bit distorced on behalf of net lost earned at some days. It’s seems prefered to normalize the profit by the revenue sold to get profit margin. Lets see this metric at the following graph.</p>
<pre class="r"><code>df_adj %&gt;%
  group_by(order_date) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit)) %&gt;% 
  mutate(roll_sum_sales = zoo::rollsum(sales, 30, na.pad = T, align = &quot;right&quot;),
         roll_sum_profit = zoo::rollsum(profit, 30, na.pad = T, align = &quot;right&quot;)) %&gt;% 
  mutate(profit_p = round(roll_sum_profit/roll_sum_sales,digits = 4),
         profit_mg = round(profit/sales, digits = 4)) %&gt;% 
  ggplot()+
  geom_line(aes(x = order_date, y = profit_p, color = &quot;roll_avg_profit&quot;))+
  #geom_line(aes(x = order_date, y = profit_mg, color = &quot;profit&quot;), alpha = .2)+
  scale_y_continuous(labels = scales::percent)+
  scale_color_manual(values = c(&quot;roll_avg_profit&quot; = graph_color,
                                &quot;profit&quot; = &quot;black&quot;),
                     labs(color = &quot;&quot;))+
  theme_graph()+
  labs(title = &quot;Profit margin time series&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;,
       caption = &quot;30 days moving average&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Beyong profit, it’s important to see if delivery costs stayed stable during the period. In accordance with the following graph, apparently the company didn’t gain operational improvement event with the revenue growth.</p>
<pre class="r"><code>df_adj %&gt;%
  group_by(order_date) %&gt;% 
  summarise(sales = sum(sales),
            shipping_cost = sum(shipping_cost)) %&gt;% 
  mutate(roll_sum_sales = zoo::rollsum(sales, 30, na.pad = T, align = &quot;right&quot;),
         roll_sum_shipping_cost = zoo::rollsum(shipping_cost, 30, na.pad = T, align = &quot;right&quot;)) %&gt;% 
  mutate(shipping_cost = round(roll_sum_shipping_cost/roll_sum_sales,digits = 4)) %&gt;% 
  ggplot()+
  geom_line(aes(x = order_date, y = shipping_cost, group = 1), color = graph_color)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;Delivery cost time series&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;,
       caption = &quot;30 days moving average&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Finally, lets see how the discount strategy was implemented during the period. The following graph will show the average discount in proportion of the revenue.</p>
<pre class="r"><code>df_adj %&gt;%
  mutate(discount_amount = discount * sales) %&gt;% 
  group_by(order_date) %&gt;% 
  summarise(sales = sum(sales),
            shipping_cost = sum(shipping_cost),
            discount = sum(discount_amount)) %&gt;% 
  mutate(roll_sum_sales = zoo::rollsum(sales, 30, na.pad = T, align = &quot;right&quot;),
         roll_sum_shipping_cost = zoo::rollsum(shipping_cost, 30, na.pad = T, align = &quot;right&quot;),
         roll_sum_discount = zoo::rollsum(discount, 30, na.pad = T, align = &quot;right&quot;)) %&gt;%
  mutate(discount = round(roll_sum_discount/roll_sum_sales,digits = 4)) %&gt;% 
  ggplot()+
  geom_line(aes(x = order_date, y = discount, group = 1), color = graph_color)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;Discount margin time series&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;,
       caption = &quot;30 days moving average&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>After we get the time series of financial metrics, we can walk in on onther aspects of the business to generate some business recommendations.</p>
</div>
<div id="region" class="section level2">
<h2><strong>Region</strong></h2>
<p>On the following graphs, we’ll see the region that generates more profit for the company. Also, we’ll analyse if the delivery cost in relation to the revenue varies between regions.</p>
<pre class="r"><code>graph_1 &lt;- df_adj %&gt;% 
  filter(order_date &gt;= &quot;2014-01-01&quot;) %&gt;% 
  group_by(region) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            shp_cost = sum(shipping_cost)) %&gt;% 
  ungroup() %&gt;% 
  mutate(region = fct_reorder(region, sales)) %&gt;% 
  ggplot()+
  geom_col(aes(x = region, y = sales), fill = graph_color)+
  scale_y_continuous(labels = scales::comma)+ 
  theme_graph()+
  coord_flip()+
  labs(y = &quot;Sales&quot;,
       x = &quot;&quot;)

graph_2 &lt;- df_adj %&gt;% 
  filter(order_date &gt;= &quot;2014-01-01&quot;) %&gt;% 
  group_by(region) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            shp_cost = sum(shipping_cost)) %&gt;% 
  mutate(profit_mg = round(profit/sales, digits = 4)) %&gt;% 
  ungroup() %&gt;% 
  mutate(region = fct_reorder(region, sales)) %&gt;% 
  ggplot()+
  geom_col(aes(x = region, y = profit_mg), fill = graph_color)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  coord_flip()+
  labs(y = &quot;Profit margin&quot;,
       x = &quot;&quot;,
       caption = &quot;&quot;)

graph_3 &lt;- df_adj %&gt;% 
  filter(order_date &gt;= &quot;2014-01-01&quot;) %&gt;% 
  group_by(region) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            shp_cost = sum(shipping_cost)) %&gt;% 
  mutate(profit_mg = round(profit/sales, digits = 4),
         ship_cost_mg = round(shp_cost/sales, digits = 4)) %&gt;% 
  ungroup() %&gt;% 
  mutate(region = fct_reorder(region, sales)) %&gt;% 
  ggplot()+
  geom_col(aes(x = region, y = ship_cost_mg), fill = graph_color)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  coord_flip()+
  labs(y = &quot;Delivery cost&quot;,
       x = &quot;&quot;,
       caption = &quot;Period: 2014 e 2015&quot;)

graph_4 &lt;- df_adj %&gt;% 
  mutate(discount_amount = discount * sales) %&gt;% 
  filter(order_date &gt;= &quot;2014-01-01&quot;) %&gt;% 
  group_by(region) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            shp_cost = sum(shipping_cost),
            discount = sum(discount_amount)) %&gt;% 
  mutate(profit_mg = round(profit/sales, digits = 4),
         ship_cost_mg = round(shp_cost/sales, digits = 4),
         discount_mg = round(discount/sales, digits = 4)) %&gt;% 
  ungroup() %&gt;% 
  mutate(region = fct_reorder(region, sales)) %&gt;% 
  ggplot()+
  geom_col(aes(x = region, y = discount_mg), fill = graph_color)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  coord_flip()+
  labs(y = &quot;Discount margin&quot;,
       x = &quot;&quot;,
       caption = &quot;Period: 2014 e 2015&quot;)

gridExtra::grid.arrange(graph_1,graph_2, graph_3,graph_4, ncol = 2, nrow = 2, top = textGrob(&quot;Revenue, Profit, Delivery cost and discount per region&quot;))</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Apparently, Canada is the region to receive more attention, since it has a profit margin higher than the average, maybe it’s because the discount policy is different atthere. On the other hand, South East Asia needs some strategy change in order to reach the other countries in profit margins. The delivery cost is the same between reagions.</p>
</div>
<div id="product-category-and-sub-category" class="section level2">
<h2><strong>Product Category and Sub-category</strong></h2>
<p>The company works with three product categories: Technologic, Furniture and Office supplies. In the next visualization, we’ll see which category has gained traction along the period as well as the profit margin of each of them</p>
<pre class="r"><code>df_adj %&gt;%
  group_by(category, order_date) %&gt;% 
  summarise(sales = sum(sales)) %&gt;% 
  ggplot()+
  geom_smooth(aes(x = order_date, y = sales, group = category, color = category), se = F)+
  scale_y_continuous(labels = scales::comma)+ 
  theme_graph()+
  labs(title = &quot;Category sales time series&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Apparently three categories present similar behavior during the period, having the technologic category as the most sold. Following we’ll see which one have bring the highest profit margin.</p>
<pre class="r"><code>df_adj %&gt;%
  filter(order_date &gt;= &quot;2014-01-01&quot;) %&gt;% 
  group_by(category) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            shp_cost = sum(shipping_cost)) %&gt;% 
  mutate(profit_mg = round(profit/sales, digits = 4)) %&gt;% 
  ggplot()+
  geom_col(aes(x = category, y = profit_mg), fill = graph_color)+
  scale_y_continuous(labels = scales::percent)+ 
  theme_graph()+
  labs(title = &quot;Category profit margin&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;,
       caption = &quot;Period: 2014 e 2015&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The technologic products presents the highest profit margin, thus the company is on the right track in its comercial strategy. Now lets analyse the product categories with focus on discount and delivery cost.</p>
<pre class="r"><code>df_adj %&gt;% 
  mutate(discount = discount * sales) %&gt;% 
  filter(order_date &gt;= &quot;2014-01-01&quot;) %&gt;% 
  group_by(category) %&gt;% 
  summarise(sales = sum(sales),
            discount = sum(discount),
            shipping_cost = sum(shipping_cost)) %&gt;% 
  mutate(discount_mg = round(discount/sales, digits = 4),
         shipping_cost_mg = round(shipping_cost/sales, digits = 4)) %&gt;% 
  select(category, discount_mg, shipping_cost_mg) %&gt;% 
  tidyr::gather(&quot;campo&quot;,&quot;valor&quot;,2:3) %&gt;% 
  ggplot(aes(x = category, y = valor, fill = campo))+
  geom_col(position = &quot;dodge&quot;,alpha = .6)+
  scale_fill_manual(values = c(&quot;discount_mg&quot; = graph_color,
                               &quot;shipping_cost_mg&quot; = &quot;blue&quot;),
                    labs(color = &quot;&quot;))+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;Discount and delivery cost by category&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>The graph above indicates that the discount applied for the “Technology” category is lower and the delivery cost is stable for all three categories. Finishing the focus on product category, we can see that there isn’t an urgent recommendation to be made for the company’s directors. Lets dive in on the sub-category part of the analyse.</p>
<pre class="r"><code>df_adj %&gt;%
  filter(order_date &gt;= &quot;2014-01-01&quot;) %&gt;% 
  group_by(sub_category) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            shp_cost = sum(shipping_cost)) %&gt;% 
  mutate(profit_mg = round(profit/sales, digits = 4)) %&gt;% 
  ungroup() %&gt;% 
  mutate(sub_category = fct_reorder(sub_category, profit_mg)) %&gt;% 
  ggplot()+
  geom_col(aes(x = sub_category, y = profit_mg), fill = graph_color)+
  scale_y_continuous(labels = scales::percent)+ 
  theme_graph()+
  labs(title = &quot;Profit margin by product category&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;,
       caption = &quot;Period: 2014 e 2015&quot;)+
  coord_flip()</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Its possible to note that “Tablets” is very prejudicial for the company. Maybe the discount applied for this sub-category is undermining the market rentability. The following graph presents the discount applied for each sub-category.</p>
<pre class="r"><code>df_adj %&gt;%
  mutate(discount = discount * sales) %&gt;% 
  filter(order_date &gt;= &quot;2014-01-01&quot;) %&gt;% 
  group_by(sub_category) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            shp_cost = sum(shipping_cost),
            discount = sum(discount)) %&gt;% 
  mutate(discount_mg = round(discount/sales, digits = 4)) %&gt;% 
  ungroup() %&gt;% 
  mutate(sub_category = fct_reorder(sub_category, discount_mg)) %&gt;% 
  ggplot()+
  geom_col(aes(x = sub_category, y = discount_mg), fill = graph_color)+
  scale_y_continuous(labels = scales::percent)+ 
  theme_graph()+
  labs(title = &quot;Average discount margin by product category&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;,
       caption = &quot;Period: 2014 e 2015&quot;)+
  coord_flip()</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Clearly the discount policy for “Tablets” will need a revision. This sub-category has 366 different products, lets see which one are receiving the highest discount, the profit margin and the revenue sold. The following graph show this view.</p>
<pre class="r"><code>df_adj %&gt;% 
  mutate(discount = discount * sales) %&gt;% 
  filter(sub_category == &quot;Tables&quot;) %&gt;% 
  select(sub_category, sales, product_id, discount, profit) %&gt;% 
  group_by(product_id) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            discount = sum(discount)) %&gt;% 
  mutate(profit_mg = round(profit/sales, digits = 4),
         discount_mg = round(discount/sales, digits = 4),
         Product_demand = ifelse(sales &gt;= 3000, &quot;high_demand&quot;,&quot;low_demand&quot;)) %&gt;% 
  filter(profit_mg &gt;= -2) %&gt;% 
  ggplot()+
  geom_point(aes(x = profit_mg, y = discount_mg, size = sales, color = Product_demand), shape = 20, alpha = .6)+
  geom_vline(xintercept = 0, lty = 2, color = &quot;red&quot;)+
  scale_x_continuous(labels = scales::percent)+
  scale_y_continuous(labels = scales::percent)+
  scale_color_manual(values = c(&quot;high_demand&quot; = &quot;black&quot;,
                                &quot;low_demand&quot; = &quot;dark orange&quot;))+
  theme_graph()+
  labs(title = &quot;Product profit and discount margin relation&quot;,
       x = &quot;Profit margin&quot;,
       y = &quot;Discount&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>A lot of products selling a relevant quantity (above 3.000 unities on the considered period) and with a high discount portion making the sales prejudicial for the company. It’s necessary to verify if this also occurs with other sub-categories.</p>
<pre class="r"><code>df_adj %&gt;% 
  mutate(discount = discount * sales) %&gt;% 
  select(sub_category, sales, product_id, discount, profit) %&gt;% 
  group_by(product_id, sub_category) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            discount = sum(discount)) %&gt;% 
  mutate(profit_mg = round(profit/sales, digits = 4),
         discount_mg = round(discount/sales, digits = 4),
         Product_demand = ifelse(sales &gt;= 3000, &quot;high_demand&quot;,&quot;low_demand&quot;)) %&gt;% 
  ungroup() %&gt;% 
  mutate(sub_category = fct_relevel(sub_category, levels = c(&quot;Tables&quot;,&quot;Machines&quot;,&quot;Chairs&quot;,&quot;Supplies&quot;,&quot;Storage&quot;,&quot;Furnishings&quot;,&quot;Bookcases&quot;,&quot;Phones&quot;,&quot;Fasteners&quot;,&quot;Binders&quot;,&quot;Appliances&quot;,&quot;Envelopes&quot;,&quot;Art&quot;,&quot;Accessories&quot;,&quot;Copiers&quot;,&quot;Labels&quot;,&quot;Paper&quot;))) %&gt;% 
  filter(profit_mg &gt;= -2) %&gt;% 
  ggplot()+
  geom_point(aes(x = profit_mg, y = discount_mg, size = sales, color = Product_demand), shape = 20, alpha = .5)+
  geom_vline(xintercept = 0, lty = 2, color = &quot;red&quot;)+
  scale_x_continuous(labels = scales::percent)+
  scale_y_continuous(labels = scales::percent)+
  scale_color_manual(values = c(&quot;high_demand&quot; = &quot;black&quot;,
                                &quot;low_demand&quot; = &quot;dark orange&quot;))+
  theme_graph()+
  facet_wrap(~sub_category)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Seems that products from other sub-categories don’t have the same aggressive discount policy. Thus, the sub-category “Tablets” and “Machines” present a lot of black points at the left part of the graph (separeted by the red line). We can conclude that the company’s directors will need some adjustments on this discount policy.</p>
</div>
<div id="operations" class="section level2">
<h2><strong>Operations</strong></h2>
<p>In this topic we’ll analyse the financial metrics by the operational perspective, aiming to analyse if the order criticality and the delivery contract cause some financial impact.</p>
<pre class="r"><code>df_adj %&gt;% 
  filter(order_date &gt;= &quot;2014-01-01&quot;) %&gt;% 
  group_by(order_priority, ship_mode) %&gt;% 
  summarise(sales = sum(sales),
            profit = sum(profit),
            shipping_cost = sum(shipping_cost)) %&gt;% 
  ungroup() %&gt;% 
  mutate(order_priority = fct_relevel(order_priority, levels = c(&quot;Low&quot;,&quot;Medium&quot;,&quot;High&quot;,&quot;Critical&quot;))) %&gt;% 
  ggplot(aes(x = order_priority, y = sales, fill = ship_mode))+
  geom_col(alpha = .7)+
  scale_y_continuous(labels = scales::comma)+
  scale_fill_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  labs(title = &quot;Sales by order priority and delivery contract&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>df_adj %&gt;%
  filter(order_date &gt;= &quot;2014-01-01&quot;,
         sub_category != &quot;Tables&quot;) %&gt;% 
  mutate(profit_mg = round(profit/sales, digits = 4)) %&gt;%
  mutate(order_priority = fct_relevel(order_priority, levels = c(&quot;Low&quot;,&quot;Medium&quot;,&quot;High&quot;,&quot;Critical&quot;)),
         ship_mode = fct_relevel(ship_mode, levels = c(&quot;Standard Class&quot;,&quot;Second Class&quot;,&quot;First Class&quot;,&quot;Same Day&quot;))) %&gt;% 
  ggplot(aes(x = profit_mg))+
  geom_histogram(alpha = .6, binwidth = .1, fill = graph_color)+
  geom_vline(xintercept = 0, lty = 2, color = &quot;red&quot;)+
  scale_x_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;Operational scenarios and profit margin&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)+
  facet_grid(order_priority~ship_mode, scales = &quot;free&quot;)</code></pre>
<p><img src="/post/2020-02-13-exploratory-data-analysis-project-I_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Apparently the operational scenario isn’t a culprit of profit margin. Orders with low priority are shipped within the “Standard Class” delivery contract (and it’s correct). On the other hand, orders with the highest priority aren’t sent by the “Standard Class” delivery contract. Finally, we conclude for this topic that the operational perspective can’t explain much of the profit margin variation, neither generate some recommendation for the company’s directors.</p>
</div>
<div id="conclusions-and-recommendations" class="section level2">
<h2><strong>Conclusions and recommendations</strong></h2>
<p>Considering the business aspects being analysed I come with some recommendations. First, the company has to verify its discount policy, urgently for its “Tablets” sub-category, since a lot of sales turns out to be prejudicial given the discount allowed. Second, it’s important to pay more attention for the Canadian operations, because it’s a prosperous market for the company. Third, the South East Asian market will need some attention in order to catch up with other regions on profit margin.</p>
</div>
]]></content>
        </item>
        
        <item>
            <title>Immigration Data Analysis</title>
            <link>/posts/2020/01/immigration-data-analysis/</link>
            <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
            
            <guid>/posts/2020/01/immigration-data-analysis/</guid>
            <description>Imigration is a central theme on political debates for presidential elections nowadays, creating a lot of controversies and also hope for many people. This social event is increasingly calling more attention, mainly after the Siria conflict and the weakness of the Venezuelan economy, perhaps it’s because it surpreended politicians that didn’t get ready earlier to obtain the benefits that migration flow can lead.
Because of this relevance about the theme, in November 2019 The Economist made a special report about imigration, with the goal of clarify the current situation and present success cases of countries that are obtaining advantages of the imigration flow.</description>
            <content type="html"><![CDATA[


<p>Imigration is a central theme on political debates for presidential elections nowadays, creating a lot of controversies and also hope for many people. This social event is increasingly calling more attention, mainly after the Siria conflict and the weakness of the Venezuelan economy, perhaps it’s because it surpreended politicians that didn’t get ready earlier to obtain the benefits that migration flow can lead.</p>
<p>Because of this relevance about the theme, in November 2019 <a href="https://www.economist.com/printedition/2019-11-16">The Economist</a> made a special report about imigration, with the goal of clarify the current situation and present success cases of countries that are obtaining advantages of the imigration flow. I really enjoyed the theme and the studies that was writen by the correspondents, and then I decided to write a post to present the mainly conclusions of the magazine and access the World Bank Database to explore the theme in depth and generate some insight.</p>
<p>Following is a picture of one of the articles presented in the magazine. This picture is awesome!</p>
<div class="figure" style="text-align: center"><span id="fig:pressure"></span>
<img src="C:/Users/francisco.piccolo/Desktop/R/franciscopiccolo.github.io/images/2020-01-09_imigration_post.png" alt="The Economist" width="476" />
<p class="caption">
Figure 1: The Economist
</p>
</div>
<div id="mainly-conclusions-of-the-report" class="section level2">
<h2><strong>Mainly conclusions of the report</strong></h2>
<ol style="list-style-type: decimal">
<li><p>People that successfully migrate from a poor country to a rich one increase their income between 3 to 6 times, because the rich country has better institutions, capital alocation and modern companies.</p></li>
<li><p>If everone that which a country migraton was allowed to do it, thw world GDP could double.</p></li>
<li><p>Imigrants are more tended to open their own business, because they percept unatended demands easier and also they find alternative solutions to existent problems on the country that they enter.</p></li>
<li><p>The mainly “problem” with imigration is the cultural change, that occurs seedly in specific places, because imigrants tend to cluster in certain locals. This culturar change is the mainly problem with imigration and also the hardest to measure, thus it becomes difficult to be contested by people that favour imigration.</p></li>
<li><p>People are more tolerant to the imigration flow in their countries when the feel that their government are in charge of the frontier, in other words, with a efficient process that only allows the migration of people by the legal process.</p></li>
<li><p>Imigratns often share their wages with their families that have stayed on the country and this income fount is called <a href="https://www.ted.com/talks/dilip_ratha_the_hidden_force_in_global_economics_sending_money_home">remitances</a>. Remitances already is the mainly font of external investment in some poor countries. This flow of money has high potential of impacts, because it goes straight to the pocket of people that are in need and also this investment doesn’t carry the risk of being misplaced by corruption. Remitances represent more than 10% of GDP of 28 countries currently (2019).</p></li>
<li><p>To obtain benefits from the imigration process, the government must be efficient on the integration of people on the labour market, in other to avoid the situation where the imigrant is demanding social care without being contributins to the tax revenue of the government.</p></li>
</ol>
</div>
<div id="themes-relevant-data" class="section level2">
<h2><strong>Theme’s relevant data</strong></h2>
<p>After we understand a little better of the imigration flow, we can analyse some public data in order to generate some new insights about this theme. For this activity, I’ll utilize the World Banck Data API that contain organized data about imigration for almost every country. In this task I’ll use the following packages:</p>
<pre class="r"><code>library(wbstats)        # accessing the World Banck API
library(tidyverse)      # data science and data manipulation packages
library(ggthemes)       # themes for ggplot graphs
library(scales)         # adjusting axis scales
library(sqldf)          # SQL script inside R

theme_graph &lt;- function(){
  theme(
    plot.title = element_text(size = 16),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(face = &quot;italic&quot;, size = 9),
    axis.text = element_text(size = 9),
    axis.title = element_text(face = &quot;italic&quot;, size = 9),
    text = element_text(family = &quot;Times New Roman&quot;),
    strip.background = element_rect(fill = &quot;grey&quot;),
    strip.text = element_text(face = &quot;bold&quot;),
    legend.title = element_blank(),
    legend.position = &quot;bottom&quot;
  )
}
  
graph_color &lt;- &quot;#006666&quot;</code></pre>
</div>
<div id="imigrants-around-the-world" class="section level2">
<h2><strong>Imigrants around the world</strong></h2>
<p>In this topic, we’ll analyse how much imigrants the World Banck accounted in 2015 (the last period that the data was collected), as well the evolution of this population throughout the year and how much this population represents of the country that they leave. To collect the data, you just have to call the World Bank API using the following code:</p>
<pre class="r"><code>migrants_pop &lt;- wbstats::wb(indicator = &quot;SM.POP.TOTL&quot;,
                              country = &quot;countries_only&quot;,
                              startdate = 1980,
                              enddate = 2019)</code></pre>
<p>On the following graph, we can see that the number of imigrants has increased continuously throughout the years, but with peaks in specific periods, for exemple between 1985 and 1990 (probably there was a civil war in an African country). Syria’s turmoil isn’t presented on the data, since probably the effect on imigration flow started in 2016. It can be seeing that the accounted population as imigrants by the institution in 2015 is around 250 millions, beyond the Brazilian population.</p>
<pre class="r"><code>migrants_pop %&gt;% 
  group_by(date) %&gt;% 
  summarise(total_migrants = sum(value)) %&gt;% 
  ggplot(aes(x = date, y = total_migrants, group = 1))+
  geom_line(color = graph_color, size = 1)+
  scale_y_continuous(labels = scales::comma)+
  theme_graph()+
  labs(title = &quot;Continuously growth of imigrant population&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>On the following visualization, we’ll see how much these people that imigrate represents on the country that they step in, considering 2015 and only countries with more than 10 million of people living in. Since this data set has too many countries (215), I’ll select just 20 that have the greatest share of imigrants on their population.</p>
<pre class="r"><code># Coletando dados da população do país
population &lt;- wbstats::wb(indicator = &quot;SP.POP.TOTL&quot;,
                          country = &quot;countries_only&quot;)

population_2015 &lt;- population %&gt;% 
  filter(date == &quot;2015&quot;) %&gt;% 
  mutate(population = value) %&gt;% 
  select(country,population)</code></pre>
<pre class="r"><code># Fazendo o join com o data set de imigrantes
migrants_pop %&gt;% 
  filter(date == &quot;2015&quot;) %&gt;% 
  select(country,value) %&gt;% 
  inner_join(population_2015,by = c(&quot;country&quot; = &quot;country&quot;)) %&gt;% 
  mutate(share_over_population = round(value/population,digits = 4)) %&gt;% 
  filter(population &gt;= 10000000) %&gt;% 
  arrange(desc(share_over_population)) %&gt;% 
  head(20) %&gt;% 
  mutate(country = fct_reorder(country,share_over_population)) %&gt;% 
  ggplot(aes(x = country, y = share_over_population))+
  geom_col(fill = graph_color)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;County rank by imigration share over population&quot;,
       x = &quot;&quot;,
       y = &quot;% of imigrants over the population&quot;)+
  coord_flip()</code></pre>
<p><img src="/post/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="countries-that-lost-more-people-during-the-last-years" class="section level2">
<h2><strong>Countries that lost more people during the last years</strong></h2>
<p>In the following analysis, I’ll use an indicator of the World Bank that shows the net output of people from a country (i.e. imigration less migration) in a given year. In this analysis, I’ll take the data from 2010 to 2017 (the last year of the collect), make the sum of this indicator (net_output) and then filter the countries that have the sum below 0, that will be the countries that had lost people in the considered period. Finnaly, I’ll classify the remaining countries to take the first 20 with more people leaving the country.</p>
<pre class="r"><code>net_migration &lt;- wbstats::wb(indicator = &quot;SM.POP.NETM&quot;,
                         country = &quot;countries_only&quot;,
                         startdate = 2010,
                         enddate = 2019) %&gt;% 
  filter(date &gt; &quot;2000&quot;) %&gt;% 
  select(country,value,date)

# Selecionando os países que tiveram saída de pessoas no período
net_migration %&gt;% 
  group_by(country) %&gt;% 
  summarise(
    total_net_migration = sum(value)
  ) %&gt;% 
  filter(total_net_migration &lt; 0) %&gt;% 
  select(country,total_net_migration) %&gt;% 
  arrange(total_net_migration) %&gt;% 
  head(20) -&gt; country_with_neg_net_migration

net_migration %&gt;% 
  inner_join(country_with_neg_net_migration, by = c(&quot;country&quot; = &quot;country&quot;)) %&gt;% 
  group_by(country) %&gt;% 
  summarise(net_migration = sum(value)) %&gt;% 
  mutate(country = fct_reorder(country,net_migration,.desc = T)) %&gt;% 
  ggplot(aes(x = country, y = net_migration))+
  geom_col(fill = graph_color)+
  scale_y_continuous(labels = scales::comma)+
  coord_flip()+
  theme_graph()+
  labs(title = &quot;Countries with the greatest output of people from their country&quot;,
       subtitle = &quot;Net output&quot;,
       x = &quot;&quot;,
       y = &quot;Net migration&quot;,
       caption = &quot;From 2010 until 2017&quot;)</code></pre>
<p><img src="/post/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The net output is interesting but the relative net output (%) can bring more clarity to the problem the country is facing. Índice, China and Bangladesh obviously are in the preceding graph because they have large population. On the following visualization I’ll re do the preceding graph, however I’ll sellect the countries considering the net output relative to the population.</p>
<pre class="r"><code>net_migration %&gt;% 
  group_by(country) %&gt;% 
  summarise(
    total_net_migration = sum(value)
  ) %&gt;% 
  filter(total_net_migration &lt; 0) %&gt;% 
  select(country,total_net_migration) %&gt;% 
  inner_join(population %&gt;% 
  filter(date == &quot;2018&quot;,
         value &gt; 5000000) %&gt;% 
  mutate(population = value) %&gt;% 
  select(country,population),by = c(&quot;country&quot; = &quot;country&quot;)) %&gt;% 
  mutate(share_over_population = round(total_net_migration/population,digits = 4)) %&gt;% 
  arrange(share_over_population) %&gt;% 
  head(20) %&gt;%
  mutate(country = fct_reorder(country,share_over_population,.desc = T)) %&gt;% 
  ggplot(aes(x = country, y = share_over_population))+
  geom_col(fill = graph_color)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;Share of population leaving the country&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)+
  coord_flip()</code></pre>
<p><img src="/post/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>As this indicator has more recent data, Syria is presented with its outflow well represented on the graph, because its civil war that is occuring since 2012. Venezuela also is well represented because of its economic crisis.</p>
</div>
<div id="remitances-sent-to-the-countries" class="section level2">
<h2><strong>Remitances sent to the countries</strong></h2>
<p>Remitances, as showed on the articles, are the monetary values sent by people that migrated to their families that stayed on the country. Let’s see how much this investment represents on the country’s GDP. According with one of the articles from the Magazine, 28 countries have a share o remitances greater than 10% of their GDP. The World Bank data presents 29 countries with this mark. The following code show this output:</p>
<pre class="r"><code>remitances_over_gdp &lt;- wbstats::wb(indicator = &quot;BX.TRF.PWKR.DT.GD.ZS&quot;,
                         country = &quot;countries_only&quot;)

remitances_over_gdp %&gt;% 
  filter(date == &quot;2018&quot;,
         country != &quot;Lesotho&quot;,
         value &gt; 10) %&gt;%
  mutate(remitance_over_gdp = paste(round(value,digits = 0),&quot;%&quot;,sep = &quot;&quot;)) %&gt;% 
  arrange(desc(value)) %&gt;% 
  select(date,country,remitance_over_gdp)</code></pre>
<pre><code>##    date                country remitance_over_gdp
## 1  2018                  Tonga                41%
## 2  2018        Kyrgyz Republic                33%
## 3  2018                  Haiti                33%
## 4  2018             Tajikistan                29%
## 5  2018                  Nepal                29%
## 6  2018            El Salvador                21%
## 7  2018               Honduras                20%
## 8  2018                  Samoa                18%
## 9  2018     West Bank and Gaza                17%
## 10 2018                Moldova                16%
## 11 2018                Jamaica                16%
## 12 2018                 Kosovo                16%
## 13 2018             Uzbekistan                15%
## 14 2018       Marshall Islands                14%
## 15 2018                Liberia                14%
## 16 2018                Comoros                14%
## 17 2018            Gambia, The                12%
## 18 2018            Yemen, Rep.                12%
## 19 2018                Lebanon                12%
## 20 2018              Guatemala                12%
## 21 2018                Armenia                12%
## 22 2018             Cabo Verde                12%
## 23 2018                Georgia                12%
## 24 2018              Nicaragua                11%
## 25 2018                Ukraine                11%
## 26 2018             Montenegro                11%
## 27 2018                 Jordan                11%
## 28 2018 Bosnia and Herzegovina                11%
## 29 2018            Philippines                10%
## 30 2018       Egypt, Arab Rep.                10%
## 31 2018                Senegal                10%</code></pre>
<p>As well as the remitances share over GDP, it’s important to see if this capital inflow is acyclic (an assumption commented in one of the articles of the special report). This characteristic indicates that the flow isn’t correlated with the GDP variation (or GDP per capita variation). The hipothesis is that the persons that send they money, will send more when the economic situation on his/her familiy cuntry deteriorate. To validate this hipothesis I’ll select some countries and plot a temporal serie for the GDP per capita and another temporal serie for the remitances per capita sent to the selected countries. The selected countries are: Suriname, Somalia, Sierra Leone, Senegal, Sudan, Rwanda, Nepal, Niger, Malawy, Mali, Lybia, Kenya, Ethiopia, Eritrea and Congo.</p>
<p>Collecting the neeeded indicators for this analysis:</p>
<p>Now lets investigate how the two variables perform throughout the years (since 2010). In order to create the ‘remitances per capita’ indicator, I brought the population data and made a join with the remitances data. Next, I made de division (remitances received/population).</p>
<pre class="r"><code>remitances_received %&gt;% 
  mutate(remitances_received = value) %&gt;% 
  select(date,country,remitances_received) %&gt;% 
  filter(country %in% country_view,
         date &gt; 2010) %&gt;% 
  inner_join(population %&gt;% mutate(population = value) %&gt;% 
               select(date,
                      country,
                      population), by = c(&quot;date&quot; = &quot;date&quot;, &quot;country&quot; = &quot;country&quot;)) %&gt;% 
  mutate(remitance_pc = round(remitances_received/population,digits = 4)) %&gt;% 
  group_by(date) %&gt;%
  summarise(remitances_received_pc = sum(remitance_pc)) %&gt;% 
  ggplot(aes(x = date, y = remitances_received_pc, group = 1))+
  geom_line(color = graph_color, size = 1)+
  scale_y_continuous(labels = scales::comma)+
  theme_graph()+
  labs(title = &quot;Remitances per capita (US$)&quot;,
       subtitle = &quot;Growth on the analysed period&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>gdp_pc %&gt;% 
  mutate(gdp_pc = value) %&gt;% 
  select(date,country,gdp_pc) %&gt;% 
  group_by(date) %&gt;% 
  summarise(gdp_pc = sum(gdp_pc)) %&gt;% 
  inner_join(remitances_received %&gt;% select(date,country), by = c(&quot;date&quot; = &quot;date&quot;)) %&gt;% 
  filter(country %in% country_view,
         date &gt; 2010) %&gt;% 
  ggplot(aes(x = date, y = gdp_pc, group = 1))+
  geom_line(color = graph_color, size = 1)+
  scale_y_continuous(labels = scales::comma)+
  theme_graph()+
  labs(title = &quot;GDP per capita (US$)&quot;,
       subtitle = &quot;Drop on the analysed period&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>It’s possible to afirm that this money inflow, sent by imigrants to their families that stay on the country, is acyclic and probably support a lot the poorest that are in need.</p>
</div>
<div id="conclusions" class="section level2">
<h2><strong>Conclusions</strong></h2>
<p>Although the simplicity of these analysed, it was possible to validate the statistics of the magazine reports and generate some insights beyone that. Hope you have enjoyed.</p>
</div>
]]></content>
        </item>
        
        <item>
            <title>World Bank database access throughout R</title>
            <link>/posts/2019/11/world-bank-database-access-throughout-r/</link>
            <pubDate>Sat, 30 Nov 2019 00:00:00 +0000</pubDate>
            
            <guid>/posts/2019/11/world-bank-database-access-throughout-r/</guid>
            <description>The World Bank was created in 1944 at the Bretton Woords Conferences and its goal is to fight poverty and inequality, throughout investments and support for developing countries. The institution’s mission can be found on its webpage and the first target seeing at the is reduce to 3% the share of people living on extreme poverty until 2030. When the World Bank was created, one of its first responsibilities was to support Europe’s infrastructure reconstrution that had been devastated by the second world war 1.</description>
            <content type="html"><![CDATA[


<p>The World Bank was created in 1944 at the Bretton Woords Conferences and its goal is to fight poverty and inequality, throughout investments and support for developing countries. The institution’s mission can be found on its <a href="https://www.worldbank.org/en/who-we-are">webpage</a> and the first target seeing at the is reduce to 3% the share of people living on extreme poverty until 2030. When the World Bank was created, one of its first responsibilities was to support Europe’s infrastructure reconstrution that had been devastated by the second world war <a href="https://www.economist.com/the-economist-explains/2019/04/09/how-does-the-world-bank-work">1</a>.</p>
<p>In order to reach its purpose, the institution relies on a huge database to support the development of research for countries in need of advise and investments. This database was built and it keeps being by the institution and it can be found <a href="https://data.worldbank.org/indicator">here</a>. Anyone can access this database, including R users, that can count with an API to access this database easier. The World Bank API is presented on the R package “wbstats” and this package will be the focus of this post.</p>
<p>To explore this package in this post, I’ll analyse some indicators controlled by the World Bank. Just to note, this package was created by Jesse Piburn in 2018 january. The indicators that I’ll analyse will be GDP, Gini, Debt and Trade. As well as “wbstats” package, I’ll use other packages to execute the datamanipulation task.</p>
<pre class="r"><code>library(wbstats)
library(tidyverse)
library(ggthemes)
library(ggalt)
library(scales)

theme_graph &lt;- function(){
  theme(
    plot.title = element_text(size = 16),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(face = &quot;italic&quot;, size = 9),
    axis.text = element_text(size = 9),
    axis.title = element_text(face = &quot;italic&quot;, size = 9),
    text = element_text(family = &quot;Times New Roman&quot;),
    strip.background = element_rect(fill = &quot;grey&quot;),
    strip.text = element_text(face = &quot;bold&quot;),
    legend.title = element_blank(),
    legend.position = &quot;bottom&quot;
  )
}
  
graph_color &lt;- &quot;#006666&quot;</code></pre>
<p>The “wb” function of the package seems to be the most important, it does the dowload of the data throughout the API e the output is a data.frame object. The main parameters of the function are: “indicator”, “country”, “startdate” and “enddate”. The indicador is a code that can be found on the World Bank URL page, for example, if you wanna take GDP per capita indicator in 2010 dolar, the URL is (<a href="https://data.worldbank.org/indicator/NY.GDP.MKTP.KD?view=chart" class="uri">https://data.worldbank.org/indicator/NY.GDP.MKTP.KD?view=chart</a>) and the code to insert on the “wb” function is <strong>“NY.GDP.MKTP.KD”</strong>. The country parameter can be one country in the iso2c format (e.g. “BR”, “AF” or “US”), a region (e.g. “1A” = Arabic countries“), all countries and regions using”all" or just countries using “countries_only”. Startdate and enddate parameters requires the data that you wanna the indicador, like from data &gt; to date, and these two parameters can be replaced by the parameter “mrv” that means “most recent value” and this parameter will require an integer to indicate how many data points will be dowloaded (e.g. mrv = 5 means the 5 most recent values of the indicator).</p>
<p>Lets see some indicators of the institution to analyse how the database is organized. To start, I’ll extract GDP per capita indicator at 2010 dolar values for the latim america region. In case you have some doubts about the country name, you can check at the indicator webpage.</p>
<pre class="r"><code># extracting the data and saving as a data.frame
df_gdp_pc &lt;- wbstats::wb(indicator = &quot;NY.GDP.PCAP.CD&quot;,
                         country = &quot;all&quot;,
                         startdate = 1990,
                         enddate = 2010)

# filtering the required countries
df_gdp_pc %&gt;% 
  select(country,value,date) %&gt;% 
  filter(country %in% c(&quot;Brazil&quot;,&quot;Chile&quot;,&quot;Ecuador&quot;,
                        &quot;Venezuela, RB&quot;,&quot;Colombia&quot;,
                        &quot;Peru&quot;,&quot;Argentina&quot;,&quot;Bolivia&quot;,
                        &quot;Uruguay&quot;,&quot;Paraguay&quot;)) %&gt;% 
  arrange(date) %&gt;% 
  head(10)</code></pre>
<pre><code>##          country     value date
## 1      Argentina 4333.4830 1990
## 2        Bolivia  709.0597 1990
## 3         Brazil 3100.2805 1990
## 4          Chile 2494.5257 1990
## 5       Colombia 1445.3284 1990
## 6        Ecuador 1489.5295 1990
## 7       Paraguay 1376.1647 1990
## 8           Peru 1196.5869 1990
## 9        Uruguay 2990.3642 1990
## 10 Venezuela, RB 2475.3805 1990</code></pre>
<p>Having the data organized as a data.frame, lets see the GDP per capita of the selected countries between 1990 and 2010.</p>
<pre class="r"><code>df_gdp_pc %&gt;% 
  select(country,value,date) %&gt;% 
  filter(country %in% c(&quot;Brazil&quot;,&quot;Chile&quot;,&quot;Ecuador&quot;,&quot;Venezuela, RB&quot;
                        ,&quot;Colombia&quot;,&quot;Peru&quot;,&quot;Argentina&quot;
                        ,&quot;Bolivia&quot;,&quot;Uruguay&quot;,&quot;Paraguay&quot;)) %&gt;% 
  filter(date %in% c(&quot;1990&quot;,&quot;2010&quot;)) %&gt;% 
  mutate(country = fct_reorder(country,value)) %&gt;% 
  ggplot(aes(x=country,y=value))+
  geom_col(fill=&quot;dark orange&quot;)+
  coord_flip()+
  theme_graph()+
  labs(title = &quot;GDP per Capita&quot;,
       subtitle = &quot;2010 US$ conversion&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)+
  facet_grid(~date)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>As well as the GDP per capital, it’s interesting to visualize the growth of this indicator. On the following graphs we’ll verify which are the 30 countries that had the greatest variation of this indicator and also the 30 countries with the lowest variation, considering the same period of 1990 to 2010. For this analyse I’ve made a simple “case when” statement to classify the countries in groups of “low income”, “middle income” and “high income”.</p>
<pre class="r"><code>df_gdp_pc_2 &lt;- wbstats::wb(indicator = &quot;NY.GDP.PCAP.CD&quot;,
                           country = &quot;countries_only&quot;,
                           startdate = 1990,
                           enddate = 2010)

df_gdp_pc_2 %&gt;%
  select(date,
         country,
         value) %&gt;% 
  filter(date %in% c(&quot;1990&quot;,max(date))) %&gt;% 
  mutate(gdp_90 = ifelse(date == &quot;1990&quot;,value,0),
         gdp_10 = ifelse(date == &quot;2010&quot;,value,0)) %&gt;% 
  group_by(country) %&gt;% 
  summarise(gdp_90 = sum(gdp_90),
            gdp_10 = sum(gdp_10)) %&gt;% 
  filter(gdp_90 &gt; 0 &amp; gdp_10 &gt; 0) %&gt;% 
  mutate(delta = round(1-(gdp_90/gdp_10),digits = 4),
         income_range = ifelse(gdp_10 &lt;= 5000,&quot;low&quot;,
                        ifelse(gdp_10 &lt;= 15000,&quot;middle&quot;,
                        ifelse(gdp_10 &lt;= 45000,&quot;up_middle&quot;,
                        ifelse(gdp_10 &lt;= 60000,&quot;high&quot;,
                        ifelse(gdp_10 &lt;= 90000,&quot;up_high&quot;,
                        &quot;ultra_rich&quot;)))))) %&gt;% 
  filter(income_range == &quot;low&quot;) %&gt;% 
  arrange(desc(delta)) %&gt;% 
  head(20) %&gt;% 
  ggplot()+
  ggalt::geom_dumbbell(aes(y = fct_reorder(country, gdp_10),
                           x = gdp_90,
                           xend = gdp_10),
  colour = &quot;#3E39CD&quot;, size = 1,
  colour_x = &quot;#BD274A&quot;, colour_xend = &quot;#60AF1A&quot;)+
  scale_x_continuous(labels = scales::comma)+ # separando por vírgula (&#39;000)
  theme_graph()+
  labs(title = &quot;20 countries with the highest per capita GDP variation&quot;,
       subtitle = &quot;Group: Low income (GDP per capita &lt;= 5.000 US$ (2010))&quot;)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The preceding code can be replied, changing the filter “income_range == low”, to middle, up_middle, high and up_high income.</p>
<pre class="r"><code>df_gdp_pc_2 %&gt;%
  select(date,
         country,
         value) %&gt;% 
  filter(date %in% c(&quot;1990&quot;,max(date))) %&gt;% 
  mutate(gdp_90 = ifelse(date == &quot;1990&quot;,value,0),
         gdp_10 = ifelse(date == &quot;2010&quot;,value,0)) %&gt;% 
  group_by(country) %&gt;% 
  summarise(gdp_90 = sum(gdp_90),
            gdp_10 = sum(gdp_10)) %&gt;% 
  filter(gdp_90 &gt; 0 &amp; gdp_10 &gt; 0) %&gt;% 
  mutate(delta = round(1-(gdp_90/gdp_10),digits = 4),
         income_range = ifelse(gdp_10 &lt;= 5000,&quot;low&quot;,
                        ifelse(gdp_10 &lt;= 15000,&quot;middle&quot;,
                        ifelse(gdp_10 &lt;= 45000,&quot;up_middle&quot;,
                        ifelse(gdp_10 &lt;= 60000,&quot;high&quot;,
                        ifelse(gdp_10 &lt;= 90000,&quot;up_high&quot;,
                        &quot;ultra_rich&quot;)))))) %&gt;% 
  filter(income_range == &quot;middle&quot;) %&gt;% 
  arrange(desc(delta)) %&gt;% 
  head(20) %&gt;% 
  ggplot()+
  ggalt::geom_dumbbell(aes(y = fct_reorder(country,gdp_10),
                           x = gdp_90,
                           xend = gdp_10),
  colour = &quot;#3E39CD&quot;, size = 1,
  colour_x = &quot;#BD274A&quot;, colour_xend = &quot;#60AF1A&quot;)+
  scale_x_continuous(labels = comma)+ # separando por vírgula (&#39;000)
  theme_graph()+
  labs(title = &quot;20 countries with the highest per capita GDP variation&quot;,
       subtitle = &quot;Group: Middle income (GDP per capita &lt;= 15.000 US$ (2010))&quot;)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>df_gdp_pc_2 %&gt;%
  select(date,
         country,
         value) %&gt;% 
  filter(date %in% c(&quot;1990&quot;,max(date))) %&gt;% 
  mutate(gdp_90 = ifelse(date == &quot;1990&quot;,value,0),
         gdp_10 = ifelse(date == &quot;2010&quot;,value,0)) %&gt;% 
  group_by(country) %&gt;% 
  summarise(gdp_90 = sum(gdp_90),
            gdp_10 = sum(gdp_10)) %&gt;% 
  filter(gdp_90 &gt; 0 &amp; gdp_10 &gt; 0) %&gt;% 
  mutate(delta = round(1-(gdp_90/gdp_10),digits = 4),
         income_range = ifelse(gdp_10 &lt;= 5000,&quot;low&quot;,
                        ifelse(gdp_10 &lt;= 15000,&quot;middle&quot;,
                        ifelse(gdp_10 &lt;= 45000,&quot;up_middle&quot;,
                        ifelse(gdp_10 &lt;= 60000,&quot;high&quot;,
                        ifelse(gdp_10 &lt;= 90000,&quot;up_high&quot;,
                        &quot;ultra_rich&quot;)))))) %&gt;% 
  filter(income_range == &quot;up_middle&quot;) %&gt;% 
  arrange(desc(delta)) %&gt;% 
  head(20) %&gt;% 
  ggplot()+
  ggalt::geom_dumbbell(aes(y = fct_reorder(country,gdp_10),
                           x = gdp_90,
                           xend = gdp_10),
  colour = &quot;#3E39CD&quot;, size = 1,
  colour_x = &quot;#BD274A&quot;, colour_xend = &quot;#60AF1A&quot;)+
  scale_x_continuous(labels = comma)+ # separando por vírgula (&#39;000)
  theme_graph()+
  labs(title = &quot;20 countries with the highest per capita GDP variation&quot;,
       subtitle = &quot;Group: Up Middle income (GDP per capita &lt;= 45.000 US$ (2010))&quot;)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>df_gdp_pc_2 %&gt;%
  select(date,
         country,
         value) %&gt;% 
  filter(date %in% c(&quot;1990&quot;,max(date))) %&gt;% 
  mutate(gdp_90 = ifelse(date == &quot;1990&quot;,value,0),
         gdp_10 = ifelse(date == &quot;2010&quot;,value,0)) %&gt;% 
  group_by(country) %&gt;% 
  summarise(gdp_90 = sum(gdp_90),
            gdp_10 = sum(gdp_10)) %&gt;% 
  filter(gdp_90 &gt; 0 &amp; gdp_10 &gt; 0) %&gt;% 
  mutate(delta = round(1-(gdp_90/gdp_10),digits = 4),
         income_range = ifelse(gdp_10 &lt;= 5000,&quot;low&quot;,
                        ifelse(gdp_10 &lt;= 15000,&quot;middle&quot;,
                        ifelse(gdp_10 &lt;= 45000,&quot;up_middle&quot;,
                        ifelse(gdp_10 &lt;= 60000,&quot;high&quot;,
                        ifelse(gdp_10 &lt;= 90000,&quot;up_high&quot;,
                        &quot;ultra_rich&quot;)))))) %&gt;% 
  filter(income_range == &quot;high&quot;) %&gt;% 
  arrange(desc(delta)) %&gt;% 
  head(20) %&gt;% 
  ggplot()+
  ggalt::geom_dumbbell(aes(y = fct_reorder(country,gdp_10),
                           x = gdp_90,
                           xend = gdp_10),
  colour = &quot;#3E39CD&quot;, size = 1,
  colour_x = &quot;#BD274A&quot;, colour_xend = &quot;#60AF1A&quot;)+
  scale_x_continuous(labels = comma)+ # separando por vírgula (&#39;000)
  theme_graph()+
  labs(title = &quot;20 countries with the highest per capita GDP variation&quot;,
       subtitle = &quot;Group: High (GDP per capita &lt;= 60.000 US$ (2010))&quot;)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>df_gdp_pc_2 %&gt;%
  select(date,
         country,
         value) %&gt;% 
  filter(date %in% c(&quot;1990&quot;,max(date))) %&gt;% 
  mutate(gdp_90 = ifelse(date == &quot;1990&quot;,value,0),
         gdp_10 = ifelse(date == &quot;2010&quot;,value,0)) %&gt;% 
  group_by(country) %&gt;% 
  summarise(gdp_90 = sum(gdp_90),
            gdp_10 = sum(gdp_10)) %&gt;% 
  filter(gdp_90 &gt; 0 &amp; gdp_10 &gt; 0) %&gt;% 
  mutate(delta = round(1-(gdp_90/gdp_10),digits = 4),
         income_range = ifelse(gdp_10 &lt;= 5000,&quot;low&quot;,
                        ifelse(gdp_10 &lt;= 15000,&quot;middle&quot;,
                        ifelse(gdp_10 &lt;= 45000,&quot;up_middle&quot;,
                        ifelse(gdp_10 &lt;= 60000,&quot;high&quot;,
                        ifelse(gdp_10 &lt;= 90000,&quot;up_high&quot;,
                        &quot;ultra_rich&quot;)))))) %&gt;% 
  filter(income_range == &quot;up_high&quot;) %&gt;% 
  arrange(desc(delta)) %&gt;% 
  head(20) %&gt;% 
  ggplot()+
  ggalt::geom_dumbbell(aes(y = fct_reorder(country,gdp_10),
                           x = gdp_90,
                           xend = gdp_10),
  colour = &quot;#3E39CD&quot;, size = 1,
  colour_x = &quot;#BD274A&quot;, colour_xend = &quot;#60AF1A&quot;)+
  scale_x_continuous(labels = comma)+ # separando por vírgula (&#39;000)
  theme_graph()+
  labs(title = &quot;20 countries with the highest per capita GDP variation&quot;,
       subtitle = &quot;Group: Up High income (GDP per capita &lt;= 90.000 US$ (2010))&quot;)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Now lets see, for the same country (i.e. Brazil), the variety of GDP indicators counted by the World Bank.</p>
<ol style="list-style-type: decimal">
<li><p>GDP per capita (PPP)</p></li>
<li><p>GDP per Capita (LCU - Local currency unit)</p></li>
<li><p>GDP per Capita (2010 US$)</p></li>
<li><p>GDP per Capita (current US$)</p></li>
</ol>
<p>When you want bring more than one indicator, you just have to list then as a list. See the following code as an example:</p>
<pre class="r"><code>df_gdp_pc_3 &lt;- wbstats::wb(indicator = c(&quot;NY.GDP.PCAP.PP.CD&quot;,
                                         &quot;NY.GDP.PCAP.CN&quot;,
                                         &quot;NY.GDP.PCAP.KD&quot;,
                                         &quot;NY.GDP.PCAP.CD&quot;),
                           country = &quot;BRA&quot;,mrv = 10)</code></pre>
<p>Lets plot a time series of these 4 indicators.</p>
<pre class="r"><code>df_gdp_pc_3 %&gt;% 
  ggplot(aes(x=date,y=value,group=indicator,color=indicator))+
  geom_line(size=1)+
  geom_point()+
  scale_y_continuous(labels = scales::comma)+
  theme_graph()+
  labs(title = &quot;Brazil GDP in 4 different visions&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Continuing with the analysis about economic indicators of the latin america region, lets see the government debt in proportion of the GDP. For this indicator, I’ll bring the last 20 result set starting from the most recent, using the “mrv” parameter.</p>
<pre class="r"><code>df_gov_debt &lt;- wbstats::wb(country = &quot;countries_only&quot;,
                           indicator = &quot;GC.DOD.TOTL.GD.ZS&quot;,
                           mrv = 20)</code></pre>
<p>We can see which latin america country increased its spenditures in proportion of the GDP more. This can occur by an increase in spenditures or by a decrease in the GDP.</p>
<pre class="r"><code>df_gov_debt %&gt;% 
  filter(country %in% c(&quot;Brazil&quot;,&quot;Chile&quot;,&quot;Ecuador&quot;,&quot;Venezuela, RB&quot;,
                        &quot;Colombia&quot;,&quot;Peru&quot;,&quot;Argentina&quot;,&quot;Bolivia&quot;,
                        &quot;Uruguay&quot;,&quot;Paraguay&quot;)) %&gt;% 
  select(country,
         date,
         value) %&gt;% 
  mutate(country = factor(country,
                          levels = c( &quot;Brazil&quot;,
                                      &quot;Bolivia&quot;,
                                      &quot;Colombia&quot;,
                                      &quot;Uruguay&quot;,
                                      &quot;Peru&quot;))) %&gt;% # reordenando os países
  ggplot(aes(x=date,y=value,group=country,color=country))+
  geom_line(size=1)+
  geom_point()+
  theme_graph()+
  labs(title = &quot;Share of debt over GDP&quot;,
       subtitle = &quot;&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Now lets see which latin america country have the greatest share of export and import over GDP. This indicator is interesting because it shows the comercial opennes of a country. We can note that Brazil and Argentina are the closest countries of the region.</p>
<pre class="r"><code>wbstats::wb(indicator = &quot;NE.TRD.GNFS.ZS&quot;,country = &quot;countries_only&quot;) %&gt;% 
  filter(country %in% c(&quot;Brazil&quot;,&quot;Chile&quot;,&quot;Ecuador&quot;,&quot;Venezuela, RB&quot;,
                        &quot;Colombia&quot;,&quot;Peru&quot;,&quot;Argentina&quot;,&quot;Bolivia&quot;,
                        &quot;Uruguay&quot;,&quot;Paraguay&quot;)) %&gt;%
  select(country,
         date,
         value) %&gt;% 
  mutate(country = fct_reorder(country,value,.desc = T)) %&gt;% 
  ggplot(aes(x=date,y=value,group=country,color=country))+
  geom_line(size=1,alpha=.6)+
  geom_point()+
  scale_x_discrete(breaks = c(seq(1970,2020,by = 10)))+
  theme_graph()+
  labs(title = &quot;% Exp + Imp over GDP&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>And to finish, lets see the Gini index.</p>
<pre class="r"><code>wbstats::wb(indicator = &quot;SI.POV.GINI&quot;,country = &quot;countries_only&quot;,mrv = 10) %&gt;% 
  filter(country %in% c(&quot;Brazil&quot;,&quot;Chile&quot;,&quot;Ecuador&quot;,&quot;Venezuela, RB&quot;,
                        &quot;Colombia&quot;,&quot;Peru&quot;,&quot;Argentina&quot;,
                        &quot;Bolivia&quot;,&quot;Uruguay&quot;,&quot;Paraguay&quot;)) %&gt;%
  select(country,
         date,
         value) %&gt;% 
  mutate(country = fct_reorder(country,value,.desc = T)) %&gt;% 
  ggplot(aes(x=date,y=value,group=country,color=country))+
  geom_line(size=1)+
  theme_graph()+
  labs(title = &quot;Latin America Gini index&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2019-11-30-world-bank-database-access-throughout-R_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>That’s it, I hope that with these examples shown it became possible to understand how the “wbstats” package works. I believe that this database can be pretty useful in a variety of projects.</p>
]]></content>
        </item>
        
        <item>
            <title>Residual analysis in Econometric models</title>
            <link>/posts/2019/11/residual-analysis-in-econometric-models/</link>
            <pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate>
            
            <guid>/posts/2019/11/residual-analysis-in-econometric-models/</guid>
            <description>The linear regression model is highly used on the prediction of continuous variables, where is one or more independent variables and one dependent one (the one that it seeks to test an hipothesis about its behavior). This is a pretty simple model to by executed and one of the firsts to be taught in econometric classes. Although its simplicity, in order to reach reliable results, some assumptions are needed, like the following:</description>
            <content type="html"><![CDATA[


<p>The linear regression model is highly used on the prediction of continuous variables, where is one or more independent variables and one dependent one (the one that it seeks to test an hipothesis about its behavior). This is a pretty simple model to by executed and one of the firsts to be taught in econometric classes. Although its simplicity, in order to reach reliable results, some assumptions are needed, like the following:</p>
<ol style="list-style-type: lower-roman">
<li><p>Absence of multicolineatiry between independent variables</p></li>
<li><p>Absence of autocorrelation on the dependent variable</p></li>
<li><p>Absence of pattern on the behavior of the model residuals, in other words, absence of heteroscedasticity</p></li>
<li><p>Normal distribution of residuals</p></li>
</ol>
<p>Having this assumptions satisfied, the model can be executed and the generated conclusions will be reliable to allow decision making. In this post, I’ll analyse the behavior of the residuals from a linear regression model in order to validate the assumptions (iii) and (iv). For this, I’ll use a data set that presents the price of suggarcane as independent variable and the planted area of this product, that will be the dependent variable.</p>
<p>The goal of this example is to analyse, through a simple linear regression model, the suggarcane supply elasticity as a function of the suggarcane price. The hipothesis is that there’s elasticity on the supply, thus the planted area increases in responde to a price increase. But, in order to validade this existence, it’s necessary to check, using a regression model, that these two variables have correlation. Then, the assumptions (iii) and (iv) will support this conclusion.</p>
<p>For this example, I’ll utilize the following R packages:</p>
<pre class="r"><code>library(tidyverse)
library(lmtest)
library(corrplot)
library(readxl)
library(gridExtra)
library(ggthemes)
library(pander)

theme_graph &lt;- function(){
  theme(
    plot.title = element_text(size = 16),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(face = &quot;italic&quot;, size = 9),
    axis.text = element_text(size = 9),
    axis.title = element_text(face = &quot;italic&quot;, size = 9),
    text = element_text(family = &quot;Times New Roman&quot;),
    strip.background = element_rect(fill = &quot;grey&quot;),
    strip.text = element_text(face = &quot;bold&quot;),
    legend.title = element_blank(),
    legend.position = &quot;bottom&quot;
  )
}
  
graph_color &lt;- &quot;#006666&quot;</code></pre>
<pre class="r"><code>dados &lt;- readxl::read_excel(&quot;C:/Users/francisco.piccolo/Desktop/R/franciscopiccolo.github.io/base_de_dados/Econometria_exercicios/autocorrelacao_cana_de_acucar.xlsx&quot;)

# Chaging the name of the field to remove blanck spaces
dados %&gt;% 
  transmute(
    periodo = Período,
    area = Área,
    valor = `Preço da Cana de Açúcar`*100,
    log_area = log(area),
    log_valor = log(valor),
    delta_area = round((area/lag(area,1))-1,digits = 2),
    delta_log_area = round(log_area-lag(log_area,1),digits = 2),
    delta_valor = round((valor/lag(valor,1))-1,digits=2),
    delta_log_valor = round(log_valor-lag(log_valor),digits = 2),
    log_area_precicion = delta_area - delta_log_area,
    log_valor_precision = delta_valor - delta_log_valor
  ) -&gt; dados_2</code></pre>
<p>The data set of this example has 34 rows, with planted area and suggarcane price data. Both fields with log transformation (natural) included. Folliwing there’s a sample of the data set:</p>
<pre class="r"><code>dados_2[sample(nrow(dados_2),10), ]</code></pre>
<pre><code>## # A tibble: 10 x 11
##    periodo  area valor log_area log_valor delta_area delta_log_area delta_valor
##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;
##  1       2    71 11.5      4.26      2.44       1.45           0.9         0.53
##  2       5    72 11.0      4.28      2.39      -0.2           -0.22       -0.01
##  3      23   230 36.0      5.44      3.58       0.67           0.51        0.26
##  4       1    29  7.53     3.37      2.02      NA             NA          NA   
##  5      10    70 19.6      4.25      2.98       0.17           0.15        0.04
##  6      31   171 36.0      5.14      3.58      -0.22          -0.25       -0.12
##  7      27    97 40.1      4.57      3.69      -0.22          -0.25        0.39
##  8      16    99 22.1      4.60      3.09      -0.21          -0.23       -0.06
##  9      21   143 36.9      4.96      3.61      -0.12          -0.12        0.6 
## 10      32   208 46.3      5.34      3.84       0.22           0.2         0.28
## # ... with 3 more variables: delta_log_valor &lt;dbl&gt;, log_area_precicion &lt;dbl&gt;,
## #   log_valor_precision &lt;dbl&gt;</code></pre>
<p>The linear regression model proposed for this problem is defined by the following equation:</p>
<p><span class="math display">\[ lnY_t = \beta_0+\beta_1 (lnX_t) + U_t \]</span></p>
<p>Where:</p>
<p><span class="math inline">\(Y_t\)</span> = planted area on year t, natural log transformed</p>
<p><span class="math inline">\(X_t\)</span> = suggarcane price on year t, natural log transformed</p>
<p><span class="math inline">\(\beta_0\)</span> = Intercept</p>
<p><span class="math inline">\(\beta_1\)</span> = slope</p>
<p><span class="math inline">\(U_t\)</span> = Model residuals</p>
<p>The natural log is used to in order to have variations between periods interpreted as percentual variation. This assumption is made possible only for natural log transformations and this adjustment is needed in problems invonving elasticity, because elasticity is interpreted as a percentual variation in one variable given a percentual variation in another variable.</p>
<p>Following is an example of this property of natural log transformations. You can see that the total variation can be seen as percentual variation after the natural log transformation. For more explanations on this topic you can see <a href="https://people.duke.edu/~rnau/411log.htm">article 1</a> <a href="https://dev.to/rokaandy/logarithmic-transformation-in-linear-regression-models-why-when-3a7c">article 2</a>.</p>
<pre class="r"><code>data.frame(
    &quot;Valor&quot; = c(100,105,112,120),
    &quot;Crescimento&quot; = c(&quot;-&quot;,&quot;5.00%&quot;,&quot;6.67%&quot;,&quot;7.14%&quot;),
    &quot;Log natural&quot; = c(log(100),log(105),log(112),log(120)),
    &quot;Delta&quot; = c(&quot;-&quot;,
                  paste(round(log(105)-log(100),4)*100,&quot;%&quot;,sep = &quot;&quot;),
                  paste(round(log(112)-log(105),4)*100,&quot;%&quot;,sep = &quot;&quot;),
                  paste(round(log(120)-log(112),4)*100,&quot;%&quot;,sep = &quot;&quot;))
) %&gt;% pander()</code></pre>
<table style="width:61%;">
<colgroup>
<col width="11%" />
<col width="19%" />
<col width="19%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Valor</th>
<th align="center">Crescimento</th>
<th align="center">Log.natural</th>
<th align="center">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">100</td>
<td align="center">-</td>
<td align="center">4.605</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="center">105</td>
<td align="center">5.00%</td>
<td align="center">4.654</td>
<td align="center">4.88%</td>
</tr>
<tr class="odd">
<td align="center">112</td>
<td align="center">6.67%</td>
<td align="center">4.718</td>
<td align="center">6.45%</td>
</tr>
<tr class="even">
<td align="center">120</td>
<td align="center">7.14%</td>
<td align="center">4.787</td>
<td align="center">6.9%</td>
</tr>
</tbody>
</table>
<p>Veja como o crescimento percentual dos valores (5.00%, 6.67% e 7.14%) ficam bem próximos do respectivo delta, que é a diferença entre o log natural do número com relação ao log natural do número anterior (e.g. 4.654 - 4.605 = 0.0488). Desta forma, se a equação estiver em logarítmo natural em ambos os lados, pode-se interpretar que a variação percentual na variável explicativa irá gerar uma variação percentual de <span class="math inline">\(\beta_1\)</span> na variável dependente.</p>
<p>It becomes clear that the percentual values of 5.00%, 6.67% and 7.14% are pretty close to the value of the delta field, that represents the differente between the natural log of the number and the natural log of the predecessor value (e.g. 4.654 - 4.605 = 0.0488).</p>
<p>Ao tentar criar esta mesma tabela com outros logarítmos (e.g. base 2 ou base 10), não se alcança os mesmos resultados. Portanto, esta interpretação e transformação só faz sentido com logaritmo natural. When we try to create this same view with others log transformations (e.g. base 2 or 10), we can’t reach the same results. Therefore, this interpretation and transformation make sense only for natural log.</p>
<pre class="r"><code>data.frame(
    &quot;Valor&quot; = c(100,105,112,120),
    &quot;Crescimento&quot; = c(&quot;-&quot;,&quot;5.00%&quot;,&quot;6.67%&quot;,&quot;7.14%&quot;),
    &quot;Log natural&quot; = c(log(100,2),log(105,2),log(112,2),log(120,2)),
    &quot;Delta&quot; = c(&quot;-&quot;,
                  paste(round(log(105,2)-log(100,2),4)*100,&quot;%&quot;,sep = &quot;&quot;),
                  paste(round(log(112,2)-log(105,2),4)*100,&quot;%&quot;,sep = &quot;&quot;),
                  paste(round(log(120,2)-log(112,2),4)*100,&quot;%&quot;,sep = &quot;&quot;))
) %&gt;% pander::pander()</code></pre>
<table style="width:61%;">
<colgroup>
<col width="11%" />
<col width="19%" />
<col width="19%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Valor</th>
<th align="center">Crescimento</th>
<th align="center">Log.natural</th>
<th align="center">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">100</td>
<td align="center">-</td>
<td align="center">6.644</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="center">105</td>
<td align="center">5.00%</td>
<td align="center">6.714</td>
<td align="center">7.04%</td>
</tr>
<tr class="odd">
<td align="center">112</td>
<td align="center">6.67%</td>
<td align="center">6.807</td>
<td align="center">9.31%</td>
</tr>
<tr class="even">
<td align="center">120</td>
<td align="center">7.14%</td>
<td align="center">6.907</td>
<td align="center">9.95%</td>
</tr>
</tbody>
</table>
<pre class="r"><code>data.frame(
    &quot;Valor&quot; = c(100,105,112,120),
    &quot;Crescimento&quot; = c(&quot;-&quot;,&quot;5.00%&quot;,&quot;6.67%&quot;,&quot;7.14%&quot;),
    &quot;Log natural&quot; = c(log(100,10),log(105,10),log(112,10),log(120,10)),
    &quot;Delta&quot; = c(&quot;-&quot;,
                  paste(round(log(105,10)-log(100,10),4)*100,&quot;%&quot;,sep = &quot;&quot;),
                  paste(round(log(112,10)-log(105,10),4)*100,&quot;%&quot;,sep = &quot;&quot;),
                  paste(round(log(120,10)-log(112,10),4)*100,&quot;%&quot;,sep = &quot;&quot;))
) %&gt;% pander::pander()</code></pre>
<table style="width:61%;">
<colgroup>
<col width="11%" />
<col width="19%" />
<col width="19%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Valor</th>
<th align="center">Crescimento</th>
<th align="center">Log.natural</th>
<th align="center">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">100</td>
<td align="center">-</td>
<td align="center">2</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="center">105</td>
<td align="center">5.00%</td>
<td align="center">2.021</td>
<td align="center">2.12%</td>
</tr>
<tr class="odd">
<td align="center">112</td>
<td align="center">6.67%</td>
<td align="center">2.049</td>
<td align="center">2.8%</td>
</tr>
<tr class="even">
<td align="center">120</td>
<td align="center">7.14%</td>
<td align="center">2.079</td>
<td align="center">3%</td>
</tr>
</tbody>
</table>
<p>Now that we understand this property of natural log, it’s possible to validate this property on the data set that will be used on the regression model proposed, for the suggarcane price and suggarcane area planted. The delta_area and delta_value fields presents the percentual variation of one period to the preceding. At delta_log_value and delta_log_area fields is calculated the difference between one period with the preceding and the fields log_area_precision and log_value_precision calculate the difference between delta_area with delta_log_area and delta_value with delta_log_value. The lower these two values, stronger is the property of the natural log transformation explained before. The following table will show the results:</p>
<pre class="r"><code>dados_2</code></pre>
<pre><code>## # A tibble: 34 x 11
##    periodo  area valor log_area log_valor delta_area delta_log_area delta_valor
##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;
##  1       1    29  7.53     3.37      2.02      NA             NA          NA   
##  2       2    71 11.5      4.26      2.44       1.45           0.9         0.53
##  3       3    42 10.1      3.74      2.31      -0.41          -0.53       -0.12
##  4       4    90 11.0      4.50      2.40       1.14           0.76        0.09
##  5       5    72 11.0      4.28      2.39      -0.2           -0.22       -0.01
##  6       6    57 13.2      4.04      2.58      -0.21          -0.23        0.21
##  7       7    44 14.2      3.78      2.65      -0.23          -0.26        0.07
##  8       8    61 21.0      4.11      3.04       0.39           0.33        0.48
##  9       9    60 18.8      4.09      2.94      -0.02          -0.02       -0.1 
## 10      10    70 19.6      4.25      2.98       0.17           0.15        0.04
## # ... with 24 more rows, and 3 more variables: delta_log_valor &lt;dbl&gt;,
## #   log_area_precicion &lt;dbl&gt;, log_valor_precision &lt;dbl&gt;</code></pre>
<p>After this explanation, we can continue with the regression model development. The variables will always be presented with the natural log transformation applied. The following graph shows the behavior of the studied variables and the regression line.</p>
<pre class="r"><code>dados_2 %&gt;% 
  ggplot(aes(x=log_area,y=log_valor))+
  geom_point()+
  geom_smooth(method = &quot;lm&quot;,se = T)+
  theme_graph()+
  labs(
    title = &quot;Area planted and suggarcane price relation&quot;,
    subtitle= &quot;There&#39;s an elasticity &quot;,
    x = &quot;Planted area (ln)&quot;,
    y = &quot;Market price (ln)&quot;
  )</code></pre>
<p><img src="/post/2019-11-20-residual-analysis-in-econometric-models_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The variable shows linear correlation, meaning that there’s supply elasticity. Following is the summary of the regression model:</p>
<pre class="r"><code>model &lt;- lm(log_area~log_valor,data = dados_2)

summary(model) %&gt;% pander()</code></pre>
<table style="width:89%;">
<colgroup>
<col width="25%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">1.642</td>
<td align="center">0.3534</td>
<td align="center">4.645</td>
<td align="center">5.559e-05</td>
</tr>
<tr class="even">
<td align="center"><strong>log_valor</strong></td>
<td align="center">0.9706</td>
<td align="center">0.1106</td>
<td align="center">8.773</td>
<td align="center">5.031e-10</td>
</tr>
</tbody>
</table>
<table style="width:88%;">
<caption>Fitting linear model: log_area ~ log_valor</caption>
<colgroup>
<col width="20%" />
<col width="30%" />
<col width="12%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">34</td>
<td align="center">0.3088</td>
<td align="center">0.7063</td>
<td align="center">0.6972</td>
</tr>
</tbody>
</table>
<p>The regression equation generated by the model is defined as: <span class="math inline">\(lnY_t = 1.64 + 0.97 (lnX_t) + U_t\)</span></p>
<p>This equation indicates that the planted area of suggarcane would be of <span class="math inline">\(e^{1.64} = 5.15\)</span> (ha) if the price was 0. As the price increase in 1%, the impact on the planted area will be of 0.97%.</p>
<p>The r² reach 70.6%, but this isn’t enough to make conclusions about the model. Its necessary to analyse the model premissed about the residuals to ensure that the model is reliable.</p>
<pre class="r"><code>residual &lt;- residuals(model)</code></pre>
<pre class="r"><code>df &lt;- cbind(dados_2, residual)</code></pre>
<p>The next graph shows the residuals distribution in order to validate if they’re normally distributed. Both histogram and quantil-quantil plot are good choices to validate this assumption.</p>
<pre class="r"><code>df %&gt;% 
  ggplot(aes(x=residual))+
  geom_histogram(binwidth = .05,alpha=.4,fill=&quot;dark orange&quot;)+
  theme_graph()+
  labs(
    title = &quot;Histograma dos resíduos&quot;,
    subtitle = &quot;Aparente normalidade dos dados&quot;,
    x = &quot;&quot;,
    y = &quot;&quot;
  )</code></pre>
<p><img src="/post/2019-11-20-residual-analysis-in-econometric-models_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>df %&gt;% 
  ggplot(aes(sample=residual))+
  geom_qq()+
  theme_graph()+
  labs(title = &quot;QQ-Plot dos resíduos&quot;,
       subtitle = &quot;Corrobora a suposta normalidade dos dados&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/post/2019-11-20-residual-analysis-in-econometric-models_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Despite the graph visualization being a good bet to analyse departure from normality, it’s also important to perform formal tests like Durbin-Watson, Breusch-Godfrey and Ljung-Box Qtest. The Durbin-Watson (DW) will be the first performed:</p>
<pre class="r"><code>lmtest::dwtest(model) %&gt;% pander::pander()</code></pre>
<table style="width:81%;">
<caption>Durbin-Watson test: <code>model</code></caption>
<colgroup>
<col width="23%" />
<col width="20%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Test statistic</th>
<th align="center">P value</th>
<th align="center">Alternative hypothesis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1.291</td>
<td align="center">0.009801 * *</td>
<td align="center">true autocorrelation is
greater than 0</td>
</tr>
</tbody>
</table>
<p>The DW value was of 1.2912 and this value will be used to generate conclusions for the test. As well as the DW value, its necessary to have the DL and DU intervals that will be compared with the DW value. The DL and DU values can be found at this <a href="http://www.portalaction.com.br/analise-de-regressao/33-diagnostico-de-independencia">table</a>. To find DL and DU values at this table, take the number of points of your data set (n = 33), the significance level (i.e. 0.05) and the degrees of freedom (i.e. 1), then we have:</p>
<p><strong>DL</strong> = 1.35</p>
<p><strong>DU</strong> = 1.49</p>
<p>With a DW of 1.2912, above 0 and under the DL value, we can conclude that the residuals are independent. Thus, we can ensure that residuals are both normally distributed and independent, then the model is reliable on what it shows (there’s elasticity on the suggarcane supply given a price variation).</p>
]]></content>
        </item>
        
        <item>
            <title>Connecting R on Amazon Redshift Database</title>
            <link>/posts/2019/10/connecting-r-on-amazon-redshift-database/</link>
            <pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate>
            
            <guid>/posts/2019/10/connecting-r-on-amazon-redshift-database/</guid>
            <description>IntroductionIn this tutorial I’ll show a step by step on how to access Amazon Redshift Database throughout R. This task will be very useful if you prefer use R intead of the traditional Excel to execute exploratory data analysis on your company. The goal of this tutorial is to access the database from R, bring a data.frame as a result set and then create your analysis.
Step 1Dowload the JDBC driver from Amazon.</description>
            <content type="html"><![CDATA[


<div id="introduction" class="section level2">
<h2><strong>Introduction</strong></h2>
<p>In this tutorial I’ll show a step by step on how to access Amazon Redshift Database throughout R. This task will be very useful if you prefer use R intead of the traditional Excel to execute exploratory data analysis on your company. The goal of this tutorial is to access the database from R, bring a data.frame as a result set and then create your analysis.</p>
</div>
<div id="step-1" class="section level2">
<h2><strong>Step 1</strong></h2>
<p>Dowload the JDBC driver from Amazon. This file will be needed to allow the connection of one tool (i.e. R Studio) to the Amazon cluster that you have access. In this <a href="https://docs.aws.amazon.com/pt_br/redshift/latest/mgmt/configure-jdbc-connection.html#obtain-jdbc-url">link</a> there’s the files indicated for each version of the JDBC.</p>
<p>Ps: If at the end of the role process the database access fail, take a look on the above file that you dowloaded to see if it came corrupted. This can be caused (not likely but it can) when the dowload is made directly from R.</p>
</div>
<div id="step-2" class="section level2">
<h2><strong>Step 2</strong></h2>
<p>Inside R Studio, install and call the following packages: “rJava” and “RJDBC”.</p>
<pre class="r"><code>install.packages(&quot;rJava&quot;)
install.packages(&quot;RJDBC&quot;)

library(rJava)
library(RJDBC)</code></pre>
</div>
<div id="step-3" class="section level2">
<h2><strong>Step 3</strong></h2>
<p>Create a variable (i.e. “driver”) that will contain the JDBC file that was dowloaded on the Step 1. Use the following code for this step:</p>
<pre class="r"><code>driver &lt;- JDBC(driverClass = &quot;com.amazon.redshift.jdbc41.Driver&quot;, 
               classPath = Sys.glob(&quot;caminho do arquivo/RedshiftJDBC41-1.1.9.1009.jar&quot;), 
               identifier.quote=&quot;`&quot;)</code></pre>
</div>
<div id="step-4" class="section level2">
<h2><strong>Step 4</strong></h2>
<p>Create a variable (i.e. “db_connection”) that will indicate the information of the database to be accessed by R Studio. Use the following code for this step:</p>
<pre class="r"><code>db_connection &lt;- sprintf(&quot;jdbc:redshift://%s:%s/%s?tcpKeepAlive=true&amp;ssl=true&amp;sslfactory=com.amazon.redshift.ssl.NonValidatingFactory&quot;, 
                         &quot;host of the database&quot;, 
                         &quot;port&quot;, 
                         &quot;database name&quot;)</code></pre>
</div>
<div id="step-5" class="section level2">
<h2><strong>Step 5</strong></h2>
<p>Create a variable (i.e. “access”) that will contain the information of the database and also the user that will access the database. Use the following code for this step:</p>
<pre class="r"><code>access &lt;- dbConnect(driver, db_end, &quot;user&quot;, &quot;password&quot;)</code></pre>
<p>Ps: Note that the funcion “dbConnect” used the database access file driver, the information of the database that will be accessed and the information of the user that will access the database.</p>
</div>
<div id="step-6" class="section level2">
<h2><strong>Step 6</strong></h2>
<p>After the creation of the necessary variables, to access the database it’ll be necessary the last one created (at step 5). To access the database, the package RJDBC will be used. Some examples can be seeing on the code chunck that follows:</p>
<p>Ps: Queries run on R Studio console must by with quotes.</p>
<pre class="r"><code>RJDBC::dbGetQuery(jconn,
                  &quot;select 
                     order
                   , client
                   , sale_date 
                  from orders_tb 
                  limit 10&quot;)

RJDBC::dbGetQuery(jconn,
                  &quot;select 
                     supplier
                   , invoice 
                  from supplier.tb 
                  where 1 = 1 
                  and delivery.date = 2019-01-30 
                  limit 10&quot;)</code></pre>
</div>
<div id="step-7" class="section level2">
<h2><strong>Step 7</strong></h2>
<p>To explore the feature in depth, it’s interesting that, intead of writing the query on the R Studio console, you save your query in a file (.sql) and make R access this file and save it as a variable (i.e. “sql_file”). To do this, it’s necessary to install and call the “readr” package.</p>
<pre class="r"><code>install.packages(&quot;readr&quot;)

library(readr)

-- Saving the .sql file as a variable
sql_file &lt;- readr::read_file(&quot;path_of_the_file/script.sql&quot;)

-- Using the .sql file to access Redshift database
query_redshift &lt;- RJDBC::dbGetQuery(jconn, sql_file)</code></pre>
</div>
<div id="final-step-plus" class="section level2">
<h2><strong>Final step (plus)</strong></h2>
<p>Sometimes you can have projects that demand more than one query, thus you will need many .sql files in order to run each query. But there’s a solution to make things simpler. You can save many queries on the same .sql file and separate each query by a string (i.e. –query_1_being, –query_1_end, –query_2_begin, –query_2_end, and so on). Then, you can use regexp to split your file and create variables for each query. The regexp will access the defined query and execute the split to create the variable with just one query code. For this step, you will need to install and call the “stringr” package.</p>
<pre class="r"><code>install.packages(&quot;stringr&quot;)

library(stringr)

-- saving the .sql file as a variable
sql_syntax &lt;- readr::read_file(&quot;.sql_file_path.all_queries.sql&quot;)

-- creating one variable with the first query of the .sql file
query_1 &lt;- 
  RJDBC::dbGetQuery(jconn, 
                    substr(sql_syntax, 
                           min(stringr::str_locate(string = sql_syntax, &#39;-- query.1.begin&#39;)), 
                           max(stringr::str_locate(string = sql_syntax, &#39;-- query.1.end&#39;))))

query_2 &lt;- 
  RJDBC::dbGetQuery(jconn, 
                    substr(sql_syntax, 
                           min(stringr::str_locate(string = sql_syntax, &#39;-- query.2.begin&#39;)), 
                           max(stringr::str_locate(string = sql_syntax, &#39;-- query.2.end&#39;))))

query_3 &lt;- 
  RJDBC::dbGetQuery(jconn, 
                    substr(sql_syntax, 
                           min(stringr::str_locate(string = sql_syntax, &#39;-- query.3.begin&#39;)),
                           max(stringr::str_locate(string = sql_syntax, &#39;-- query.3.end&#39;))))</code></pre>
</div>
<div id="conclusion" class="section level2">
<h2><strong>Conclusion</strong></h2>
<p>This tutorial was aimed to show how simple it is to access Amazon Redshift database throughout R Studio. It’s an excellent alternative when you want to perform robust exploratory data analysis (where Excel will let you down).</p>
</div>
<div id="more-tutorials-on-the-subject" class="section level2">
<h2><strong>More tutorials on the subject</strong></h2>
<p><a href="https://www.progress.com/tutorials/jdbc/connecting-to-amazon-redshift-from-r-via-jdbc-driver">Connecting to Amazon Redshift from R</a></p>
<p><a href="https://www.r-bloggers.com/a-comprehensive-guide-to-connect-r-to-amazon-redshift/">A comprehensive guide to connect R to Amazon Redshift</a></p>
</div>
]]></content>
        </item>
        
    </channel>
</rss>
