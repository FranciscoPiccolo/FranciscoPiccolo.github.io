<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Glass Frog</title>
        <link>/posts/</link>
        <description>Recent content in Posts on Glass Frog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Thu, 14 May 2020 00:00:00 +0000</lastBuildDate>
        <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Book Review: Data Analysis and Graphs with R by J.Maindonald and J.Braun</title>
            <link>/posts/2020/05/book-review-data-analysis-and-graphs-with-r-by-j.maindonald-and-j.braun/</link>
            <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
            
            <guid>/posts/2020/05/book-review-data-analysis-and-graphs-with-r-by-j.maindonald-and-j.braun/</guid>
            <description>OverviewThis book is part of a project made by Cambrige University called Cambridge Series in Statistics and Probabilistic Mathematics aiming to spread knowledge in statistics. As well as this book, there’re other 19 that explore other areas of statistics.
The content of this book is more generic, where the authors explain a range of concepts, from exploratory data analysis, statistical tests, tree models and survival analysis. The book was published in 2006 (the second edition) and I think because of that the authors didn’t use libraries like ggplot or the tidyverse universe.</description>
            <content type="html"><![CDATA[


<div id="overview" class="section level2">
<h2><strong>Overview</strong></h2>
<p>This book is part of a project made by Cambrige University called <strong>Cambridge Series in Statistics and Probabilistic Mathematics</strong> aiming to spread knowledge in statistics. As well as this book, there’re other 19 that explore other areas of statistics.</p>
<p>The content of this book is more generic, where the authors explain a range of concepts, from exploratory data analysis, statistical tests, tree models and survival analysis. The book was published in 2006 (the second edition) and I think because of that the authors didn’t use libraries like ggplot or the tidyverse universe.</p>
<p>The explanations given by the authors are clear and easy to understand, also the utilization of data sets from R packages (e.g. DAAG) simplify reproducibility of examples and execution of models to solve the proposed exercises.</p>
<div class="figure">
<img src="/images/2020-02-07_data_analysis_with_r.png" alt="Book cover" />
<p class="caption">Book cover</p>
</div>
</div>
<div id="chapter-1---a-brief-introduction-to-r" class="section level2">
<h2><strong>Chapter 1 - A brief introduction to R</strong></h2>
<p>In this chapter the focus is to introduce the R enviroment to the reader. With this goal, the authors explain the types of data that R can handle, how to build functions and what kind of graphs are possible with this tool. Let’s see some exercises presented this chapter.</p>
<div id="exercises" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="the-following-table-gives-the-size-of-the-floor-area-ha-and-the-price-a000-for-15-houses-sold-in-the-canberra-australia-suburb-of-aranda-in-1999." class="section level3">
<h3><strong>1. The following table gives the size of the floor area (ha) and the price ($A000), for 15 houses sold in the Canberra (Australia) suburb of Aranda in 1999.</strong></h3>
<pre class="r"><code>df &lt;- data.frame(area = c(694,905,802,1366,716,963,821,714,1018,887,790,696,771,1006,1191),
                 sale.price = c(192,215,215,274,112.7,185,212,220,276,260,221,255,260,293,375))</code></pre>
</div>
<div id="a-plot-sale.price-versus-area." class="section level3">
<h3><strong>a) Plot sale.price versus area.</strong></h3>
<p><strong>A:</strong> For this, I’ll use ggplot library instead of base R plots, because it’s easier and prettier.</p>
<pre class="r"><code>df %&gt;% 
  ggplot2::ggplot()+
  geom_point(mapping = aes(x = area, y = sale.price), 
             color = &quot;darkred&quot;)+
  theme_graph()+
  labs(title = &quot;Sales Price vs Area&quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="b-use-hist-command-to-plot-a-histogram-of-the-sales.price." class="section level3">
<h3><strong>b) Use hist() command to plot a histogram of the sales.price.</strong></h3>
<p><strong>A:</strong> In this alternative I won’t use the hist() command. Instead I’ll opt for the geom_histogram from ggplot.</p>
<pre class="r"><code>df %&gt;% 
  ggplot2::ggplot()+
  geom_histogram(mapping = aes(x = sale.price),
                 fill = &quot;darkblue&quot;,
                 alpha = .4)+
  theme_graph()+
  labs(title = &quot;Sales Price Histogram&quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="c-repeat-a-and-b-after-taking-logarithms-of-sale-prices." class="section level3">
<h3><strong>c) Repeat (a) and (b) after taking logarithms of sale prices.</strong></h3>
<p><strong>A:</strong> It’s possible to reproduce the same graphs and put the log() formula on the variables inside aes() function.</p>
<pre class="r"><code>df %&gt;% 
  ggplot2::ggplot()+
  geom_point(mapping = aes(x = log(area), y = log(sale.price)),
             color = &quot;darkred&quot;)+
  theme_graph()+
  labs(title = &quot;Sales Price vs Area using log transformation&quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The histogram will follow the same logic.</p>
<pre class="r"><code>df %&gt;% 
  ggplot2::ggplot()+
  geom_histogram(mapping = aes(x = log(sale.price)),
                 fill = &quot;darkblue&quot;,
                 alpha = .4)+
  theme_graph()+
  labs(title = &quot;Sale Price Histograma&quot;,
       subtitle = &quot;Log transformed&quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-2---styles-of-data-analysis" class="section level2">
<h2><strong>Chapter 2 - Styles of data analysis</strong></h2>
<p>This chapter explains about exploratory data analysis, a technique developed by John Tukey to support researchers during their initial problem exploration. The aythors present the types of graphs available for researchers and when to use each of them. Also, they explain about outliers and data skewness, showing how to deal with each situation.</p>
<div id="exercises-1" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="use-the-lattice-function-bwplot-to-display-for-each-combination-of-site-and-sex-in-the-data-frame-possumdaag-package-the-distribution-of-ages.-show-the-different-sites-on-the-same-panel-with-different-panels-for-different-sexes." class="section level3">
<h3><strong>1) Use the lattice function bwplot() to display, for each combination of <em>site</em> and <em>sex</em> in the data frame <em>possum</em>(DAAG package), the distribution of ages. Show the different sites on the same panel, with different panels for different sexes.</strong></h3>
<p><strong>A:</strong> bwplot means box and whisker plot (a.k.a boxplot). It’s a good visualization to present distribution among different categories. Let’s create the boxplot using ggplot.</p>
<pre class="r"><code>DAAG::possum %&gt;%
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = age, group = site, fill = as.factor(site)),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  theme_graph()+
  theme(legend.title = element_text(size = 7))+
  scale_fill_viridis_d()+
  facet_grid(~sex)+
  labs(title = &quot;Age distribution for different sites&quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="plot-a-histogram-of-the-earconch-measurements-for-the-possum-data.-the-distribution-should-appear-bimodal-two-peaks.-this-is-a-simple-indication-of-clustering-possibly-due-to-sex-differences.-obtain-side-by-side-boxplots-of-the-male-and-female-earconch-measurements.-how-do-these-measurement-distributions-differ-can-you-predict-what-the-corresponding-histograms-would-look-like-plot-them-to-check-your-answer." class="section level3">
<h3><strong>3) Plot a histogram of the earconch measurements for the possum data. The distribution should appear bimodal (two peaks). This is a simple indication of clustering, possibly due to sex differences. Obtain side-by-side boxplots of the male and female earconch measurements. How do these measurement distributions differ? Can you predict what the corresponding histograms would look like? Plot them to check your answer.</strong></h3>
<p><strong>A:</strong> If the data ser contains men and women in a similar proportion, the histogram will be bicaudal, because the measure differ for each gender.</p>
<pre class="r"><code>DAAG::possum %&gt;% 
  ggplot2::ggplot()+
  geom_histogram(mapping = aes(x = earconch),
                 fill = &quot;darkred&quot;,
                 alpha = .2,
                 color = &quot;darkred&quot;,
                 size = 1)+
  theme_graph()+
  labs(title = &quot;Earconch Distribution&quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Now let’s see the boxblot for each gender.</p>
<pre class="r"><code>DAAG::possum %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = earconch, fill = sex),
               alpha = .2)+
  scale_fill_viridis_d()+
  theme_graph()+
  labs(title = &quot;Earconch box plot distribution&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-3---statistical-models" class="section level2">
<h2><strong>Chapter 3 - Statistical models</strong></h2>
<p>This chapter introduce the fundations of statistical models. First the authors explain the importance of statistical models, showing the difference between physical phenomenon and random events that has singnal and noise well established.</p>
<p>They affirm that models have to provide accurate inferences in order to allow predictions. Thus, for researchers build reliable models it’s necessary to set hypothesis about data distribution, sampling method and also assumptions about the parameters of the model.</p>
<p>As the assumption of the data distribution is an important part of model definition, the authors explain how to validate if the data is normally distributed, using visual displays (i.e. histograms on qq-plots) or formal tests.</p>
<div id="exercises-2" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="use-y---rnorm100-to-generate-a-random-sample-of-100-numbers-from-a-normal-distribution.-calculate-the-mean-and-standard-deviation-of-y.-now-put-the-calculation-in-a-loop-and-repeat-25-times.-store-the-25-means-in-a-vector-named-av.-calculate-the-standard-deviation-of-the-values-in-av." class="section level3">
<h3><strong>2) Use y &lt;- rnorm(100) to generate a random sample of 100 numbers from a normal distribution. Calculate the mean and standard deviation of y. Now put the calculation in a loop and repeat 25 times. Store the 25 means in a vector named av. Calculate the standard deviation of the values in av.</strong></h3>
<pre class="r"><code>y &lt;- rnorm(100)

tibble(mean = mean(y),
           sd = sd(y))</code></pre>
<pre><code>## # A tibble: 1 x 2
##      mean    sd
##     &lt;dbl&gt; &lt;dbl&gt;
## 1 -0.0223  1.00</code></pre>
<p><strong>A:</strong> In order to run this process, it’s necessary to create an empty variable with 25 slots available, then create a <em>for lopp</em> that will calculate the mean for a sample of 100 values taken from a normal distribution, 25 times.</p>
<pre class="r"><code># Creating an empty variable with 25 slots
output &lt;- rep(NA,25)

# Creating the for loop
for(i in 1:25){
  output[i] &lt;- data.frame(
    mean(rnorm(100))
  )
}

# Extracting the output and inserting it in a data.frame
df &lt;- data.frame(
  a = matrix(unlist(output),nrow = 25,byrow = F), stringsAsFactors = F)

# Calculating the mean and sd
tibble(mean = mean(df[,1]),
           sd = sd(df[,1]))</code></pre>
<pre><code>## # A tibble: 1 x 2
##       mean    sd
##      &lt;dbl&gt; &lt;dbl&gt;
## 1 -0.00457 0.112</code></pre>
</div>
<div id="create-a-function-that-does-the-calculations-of-exercise-2.-run-the-function-several-times-showing-each-of-the-distributions-of-25-means-in-a-density-plot." class="section level3">
<h3><strong>3) Create a function that does the calculations of Exercise 2. Run the function several times, showing each of the distributions of 25 means in a density plot.</strong></h3>
<pre class="r"><code># Creating the function, x = repetitions, y = random samples
fun &lt;- function(x, y){
  output &lt;- rep(NA, x)
  for(i in 1:x){
    output[i] &lt;- data.frame(
      round(mean(rnorm(y)), digits = 2)
    )
  }
  df &lt;- data.frame(
    a = matrix(unlist(output), 
               nrow = x,
               byrow = T), 
    stringsAsFactors = F)
}

# Running the function and ploting the results
fun(25, 100) %&gt;% 
  ggplot2::ggplot()+
  geom_density(mapping = aes(x = a),
               fill = &quot;dark orange&quot;,
               alpha = .2)+
  theme_graph()+
  labs(title = &quot;Density plot for &quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-4---an-introduction-to-formal-inference" class="section level2">
<h2><strong>Chapter 4 - An introduction to formal inference</strong></h2>
<p>This chapter introduces some basic concepts about statistical inference. The main concepts about this subject are <strong>Standard Error of the Mean</strong>; <strong>student distribution</strong>; <strong>degress of freedom</strong>; <strong>p-values</strong>; <strong>confidence intervals</strong>; <strong>test of hypothesis</strong>.</p>
<p>The authors explain the formulas for some of these variables, how to perform hypothesis test and build confidence intervals and also, why researchers are moving from hypothesis test to confidence intervals.</p>
<div id="exercises-3" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="in-the-data-set-nsw74demo-daag-determine-95-confidence-intervals-for-a-the-1974-incomes-of-each-of-the-treated-and-control-groups-b-the-1975-incomes-of-each-group-c-the-1978-incomes-of-each-group.-finally-calculate-a-95-confidence-interval-for-the-difference-in-income-between-treated-and-controls-in-1978." class="section level3">
<h3><strong>1) In the data set nsw74demo (DAAG) determine 95% confidence intervals for: (a) the 1974 incomes of each of the “treated” and “control” groups; (b) the 1975 incomes of each group; (c) the 1978 incomes of each group. Finally, calculate a 95% confidence interval for the difference in income between treated and controls in 1978.</strong></h3>
<p><strong>A:</strong> As explained in this chapter, confidence intervals are build using the following equation:</p>
<p><span class="math display">\[\mu - (t_\frac{\alpha}{2} \times SEM)\]</span>
Changing the signals from minus to plus in order to obtain the the upper limit instead of the lower.</p>
<p>Where:</p>
<p><span class="math inline">\(\mu\)</span> = the sample mean given in the data set.</p>
<p>$t_ = $ Critical t value obtained using the confidence level (<span class="math inline">\(\alpha\)</span>) and dividing by two because we’ll validate the two side of the distribution (bicaudal test).</p>
<p><span class="math inline">\(SEM = \frac{s}{\sqrt{n}}\)</span>.</p>
<p>$s = $ Sample standard deviation.</p>
<p>$n = $ Number of observations taken.</p>
<p>In the data set mentioned, we have one group being studied in different time and then being compared with a control group. The groups are identified by the field <strong>trt</strong> that will be equal 1 for treatment groups and 0 for control grups. Let’s develop a data.frame that will allow us to answer all three questions.</p>
<p>First I’ll group the data set and take the average income of each group in different columns. Then, I’ll calculate the critical t value from the student distribution and with this value I’ll create the standard error of the mean for each timeframe. With this values it will be possible to calculate the limits of the invervals.</p>
<pre class="r"><code>DAAG::nsw74demo %&gt;%
  mutate(group = ifelse(trt == 0, &quot;control&quot;,&quot;test&quot;)) %&gt;% 
  group_by(group) %&gt;% 
  summarise(n = n(),
            avg_income_74 = mean(re74),
            avg_income_75 = mean(re75),
            avg_income_78 = mean(re78),
            sem_74 = sd(re74)/sqrt(n),
            sem_75 = sd(re75)/sqrt(n),
            sem_78 = sd(re78)/sqrt(n)) %&gt;% 
  mutate(critical_t = qt(0.975, n),
         sem_74_times_critical_t = sem_74 * critical_t,
         sem_75_times_critical_t = sem_75 * critical_t,
         sem_78_times_critical_t = sem_78 * critical_t,
         inc74_lower = avg_income_74 - sem_74_times_critical_t,
         inc74_upper = avg_income_74 + sem_74_times_critical_t,
         inc75_lower = avg_income_75 - sem_74_times_critical_t,
         inc75_upper = avg_income_75 + sem_74_times_critical_t,
         inc78_lower = avg_income_78 - sem_74_times_critical_t,
         inc78_upper = avg_income_78 + sem_74_times_critical_t)</code></pre>
<pre><code>## # A tibble: 2 x 18
##   group     n avg_income_74 avg_income_75 avg_income_78 sem_74 sem_75 sem_78
##   &lt;chr&gt; &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 cont~   260         2107.         1267.         4555.   353.   192.   340.
## 2 test    185         2096.         1532.         6349.   359.   237.   578.
## # ... with 10 more variables: critical_t &lt;dbl&gt;, sem_74_times_critical_t &lt;dbl&gt;,
## #   sem_75_times_critical_t &lt;dbl&gt;, sem_78_times_critical_t &lt;dbl&gt;,
## #   inc74_lower &lt;dbl&gt;, inc74_upper &lt;dbl&gt;, inc75_lower &lt;dbl&gt;, inc75_upper &lt;dbl&gt;,
## #   inc78_lower &lt;dbl&gt;, inc78_upper &lt;dbl&gt;</code></pre>
<p>Now, the final question demand us to estimate the income difference between treated and control groups in 1978, with 95% of confidence. For this estimation, the book recommends the following equation:</p>
<p><span class="math inline">\(SED = \sqrt{SEM_1^2 + SEM_2^2}\)</span></p>
<p>Where:</p>
<p>SED = <em>standard error of the difference</em></p>
<p><span class="math inline">\(SEM_control\)</span> = <em>standard error of the mean</em></p>
<p><span class="math inline">\(SEM_test\)</span> = <em>standard error of the mean</em></p>
<p><span class="math inline">\(SED = \sqrt{415^2+680^2}\)</span></p>
<p><span class="math inline">\(SED = \sqrt{172.225+462.400}\)</span></p>
<p><span class="math inline">\(SED = 796,6\)</span></p>
<p><strong>A:</strong> The income difference between groups, with 95% of confidence is of 796,6.</p>
</div>
<div id="draw-graphs-that-show-for-degrees-of-freedom-between-1-and-100-the-change-in-the-5-critical-value-of-the-t-statistic.-compare-a-graph-on-which-neither-axis-is-transformed-with-a-graph-on-which-the-respective-axis-scales-are-proportional-to-logt-statistic-and-logdegrees-of-freedom.-which-graph-gives-the-more-useful-visual-indication-of-the-change-in-the-5-critical-value-of-the-t-statistic-with-increasing-degrees-of-freedom." class="section level3">
<h3><strong>2) Draw graphs that show, for degrees of freedom between 1 and 100, the change in the 5% critical value of the t-statistic. Compare a graph on which neither axis is transformed with a graph on which the respective axis scales are proportional to log(t-statistic) and log(degrees of freedom). Which graph gives the more useful visual indication of the change in the 5% critical value of the t-statistic with increasing degrees of freedom?.</strong></h3>
<pre class="r"><code>t_value &lt;- 
  data.frame(df = seq(1, 100, by = 1)) %&gt;% 
  mutate(t_value = qt(0.975, df = row_number()))

g1 &lt;- t_value %&gt;% 
  ggplot2::ggplot()+
  geom_line(mapping = aes(x = df, y = t_value))+
  scale_x_log10()+
  scale_y_log10()+
  theme_graph()+
  labs(title = &quot;Log transformation applied&quot;,
       x = &quot;log_10 of Degrees of freedom&quot;,
       y = &quot;log_10 of t-statistics&quot;)

g2 &lt;- t_value %&gt;% 
  ggplot2::ggplot()+
  geom_line(mapping = aes(x = df, y = t_value))+
  theme_graph()+
  labs(title = &quot;Log transformation not applied&quot;,
       x = &quot;Degrees of freedom&quot;,
       y = &quot;t-statistics&quot;)

g1 + g2</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="generate-a-random-sample-of-10-numbers-from-a-normal-distribution-with-mean-0-and-standard-deviation-2.-use-t.test-to-test-the-null-hypothesis-that-the-mean-is-0.-now-generate-a-random-sample-of-10-numbers-from-a-normal-distribution-with-mean-1.5-and-standard-deviation-2.-again-use-t.test-to-test-the-null-hypothesis-that-the-mean-is-0.-finally-write-a-function-that-generates-a-random-sample-of-n-numbers-from-a-normal-distribution-with-mean-and-standard-deviation-1-and-returns-the-p-value-for-the-test-that-the-mean-is-0.." class="section level3">
<h3><strong>3) Generate a random sample of 10 numbers from a normal distribution with mean 0 and standard deviation 2. Use t.test() to test the null hypothesis that the mean is 0. Now generate a random sample of 10 numbers from a normal distribution with mean 1.5 and standard deviation 2. Again use t.test() to test the null hypothesis that the mean is 0. Finally write a function that generates a random sample of n numbers from a normal distribution with mean and standard deviation 1, and returns the p-value for the test that the mean is 0..</strong></h3>
<p><strong>A:</strong> This question demands us to make a hypothesis test using a random data set. Let’s create this data set with using a normal distribution.</p>
<pre class="r"><code>df &lt;- rnorm(n = 10, mean = 0, sd = 2)

# Visualizing the data set
df</code></pre>
<pre><code>##  [1]  0.3384814 -2.0430390  1.2326679  2.3316873 -1.1990960 -0.1691513
##  [7] -1.0768319  3.2526479 -0.6622174  0.9717683</code></pre>
<p>Now, let’s make R calculate the t.test() and see the result and after this I’ll calculate the p-value by hand to reach the same result. I expect that the null hypothesis (<span class="math inline">\(H0: \mu = 0\)</span>) won’t be rejected, because the data set was generated from a population with mean 0.</p>
<pre class="r"><code>df %&gt;% 
  t.test(mu = 0, conf.level = 0.975)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  .
## t = 0.56543, df = 9, p-value = 0.5856
## alternative hypothesis: true mean is not equal to 0
## 97.5 percent confidence interval:
##  -1.115938  1.711322
## sample estimates:
## mean of x 
## 0.2976917</code></pre>
<p>The t.test from R generated a p-value of 0.7261. In this case we can’t reject H0 as expected. Now let’s make the calculation step by step.</p>
<pre class="r"><code>mean &lt;- mean(df)
sd &lt;- sd(df)
n_sqrt &lt;- sqrt(length(df))

sem &lt;- sd/n_sqrt

(1-pt(mean/sem, 10-1))*2</code></pre>
<pre><code>## [1] 0.5856006</code></pre>
<p>The same p-value was obtained. Thus, we can rely on our t.test() from R and understand what it is happening on backstage.</p>
<p>Now, the question ask us to perform the same method in a population with mean 1.5. In this case, let’s make the sample generation and statistical modeling all at once.</p>
<pre class="r"><code>data.frame(rnorm(n = 10, mean = 1.5, sd = 2)) %&gt;% 
  t.test(mu = 0, conf.level = 0.975)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  .
## t = 2.0968, df = 9, p-value = 0.06546
## alternative hypothesis: true mean is not equal to 0
## 97.5 percent confidence interval:
##  -0.3985366  3.2399249
## sample estimates:
## mean of x 
##  1.420694</code></pre>
<p>In this test, the p-value obtained was lower, but not sufficient to make us reject H0: <span class="math inline">\(\mu = 0\)</span>. Maybe it’s because the sample size is small. Let’s remake the test with a larger sample.</p>
<pre class="r"><code>data.frame(rnorm(n = 100, mean = 1.5, sd = 2)) %&gt;% 
  t.test(mu = 0, conf.level = 0.975)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  .
## t = 7.9088, df = 99, p-value = 3.762e-12
## alternative hypothesis: true mean is not equal to 0
## 97.5 percent confidence interval:
##  1.083773 1.959600
## sample estimates:
## mean of x 
##  1.521686</code></pre>
<p>Now, the p-value is pretty lower and we can reject H0 with a 95% of confidence.</p>
</div>
<div id="use-the-function-that-was-created-in-exercise-3-to-generate-50-independent-p-values-all-with-a-sample-size-n-10-and-with-mean-0.-use-qqplot-with-the-parameter-setting-x-qunifppoints50-to-compare-the-distribution-of-the-p-values-with-that-of-a-uniform-random-variable-on-the-interval-0-1.-comment-on-the-plot." class="section level3">
<h3><strong>4) Use the function that was created in Exercise 3 to generate 50 independent p-values, all with a sample size n = 10 and with mean = 0. Use qqplot(), with the parameter setting x = qunif(ppoints(50)), to compare the distribution of the p-values with that of a uniform random variable, on the interval [0, 1]. Comment on the plot.</strong></h3>
<p><strong>A:</strong> In the last exercise I didn’t create the function, then I’ll create it now. The function will generates a random sample of <em>n</em> numbers from a normal distribution with mean <span class="math inline">\(\mu = 0\)</span> and standard deviation 1, and will return the <strong>p-value</strong> from a test considering <strong>H0: <span class="math inline">\(\mu = 0\)</span></strong>.</p>
<p>The first thing we have to make in order to build a function, is to create an empty vector that will receive the output. Then, we create the function and fill this vector with the output from the function. Let’s see how it works.</p>
<pre class="r"><code># Creating the empty vector
output &lt;- rep(NA,50)

# Creating the function with the p-value
for(i in 1:50){
  sample &lt;- rnorm(n = 10, mean = 0, sd = 1)
  mean &lt;- mean(sample)
  sd &lt;- sd(sample)
  sem &lt;- sd/sqrt(10)
  output[i] &lt;- data.frame(p_value = (1-pt((abs(mean)/sem),9))*2)
}

# Putting function output in a data.frame
df &lt;- data.frame(a = matrix(unlist(output), 
                            nrow = 50, 
                            byrow = T))</code></pre>
<p>Now we have the data.frame and we’re ready to plot the requested qq-plot. This plot is nice to visualize how normal is the data distribution. For this case, we have a sample obtained from a population with mean 0 and sd 1 and we’re testing the Null Hypothesis that population mean is 0. Then, the p-value has to by high (more than 0.05 at least) in order to preclude the rejection of H0. The qq-plot will show that p-value is normally distributed. I’ll also generate a histogram to show that p-value is high in most of the cases (the data.frame contains 50 p-values). Let’s see both graphs</p>
<pre class="r"><code># Generating the qq-plot
g1 &lt;- df %&gt;% 
  ggplot2::ggplot()+
  geom_qq(mapping = aes(sample = a),
          color = &quot;dark blue&quot;)+
  geom_qq_line(mapping = aes(sample = a),
               lty = 2,
               color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;QQ plot for p-values&quot;)

# Generating the histogram
g2 &lt;- df %&gt;% 
  ggplot2::ggplot()+
  geom_histogram(mapping = aes(x = a),
                 fill = &quot;dark blue&quot;,
                 alpha = .3)+
  geom_vline(xintercept = 0.05,
             lty = 2,
             color = &quot;dark orange&quot;)+
  scale_x_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;Histogram of p-values&quot;)

g1+g2</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Look that most of the p-values generated are on the righ side of the histogram’s yellow dached line. This indicates that most of the Null Hypothesis wouldn’t be rejected considering a significance level of 5%.</p>
</div>
<div id="here-we-generate-random-normal-numbers-with-a-sequential-dependence-structure.-repeat-this-several-times.-there-should-be-no-consistent-pattern-in-the-acf-plot-for-different-random-samples-y1.-there-will-be-a-fairly-consistent-pattern-in-the-acf-plot-for-y-a-result-of-the-correlation-that-is-introduced-by-adding-to-each-value-the-next-value-in-the-sequence." class="section level3">
<h3><strong>6) Here we generate random normal numbers with a sequential dependence structure. Repeat this several times. There should be no consistent pattern in the ACF plot for different random samples y1. There will be a fairly consistent pattern in the ACF plot for y, a result of the correlation that is introduced by adding to each value the next value in the sequence.</strong></h3>
<pre class="r"><code># Random numbers mentioned in the exercise
y1 &lt;- rnorm(51)
y &lt;- y1[-1] + y1[-51]
acf(y1) # acf is ‘autocorrelation function’</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>acf(y)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-22-2.png" width="672" /></p>
</div>
<div id="a-for-each-column-of-the-data-set-nsw74psid1-after-the-first-compare-the-control-group-trt0-with-the-treatment-group-trt1.-use-overlaid-density-plots-to-compare-the-continuous-variables-and-two-way-tables-to-compare-the-binary-01-variables.-where-are-the-greatest-differences." class="section level3">
<h3><strong>8) (a) For each column of the data set nsw74psid1 after the first, compare the control group (trt==0) with the treatment group (trt==1). Use overlaid density plots to compare the continuous variables, and two-way tables to compare the binary (0/1) variables. Where are the greatest differences?.</strong></h3>
<p><strong>A:</strong> In this exercise I’ll generate box plots for continuous variables in order to compare the control and treated groups.</p>
<pre class="r"><code>g1 &lt;- DAAG::nsw74psid1 %&gt;%
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = age, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme(axis.text.x = element_blank())+
  theme_graph()

g2 &lt;- DAAG::nsw74psid1 %&gt;% 
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = educ, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  theme(axis.text.x = element_blank())

g3 &lt;- DAAG::nsw74psid1 %&gt;% 
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = re74, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_y_log10(labels = scales::comma)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  theme(axis.text.x = element_blank())

g4 &lt;- DAAG::nsw74psid1 %&gt;% 
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = re75, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_y_log10(labels = scales::comma)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  theme(axis.text.x = element_blank())

g5 &lt;- DAAG::nsw74psid1 %&gt;% 
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = re78, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_y_log10(labels = scales::comma)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  theme(axis.text.x = element_blank())

# Putting all graphs together
plots &lt;- (g1+g2)/(g3|g4|g5)

plots +
  plot_layout(guides = &quot;collect&quot;)+
  plot_annotation(title = &quot;Continuous variables comparison&quot;,
                  subtitle = &quot;Control (0) and treated (1) groups&quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-5---regression-with-a-single-predictor" class="section level2">
<h2><strong>Chapter 5 - Regression with a Single Predictor</strong></h2>
<p>This chapter provides some foundations on linear regression models, where the authors walk through parameters estimation using confidence intervals and hypothesis test, R’s <strong>lm function</strong> and outputs, residual analysis and the use of cross-validation and bootstrap to reproduce the model multiple times in order to assess how it’s stable and reliable.</p>
<div id="for-each-of-the-data-sets-elastic1-and-elastic2-determine-the-regression-of-stretch-on-distance.-in-each-case-determine." class="section level3">
<h3><strong>2) For each of the data sets <em>elastic1</em> and <em>elastic2</em>, determine the regression of stretch on distance. In each case determine.</strong></h3>
<p><strong>(i) fitted values and standard errors of fitted values</strong></p>
<p><strong>(ii) the R² statistic. Compare the two sets of results. What is the key difference between the two sets of data?</strong></p>
<p><strong>Use the robust regression function rlm() from the MASS package to fit lines to the data in elastic1 and elastic2. Compare the results with those from use of lm(). Compare regression coefficients, standard errors of coefficients, and plots of residuals against fitted values.</strong></p>
<p><strong>A:</strong> The data sets mentioned for this exercise is:</p>
<pre class="r"><code>df1 &lt;- data.frame(stretch = c(46,54,48,50,44,42,52),
                  distance = c(183,217,189,208,178,150,249))

df2 &lt;- data.frame(stretch = c(25,45,35,40,55,50,30,50,60),
                  distance = c(71,196,127,187,249,217,114,228,291))</code></pre>
<p>With these two data sets, let’s determine the regression of stretch (y) on distance (x). For this analysis, I’ll plot the scaterplot with the model output as anottation on the graph.</p>
<pre class="r"><code>g1 &lt;- df1 %&gt;%
  ggplot2::ggplot()+
  geom_point(mapping = aes(x = distance, y = stretch),
             shape = 20)+
  geom_smooth(mapping = aes(x = distance, y = stretch),
              method = &quot;lm&quot;,
              formula = &quot;y ~ x&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  geom_text(mapping = aes(x = 165, y = 55, label = &quot;y = 24.13 + 0.12 X1&quot;),
            size = 3)+
  geom_text(mapping = aes(x = 167, y = 54, label = &quot;Std.Error = (5.41) | (0.027)&quot;),
            size = 3)+
  geom_text(mapping = aes(x = 165, y = 53, label = &quot;R²: 79.92%&quot;),
            size = 3)+
  theme_graph()

g2 &lt;- df2 %&gt;%
  ggplot2::ggplot()+
  geom_point(mapping = aes(x = distance, y = stretch),
             shape = 20)+
  geom_smooth(mapping = aes(x = distance, y = stretch),
              method = &quot;lm&quot;,
              formula = &quot;y ~ x&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  geom_text(mapping = aes(x = 105, y = 60, label = &quot;y = 12.56 + 0.16 X1&quot;),
            size = 3)+
  geom_text(mapping = aes(x = 105, y = 55, label = &quot;Std.Error = (1.73) | (0.009)&quot;),
            size = 3)+
  geom_text(mapping = aes(x = 105, y = 50, label = &quot;R²: 98.08%&quot;),
            size = 3)+
  theme_graph()

g1/g2</code></pre>
<pre><code>## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>The second data set has a stronger correlation betweem stretch and distance than the first one. The estimators of the second model also are more reliable given their lower standard error.</p>
<p>Now let’s see what we obtain using the function <em>rlm()</em> from the <em>MASS</em> package.</p>
<pre class="r"><code>df1 %&gt;% 
  MASS::rlm(stretch ~ distance, data = .) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call: rlm(formula = stretch ~ distance, data = .)
## Residuals:
##       1       2       3       4       5       6       7 
## -0.2207  3.8393  1.0840  0.8822 -1.6413 -0.3967 -1.8689 
## 
## Coefficients:
##             Value   Std. Error t value
## (Intercept) 25.0146  5.0698     4.9341
## distance     0.1159  0.0255     4.5366
## 
## Residual standard error: 1.607 on 5 degrees of freedom</code></pre>
<pre class="r"><code>df2 %&gt;% 
  MASS::rlm(stretch ~ distance, data = .) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call: rlm(formula = stretch ~ distance, data = .)
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.49045 -0.63426  0.02595  1.28920  1.56422 
## 
## Coefficients:
##             Value   Std. Error t value
## (Intercept) 12.6646  1.7257     7.3390
## distance     0.1648  0.0087    18.9266
## 
## Residual standard error: 1.911 on 7 degrees of freedom</code></pre>
</div>
<div id="using-the-data-frame-cars-datasets-plot-distance-i.e.-stopping-distance-versus-speed.-fit-a-line-to-this-relationship-and-plot-the-line.-then-try-fitting-and-plotting-a-quadratic-curve.-does-the-quadratic-curve-give-a-useful-improvement-to-the-fit" class="section level3">
<h3><strong>3) Using the data frame cars (datasets), plot distance (i.e. stopping distance) versus speed. Fit a line to this relationship, and plot the line. Then try fitting and plotting a quadratic curve. Does the quadratic curve give a useful improvement to the fit?</strong></h3>
<p><strong>A:</strong> Let’s plot the scateplot and both curves (linear and quadratic curves). I’ll consider distance on the y axis and speed on the x axis, in order to evaluate what distance will take for the car to stop given it’s speed before it start to brake.</p>
<pre class="r"><code>cars %&gt;% 
  ggplot2::ggplot()+
  geom_point(mapping = aes(x = speed, y = dist),
             shape = 7,
             color = &quot;dark blue&quot;)+
  geom_smooth(mapping = aes(x = speed, y = dist),
              method = &quot;lm&quot;,
              formula = &quot;y ~ x&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  geom_smooth(mapping = aes(x = speed, y = dist),
              method = &quot;lm&quot;,
              formula = &quot;y ~ I(x^1.7)&quot;,
              se = F,
              lty = 2,
              color = &quot;dark red&quot;)</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Know, let’s summarise both regression models and compare them. The comparison will consider the residual standard error.</p>
<pre class="r"><code>cars %&gt;% 
  lm(formula = &quot;dist ~ speed&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;dist ~ speed&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -29.069  -9.525  -2.272   9.215  43.201 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -17.5791     6.7584  -2.601   0.0123 *  
## speed         3.9324     0.4155   9.464 1.49e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.38 on 48 degrees of freedom
## Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 
## F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12</code></pre>
<pre class="r"><code>cars %&gt;% 
  lm(formula = &quot;dist ~ I(speed^1.7)&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;dist ~ I(speed^1.7)&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -28.875  -9.330  -3.155   4.548  45.036 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.88615    4.52194   0.859    0.394    
## I(speed^1.7)  0.34997    0.03573   9.796 4.96e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.03 on 48 degrees of freedom
## Multiple R-squared:  0.6666, Adjusted R-squared:  0.6596 
## F-statistic: 95.96 on 1 and 48 DF,  p-value: 4.956e-13</code></pre>
<p>Thus, we have the second model with a residual standard error of 15.03, lower than the 15.38 given by the first model (linear). Then, apparently the second model is better.</p>
</div>
<div id="calculate-volumes-volume-and-page-areas-area-for-the-books-on-which-information-is-given-in-the-data-frame-oddbooks-daag." class="section level3">
<h3><strong>4) Calculate volumes (volume) and page areas (area) for the books on which information is given in the data frame oddbooks (DAAG).</strong></h3>
</div>
<div id="a-plot-logweight-against-logvolume-and-fit-a-regression-line." class="section level3">
<h3><strong>a) Plot log(weight) against log(volume), and fit a regression line.</strong></h3>
<pre class="r"><code>DAAG::oddbooks %&gt;% 
  mutate(volume = (height*breadth*weight)) %&gt;% 
  ggplot2::ggplot()+
  geom_point(mapping = aes(x = log(weight), y = log(volume)))+
  geom_smooth(mapping = aes(x = log(weight), y = log(volume)),
              method = &quot;lm&quot;,
              formula = &quot;y ~ x&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Volume vs Weigth scaterplot&quot;)</code></pre>
<pre><code>## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
</div>
<div id="b-plot-logweight-against-logarea-and-again-fit-a-regression-line." class="section level3">
<h3><strong>b) Plot log(weight) against log(area), and again fit a regression line.</strong></h3>
<pre class="r"><code>DAAG::oddbooks %&gt;% 
  mutate(area = (height*weight)) %&gt;% 
  ggplot2::ggplot()+
  geom_point(mapping = aes(x = log(weight), y = log(area)))+
  geom_smooth(mapping = aes(x = log(weight), y = log(area)),
              method = &quot;lm&quot;,
              formula = &quot;y ~ x&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Area vs Weigth scaterplot&quot;)</code></pre>
<pre><code>## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
</div>
<div id="c-which-of-the-lines-a-and-b-gives-the-better-fit" class="section level3">
<h3><strong>c) Which of the lines (a) and (b) gives the better fit?</strong></h3>
<p><strong>A:</strong> In order to evaluate the best fit, I’ll use the metric standard error of the residuals and see which model has a lower value.</p>
<pre class="r"><code>DAAG::oddbooks %&gt;% 
  mutate(volume = (height*breadth*weight),
         area = (height*weight)) %&gt;% 
  lm(formula = &quot;log(volume) ~ log(weight)&quot;) %&gt;%
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;log(volume) ~ log(weight)&quot;, data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.19855 -0.10268 -0.03917  0.01860  0.40736 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -1.1527     0.8268  -1.394    0.193    
## log(weight)   2.1034     0.1321  15.921 1.97e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1884 on 10 degrees of freedom
## Multiple R-squared:  0.962,  Adjusted R-squared:  0.9582 
## F-statistic: 253.5 on 1 and 10 DF,  p-value: 1.97e-08</code></pre>
</div>
<div id="in-the-data-set-pressure-examine-the-dependence-of-pressure-on-temperature.-the-relevant-theory-is-that-associated-with-the-claudius-clapeyron-equation-by-which-the-logarithm-of-the-vapor-pressure-is-approximately-inversely-proportional-to-the-absolute-temperature." class="section level3">
<h3><strong>5) In the data set <em>pressure</em>, examine the dependence of pressure on temperature. The relevant theory is that associated with the Claudius-Clapeyron equation, by which the logarithm of the vapor pressure is approximately inversely proportional to the absolute temperature.</strong></h3>
<p><strong>A:</strong> Let’s see this data set, plot the graph and the fitted mdoel.</p>
<pre class="r"><code>pressure %&gt;% 
  ggplot2::ggplot()+
  geom_point(mapping = aes(y = log(pressure), x = 1/temperature))+
  geom_smooth(mapping = aes(y = log(pressure), x = 1/temperature),
              method = &quot;lm&quot;,
              formula = &quot;y ~ I(1/x)&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = TeX(&quot;Claudius-Clapeyron equation: $log(p) = \\frac{1}{x}$&quot;),
       y = TeX(&quot;$log(p)$&quot;),
       x = TeX(&quot;$\\frac{1}{x}$&quot;))</code></pre>
<pre><code>## Warning: Removed 1 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database

## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
## family not found in Windows font database</code></pre>
<pre><code>## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database

## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
## font family not found in Windows font database</code></pre>
<p><img src="/posts/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
</div>
</div>
]]></content>
        </item>
        
        <item>
            <title>Book Review: Basic Econometrics by Gujarati and Porter</title>
            <link>/posts/2020/03/book-review-basic-econometrics-by-gujarati-and-porter/</link>
            <pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate>
            
            <guid>/posts/2020/03/book-review-basic-econometrics-by-gujarati-and-porter/</guid>
            <description>OverviewThis book is the tipical book reference for a given subject, and when the theme is Econometrics, this material is highly recommended for economy grad students. For its importance, in this post I’ll present a brief summary of each chapter and solve some exercises (because the book doesn’t present the answer for the problems). Also, I’ll release the datasets used on the problems on my github account. To beggin this summary, lets see what the authors say about the subject:</description>
            <content type="html"><![CDATA[


<div id="overview" class="section level2">
<h2><strong>Overview</strong></h2>
<p>This book is the tipical book reference for a given subject, and when the theme is Econometrics, this material is highly recommended for economy grad students. For its importance, in this post I’ll present a brief summary of each chapter and solve some exercises (because the book doesn’t present the answer for the problems). Also, I’ll release the datasets used on the problems on my <a href="https://github.com/FranciscoPiccolo/franciscopiccolo.github.io/tree/master/datasets/livro-econometria-basica-gujarati">github</a> account. To beggin this summary, lets see what the authors say about the subject:</p>
<blockquote>
<p>“Econometry is the application of statistics in economic data”.</p>
</blockquote>
<blockquote>
<p>“Econometry is the quantiative analysis of economic phenomena”.</p>
</blockquote>
<div class="figure">
<img src="/images/2020-03-03_econometria_basica.png" alt="Book cover" />
<p class="caption">Book cover</p>
</div>
</div>
<div id="chapter-1---the-nature-of-regression-analysis" class="section level2">
<h2><strong>Chapter 1 - The nature of regression analysis</strong></h2>
<p>This chapter presents some foundations of regression analysis. First it’s defined the concept and then its goals. Accordingly with the authors, regression is the study of the dependence of one variable in relation to one or more independent variables.</p>
<p>Following, the authors indicate the difference between statistical relations and deterministic relations, presenting some scenarios for each one. Also, it’s explained the difference between regression and causality and between regression and correlation.</p>
<p>Now let’s see some problems presented in this chapter.</p>
<div id="exercises" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="bring-the-dataset-from-table-1.3-related-to-consumer-price-index-to-answer-the-following-questions." class="section level3">
<h3><strong>1.1) Bring the dataset from table 1.3 related to consumer price index to answer the following questions.</strong></h3>
<pre class="r"><code>tbl_1.3 &lt;- read.table(file = paste(path,&quot;tabela_1.3_IPC.txt&quot;,
                                   sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;,&quot;)

# Random sample of the dataset
tbl_1.3[sample(nrow(tbl_1.3),5), ] %&gt;% as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 8
##    year   usa canada japan france germany italy    uk
##   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  1983  99.6   100.  99.8   100.    100.  101.  99.8
## 2  2002 180.    173. 119     163.    147.  244. 207  
## 3  2001 177.    169. 120.    160.    145.  238. 204. 
## 4  1992 140.    145. 117     140.    122.  180. 163. 
## 5  2003 184     178. 119.    167.    148.  251. 213</code></pre>
</div>
<div id="b-graphically-represent-the-inflation-rate-of-each-country." class="section level3">
<h3><strong>b) Graphically represent the inflation rate of each country.</strong></h3>
<pre class="r"><code>tbl_1.3 %&gt;%
  tidyr::gather(&quot;campo&quot;, &quot;valor&quot;, 2:8) %&gt;% 
  ggplot()+
  geom_line(mapping = aes(x = year, y = valor, group = campo, color = campo),
            size = 1)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  labs(title = &quot;Anual inflation for selected countries&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="the-table-1.6-represents-the-budget-used-with-advertising-and-the-retention-of-impressions-from-customers-based-in-a-survey-with-4.000-people." class="section level3">
<h3><strong>1.7) The table 1.6 represents the budget used with advertising and the retention of impressions from customers, based in a survey with 4.000 people.</strong></h3>
<pre class="r"><code>tbl_1.6 &lt;- read.table(file = paste(path,
                                   &quot;tabela_1.6_advertising.txt&quot;,
                                   sep = &quot;&quot;),
                      sep = &quot;;&quot;,
                      header = T, 
                      dec = &quot;,&quot;
                      )

# Sample from the dataset
tbl_1.6[sample(nrow(tbl_1.6),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 3
##   company      impressions investment
##   &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt;
## 1 Meow_Mix            12.3        7.6
## 2 Levis               40.8       27  
## 3 Fed_Express         21.9       22.9
## 4 Oscar_Meyer         23.4        9.2
## 5 Calvin_Klein        12          5</code></pre>
</div>
<div id="a-present-the-relation-between-impressions-and-advertising-expenditures." class="section level3">
<h3><strong>a) Present the relation between impressions and advertising expenditures.</strong></h3>
<pre class="r"><code>tbl_1.6 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = investment, y = impressions))+
  geom_smooth(mapping = aes(x = investment, y = impressions),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Investment and Impressions relation&quot;,
       x = &quot;Investment&quot;,
       y = &quot;Impressions&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-2---regression-analysis-with-two-variables-some-basic-ideas" class="section level2">
<h2><strong>Chapter 2 - Regression analysis with two variables: Some basic ideas</strong></h2>
<p>In this chapter the authors go a little deep on simple regression models, introducing the concept of <strong>expected value of Y</strong>. One of the key findings of this chapter is…</p>
<blockquote>
<p>“The essence of regression models is to supply conditions to explain the average value of the Y variable throughout one equation.”</p>
</blockquote>
<p>The authors also affirm that the linear regression model will try to estimate the <strong>population regression function</strong> using economic theories and sample date of the population being studied. With the sample data occur the creation of the regression model for this sample that will be a proxy for the population regression function. Thus, regression models have to deal with inferences and uncertainty.</p>
<p>Also, in this chapter the authors explain that the models of the book are linear on the independent variables (x1, x2, etc), but not on the parameters (<span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, etc). At the end of the chapter, the authors explain about the stochastic erros of the model, that represent all other variables that weren’t included in the developed model.</p>
<div id="exercises-1" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="use-the-table-2.8-with-data-about-food-expenses-and-total-expenses-from-55-houses-in-india-to-answer-the-following-questions." class="section level3">
<h3><strong>2.15) Use the table 2.8 with data about food expenses and total expenses from 55 houses in India to answer the following questions.</strong></h3>
<pre class="r"><code>tbl_2.8 &lt;- read.table(file = paste(path,
                                   &quot;tabela_2.8_expenses_in_india.txt&quot;,
                                   sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T,
                      dec = &quot;,&quot;)

# Random sample
tbl_2.8[sample(nrow(tbl_2.8),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 2
##    food total
##   &lt;int&gt; &lt;int&gt;
## 1   380   785
## 2   305   801
## 3   390   655
## 4   530   790
## 5   410   635</code></pre>
</div>
<div id="a-represent-the-data-drawing-the-food-expenses-in-the-y-axis-and-total-expenses-in-the-x-axis-and-then-plot-the-regression-line." class="section level3">
<h3><strong>a) Represent the data drawing the food expenses in the Y axis and total expenses in the X axis and then plot the regression line.</strong></h3>
<pre class="r"><code>tbl_2.8 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = total, y = food))+
  geom_smooth(mapping = aes(x = total, y = food),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Food expenditures and total expenditures&quot;,
       x = &quot;Total expenditures&quot;,
       y = &quot;Food expenditures&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-3---regression-model-with-two-variables-the-estimation-problem" class="section level2">
<h2><strong>Chapter 3 - Regression model with two variables: The estimation problem</strong></h2>
<p>The goal of this chapter will be the estimation of the populational regression function using a regression model created from a sample data of the population. The authors indicate two possible methods for this task, (i) the ordinary least squares (OLS) and (ii) the maximum likelihood. Both methods lead to the same results but the first one is easier to apply.</p>
<p>As the focus of the chapter is about estimation, the authors explain the hipothesis that support the estimation when using the OLS method. They are summed up below:</p>
<ul>
<li><p>Independency of the X variables and the error term (<span class="math inline">\(\mu\)</span>).</p></li>
<li><p>The mean value of <span class="math inline">\(\mu\)</span> should be zero. This hipothesis subjectively afirm that there’s not specification mistakes on the model.</p></li>
<li><p>Homoscedasticity of <span class="math inline">\(\mu\)</span>. In other words, there’s constant variation of the <span class="math inline">\(\mu\)</span> term.</p></li>
<li><p>There’s no correlation between <span class="math inline">\(\mu\)</span> values of the model.</p></li>
</ul>
<div id="exercises-2" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="the-following-equation-shows-the-relation-between-exchange-rate-and-inflation-rate-for-canada-and-us-in-the-period-of-1985-to-2005." class="section level3">
<h3><strong>3.19) The following equation shows the relation between exchange rate and inflation rate for Canada and US in the period of 1985 to 2005.</strong></h3>
<p><span class="math display">\[\widehat{Y_t} = 0.912 + 2.25X_t\]</span></p>
<p>Where:</p>
<p><span class="math inline">\(\widehat{Y_t}\)</span> = Canadian Exchange Rate (DC/US$)</p>
<p><span class="math inline">\(X_t\)</span> = US Inflation Rate/Canada Inflation Rate</p>
</div>
<div id="a-write-the-interpretation-for-this-regression-model." class="section level3">
<h3><strong>a) Write the interpretation for this regression model.</strong></h3>
<p><strong>A:</strong> This regression model indicates that as US inflation rate is higher than Canadian inflation rate, the Canadian currency will appreciate, thus the denominator of the fraction <span class="math inline">\(DC/US\)</span> will be lower because inflation erodes the purchasing power of the currency. The r² of 44% indicates that ther’re other variables that could support the model.</p>
</div>
<div id="b-the-positive-value-of-x-has-economic-foundations" class="section level3">
<h3><strong>b) The positive value of X has economic foundations?</strong></h3>
<p><strong>A:</strong> Yes, it does. As the inflation rate is higher in the US, the Canadian dolar will be traded by more US dolars.</p>
</div>
<div id="the-next-table-presents-a-dataset-about-hourly-production-index-x-and-hourly-real-wages-y-for-the-industrial-sector-and-non-agricultural-sector-between-1960-and-2005" class="section level3">
<h3><strong>3.20) The next table presents a dataset about hourly production index (X) and hourly real wages (Y) for the industrial sector and non agricultural sector, between 1960 and 2005</strong></h3>
<pre class="r"><code>tbl_3.6 &lt;- read.table(file = paste(path,&quot;tabela_3.6_wage_and_productivity.txt&quot;,sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;,&quot;)

# Random sample
tbl_3.6[sample(nrow(tbl_3.6),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 5
##    year corporate non_agricultural corporate_wages non_agricultural_wages
##   &lt;int&gt;     &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;                  &lt;dbl&gt;
## 1  1999     113.             112.            108                    108. 
## 2  1978      79.3             81              89.1                   89.6
## 3  1993     100.             100.             99.7                   99.5
## 4  1979      79.3             80.7            89.3                   89.7
## 5  1973      73.4             75.3            84.3                   84.7</code></pre>
</div>
<div id="a-graphically-represent-separetely-y-versus-x-for-the-two-sectors-of-the-economy." class="section level3">
<h3><strong>a) Graphically represent separetely Y versus X for the two sectors of the economy.</strong></h3>
<pre class="r"><code>tbl_3.6 %&gt;% 
  tidyr::gather(&quot;production&quot;,&quot;prod&quot;,2:3) %&gt;% 
  tidyr::gather(&quot;wage&quot;,&quot;wag&quot;,2:3) %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = prod, y = wag, color = production))+
  geom_smooth(mapping = aes(x = prod, y = wag),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F, 
              lty = 2, 
              color = &quot;dark orange&quot;)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  labs(title = &quot;Production index and hourly wages&quot;,
       x = &quot;Production&quot;,
       y = &quot;Wage&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="c-create-a-regression-model-usgin-ols-method." class="section level3">
<h3><strong>c) Create a regression model usgin OLS method.</strong></h3>
<pre class="r"><code>tbl_3.6 %&gt;% 
  tidyr::gather(&quot;production&quot;,&quot;X&quot;,2:3) %&gt;% 
  tidyr::gather(&quot;wage&quot;,&quot;Y&quot;,2:3) -&gt; df_tbl_3.6

lm(Y~X, data = df_tbl_3.6) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y ~ X, data = df_tbl_3.6)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.6041 -1.5583  0.3798  1.8433  4.5035 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 32.633974   0.712458   45.80   &lt;2e-16 ***
## X            0.669944   0.007976   83.99   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.37 on 182 degrees of freedom
## Multiple R-squared:  0.9749, Adjusted R-squared:  0.9747 
## F-statistic:  7055 on 1 and 182 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="the-following-dataset-presents-gold-price-nyse-index-and-cpi-index-between-1974-to-2006.-use-this-dataset-to-answer-the-next-questions." class="section level3">
<h3><strong>3.22) The following dataset presents gold price, NYSE index and CPI index between 1974 to 2006. Use this dataset to answer the next questions.</strong></h3>
<pre class="r"><code>tbl_3.7 &lt;- read.table(file = 
                      paste(path,&quot;tabela_3.7_gold_nyse_cpi.txt&quot;,sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;,&quot;)

# Sample from dataset
tbl_3.7[sample(nrow(tbl_3.7),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 4
##    year  gold  nyse   cpi
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  1988  437. 1585. 118. 
## 2  1976  125.  576.  56.9
## 3  1979  307.  617.  72.6
## 4  1996  388. 3787. 157. 
## 5  1980  613.  720.  82.4</code></pre>
</div>
<div id="a-plot-gold-price-nyse-and-cpi-index-in-a-dispersion-graph." class="section level3">
<h3><strong>a) Plot Gold price, NYSE and CPI index in a dispersion graph.</strong></h3>
<pre class="r"><code>tbl_3.7 %&gt;% 
  transmute(x = gold,
            y = nyse,
            z = cpi) -&gt; df_test

x &lt;- df_test$x
y &lt;- df_test$y
z &lt;- df_test$z
  
plot3D::scatter3D(x, y, z, 
                  colvar = NULL, 
                  add = FALSE, 
                  col = &quot;blue&quot;, 
                  pch = 19, 
                  phi = 20, 
                  theta = 20, 
                  cex = .9, 
                  bty = &quot;b2&quot;, 
                  expand = .9, 
                  col.grid = &quot;darkblue&quot;, 
                  col.panel = &quot;steelblue&quot;, 
                  colkey = T, 
                  main = &quot;Table 3.7 data&quot;, 
                  xlab = &quot;gold&quot;, 
                  ylab = &quot;nyse&quot;, 
                  zlab = &quot;cpi&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-12-1.png" width="672" />
### <strong>b) Suposing that an investment works as an inflation edge if its profitability follows the inflation rate, test the following regression models using the variables of the dataset.</strong></p>
<p><span class="math display">\[gold = \beta_1 + \beta_2 + CPI + \mu\]</span>
<span class="math display">\[nyse = \beta_1 + \beta_2 + CPI + \mu\]</span></p>
<pre class="r"><code># Adjusting the dataset to take the profitability of each asset
tbl_3.7 %&gt;% 
  mutate(gold_price_var = round(gold/lag(gold, 1, default = NA), digits = 8)-1,
         nyse_index_var = round(nyse/lag(nyse, 1, default = NA), digits = 8)-1,
         ipc_index_var = round(cpi/lag(cpi, 1, default = NA), digits = 8)-1) %&gt;% 
  filter(!is.na(gold)) -&gt; tbl_3.7_df_adj</code></pre>
<pre class="r"><code>g1 &lt;- 
  tbl_3.7_df_adj %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = ipc_index_var, y = nyse_index_var))+
  geom_smooth(mapping = aes(x = ipc_index_var, y = nyse_index_var),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  expand_limits(x = 0, y = 0)+
  scale_x_continuous(labels = scales::percent)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  theme(title = element_text(size = 8))+
  labs(title = &quot;NYSE vs CPI&quot;,
       x = &quot;CPI index variation&quot;,
       y = &quot;NYSE index variation&quot;)

g2 &lt;- 
  tbl_3.7_df_adj %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = ipc_index_var, y = gold_price_var))+
  geom_smooth(mapping = aes(x = ipc_index_var, y = gold_price_var),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  expand_limits(x = 0, y = 0)+
  scale_x_continuous(labels = scales::percent)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  theme(title = element_text(size = 8))+
  labs(title = &quot;Gold Price vs CPI&quot;,
       x = &quot;CPI index variation&quot;,
       y = &quot;Gold price variation&quot;)

g1 + g2</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code># Modelo 1, gold_price ~ ipc_index
tbl_3.7_df_adj %&gt;% 
  lm(formula = gold_price_var ~ ipc_index_var) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = gold_price_var ~ ipc_index_var, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.5428 -0.1114 -0.0066  0.1246  0.5776 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   -0.11471    0.07197  -1.594  0.12148   
## ipc_index_var  3.95969    1.33692   2.962  0.00593 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.219 on 30 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.2263, Adjusted R-squared:  0.2005 
## F-statistic: 8.772 on 1 and 30 DF,  p-value: 0.005932</code></pre>
<pre class="r"><code># Modelo 2, nyse_index ~ ipc_index
tbl_3.7_df_adj %&gt;% 
  lm(formula = nyse_index_var ~ ipc_index_var) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = nyse_index_var ~ ipc_index_var, data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.24198 -0.08735  0.01355  0.09243  0.23748 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)    0.12125    0.03777   3.210  0.00316 **
## ipc_index_var -0.46036    0.70166  -0.656  0.51676   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1149 on 30 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.01415,    Adjusted R-squared:  -0.01872 
## F-statistic: 0.4305 on 1 and 30 DF,  p-value: 0.5168</code></pre>
<p><strong>A:</strong> Apparently, gold price is a better asset to edge inflation rate than the NYSE index. Althought there’re two outliers on the gold price data that are impacting the model.</p>
</div>
</div>
<div id="chapter-4---classical-normal-linear-regression-model" class="section level2">
<h2><strong>Chapter 4 - Classical Normal Linear Regression Model</strong></h2>
<p>In this chapter the authors gave attention to the classical normal linear regression model that introduces the concept of normal distrubutions of residuals. As was informed in previus chapter, the goal of the regression model is to estimate parameters (<span class="math inline">\(\hat{\beta}\)</span>) for the populational parameter (<span class="math inline">\(\beta\)</span>) and because it’s an estimation based on samples, the result will vary on each model and the variation of (<span class="math inline">\(\hat{\beta}\)</span>) will follow a particular distribution.</p>
<p>Knowing that <span class="math inline">\(\hat{\beta}\)</span> should follow a particular distribution, the most desired one is the normal distribution. To afirm that the parameter follows the normal distribution, the author prove algebrically that <span class="math inline">\(\hat{\beta}\)</span> is a linear function of the residuals <span class="math inline">\(\mu\)</span>. Then, having the premise that <span class="math inline">\(\mu\)</span> is normally distributed, the premise that <span class="math inline">\(\hat{\beta}\)</span> is normally distributed will be attended. The premise of <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\mu\)</span> being normally distributed will allow inferences about the population’s parameters using confident intervals and test of hypothesis, two techniques developed by classical statisticians.</p>
</div>
<div id="chapter-5---two-variables-regression-model-the-confidence-interval-and-hypothesis-test" class="section level2">
<h2><strong>Chapter 5 - Two variables regression model: The confidence interval and hypothesis test</strong></h2>
<p>After explaining the premises of the classical linear regression model on the last chapter, the authors develop in this chapter the hole regression model to draw inferences about the population parameters being studied. Before they start the explanation, they indicate that the expected value of the sample parameter will be the true value of the populational parameter. Mathematically it means that <span class="math inline">\(e(\hat{\beta}) = \beta\)</span>. Also, they indicate that the reliability of the parameters created by the regression model will be measured by the <strong>standard error</strong>, and this metric will support the development of the confidence interval and test of hypothesis.</p>
<p>After explaining this initial assumptions, the authors explain how to develop confidence intervals for the parameters and how to conduct the test of hypothesis.</p>
<p><strong>Confidence Intervals</strong></p>
<p>In this method, intervals are constructed by two distinct methods. The first is when the populational variance is known and the second is when it isn’t known. For the first case, the <strong>t distribution</strong> is used, for the second, the <strong>normal distribution</strong> is used. In both cases, the interval range will be proportional to the standard error.</p>
<p>The following equation present this method:</p>
<p><span class="math display">\[\hat{\beta} +/- (z|t_\frac{\alpha}{2}  ep(\hat{\beta}))\]</span></p>
<p><strong>Hypothesis Test</strong></p>
<p>In this method, the researcher can use the confidence interval explained before or the test of significance that will create the p-value to allow the conclusion about the hypothesis.</p>
<div id="exercises-3" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="consult-the-regression-model-of-the-equation-3.7.3-to-answer-the-following-questions." class="section level3">
<h3><strong>5.3) Consult the regression model of the equation (3.7.3) to answer the following questions.</strong></h3>
<p><span class="math display">\[\hat{Y} = 14.4773 + 0.0022X_i\]</span></p>
<p><span class="math display">\[ep(\hat{\beta_1}) = 6.1523; ep(\hat{\beta_2}) = 0.00032; r² = 0.6023\]</span></p>
</div>
<div id="a-the-estimated-intercept-beta-1-is-statistically-significant-at-5-what-is-the-null-hypothesis-for-this-scenario" class="section level3">
<h3><strong>a) The estimated intercept (beta 1) is statistically significant at 5%? What is the Null hypothesis for this scenario?</strong></h3>
<p><strong>A:</strong> The significance of the intercept (<span class="math inline">\(\beta_1\)</span>) will be validated by testing the hypothesis that it is zero. Then we have H0: <span class="math inline">\(\beta_1 = 0\)</span> and H1: <span class="math inline">\(\beta_1 \neq 0\)</span>. For this test we’ll generate the <strong>t-statistics</strong> (from the student distribution, because the population variation is unknown) in order to calculate the p-value and make the conclusion about the Null hypothesis (reject H0 or fail to reject it).</p>
<p><strong>t-statistics</strong></p>
<p><span class="math display">\[\frac{\hat{\beta_1}-\beta_1}{ep(\hat{\beta_1})}\]</span></p>
<p><span class="math display">\[\frac{14,47-0}{6.1523} = 2.35\]</span></p>
<p>Having the <strong>t-statistics</strong>, it’s possible to calculate the <strong>p-value</strong>. For this, we have to use the distribution function for the t distribution, that will ask for the t-statistics and degrees of freedom. This function will show the area under de curve that would contain this t-statistics given this degree of freedom and when we make 1 minus this area under de curve and multiply the result by 2 (because it’s a bicaudal test), we generate the p-value.</p>
<p>The area under the curve for the t distribution with a t-statistics of 2.35 and 34 df is 0.9876. In other words, 98.76% probability of having a value inside the range (-2.35 to 2.35) given 34 df.</p>
<pre class="r"><code>pt(2.35, 34)</code></pre>
<pre><code>## [1] 0.9876413</code></pre>
<p>The p-value will be the probability of having a value outside this range, in this case 1-0.9876. Also, as is a bicaudal test (because H1 is <span class="math inline">\(\beta_1\neq 0\)</span>) it’s necessary to multiply the value by 2. Then, we have:</p>
<pre class="r"><code># p-value calculation in R
(1-pt(2.35, 34))*2</code></pre>
<pre><code>## [1] 0.02471741</code></pre>
<p>Concluding, with a p-value of 2.437% it’s possible to reject H0 because the significance level is 5%. In other words the intercept of the model is statistically significant at 5% of significance level.</p>
</div>
<div id="b-the-slope-is-statistically-significant-at-5-of-confidence-level-what-is-the-null-hypothesis-for-this-test" class="section level3">
<h3><strong>b) The slope is statistically significant at 5% of confidence level? What is the Null hypothesis for this test?</strong></h3>
<p>The same approach can be used for this question. The hypothesis are H0: <span class="math inline">\(\beta_2 = 0\)</span> e H1: <span class="math inline">\(\beta_2 \neq 0\)</span>. The same equations can be used, adjusting the values of the variables.</p>
<p><strong>t-statistics</strong></p>
<p><span class="math display">\[\frac{0.0022}{0.00032} = 6.88\]</span></p>
<p>This <strong>t-statistics</strong> will generate a pretty low <strong>p-value</strong>, thus the Null hypothesis can be rejected without concern and the conclusion that the parameter of the slope is statistically significant at 5% of confidence can be made.</p>
<pre class="r"><code># p-value using the t-statistics calculated before
(1-pt(6.88, 34))*2</code></pre>
<pre><code>## [1] 6.349556e-08</code></pre>
</div>
<div id="the-table-5.5-present-data-about-teachers-earnings-and-students-expenditures-in-1985-for-50-us-states.-in-order-to-verify-the-relation-between-earnings-and-expenditures-the-following-model-was-sugested." class="section level3">
<h3><strong>5.9) The table 5.5 present data about teachers’ earnings and students’ expenditures in 1985 for 50 US states. In order to verify the relation between earnings and expenditures, the following model was sugested.</strong></h3>
<p><span class="math display">\[r = \beta_1+\beta_2\times g + \mu\]</span></p>
<p>Where:</p>
<p>r = teachers’ earnings</p>
<p>g = students’ expenditures</p>
<pre class="r"><code>tbl_5.5 &lt;- 
  read.table(file = paste(path,&quot;tabela_5.5_wage_and_college_expenditures.txt&quot;, sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;.&quot;)

# Dataset sample
tbl_5.5[sample(nrow(tbl_5.5),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 3
##     obs  wage expense
##   &lt;int&gt; &lt;dbl&gt;   &lt;int&gt;
## 1    44  24.6    2829
## 2    15  27.4    3982
## 3    26  20.6    2821
## 4    36  20.5    3124
## 5    17  22.0    3155</code></pre>
</div>
<div id="b-create-a-regression-model-to-analyze-the-standard-error-and-r²." class="section level3">
<h3><strong>b) Create a regression model to analyze the standard error and r².</strong></h3>
<pre class="r"><code>tbl_5.5 %&gt;%
  lm(formula = wage ~ expense) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = wage ~ expense, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8480 -1.8446 -0.2175  1.6600  5.5293 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.213e+01  1.197e+00   10.13 1.31e-13 ***
## expense     3.308e-03  3.117e-04   10.61 2.71e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.325 on 49 degrees of freedom
## Multiple R-squared:  0.6968, Adjusted R-squared:  0.6906 
## F-statistic: 112.6 on 1 and 49 DF,  p-value: 2.707e-14</code></pre>
<p>With the output, the following equations is generated:</p>
<p><span class="math display">\[\hat{r} = \hat{\beta_1}+\hat{\beta_2}\times g+\mu\]</span></p>
<p><span class="math display">\[\hat{r} = 12.13+0.003308\times g+\mu\]</span></p>
</div>
<div id="d-test-if-the-true-parameter-of-beta-2-is-3.-use-a-confidence-interval-for-this-task." class="section level3">
<h3><strong>d) Test if the true parameter of beta 2 is 3. Use a confidence interval for this task.</strong></h3>
<p><strong>A:</strong> For this scenario, we have: H0: <span class="math inline">\(\beta_2 = 3\)</span> and H1: <span class="math inline">\(\beta_2 \neq 3\)</span>. The equation to build the limits is:</p>
<p><span class="math display">\[\hat{\beta_2} +/- t_\frac{\alpha}{2} \times ep(\hat{\beta_2})\]</span>
Using the <strong>t distribution</strong> because population variance is unknown and <span class="math inline">\(\frac{\alpha}{2}\)</span> because is a two side test (i.e. H1: <span class="math inline">\(\beta_2 \neq 3\)</span>).</p>
<p>To solve this equation, it’s necessary to get the <strong>critical t value</strong> (not <strong>t-statistics</strong> in this case). This value is calculated with the significance level of [1-(5%/2)] and degrees of freedom of the model (49).</p>
<pre class="r"><code># Critical t value
qt(0.975, df = 49)</code></pre>
<pre><code>## [1] 2.009575</code></pre>
<p>Using this value with the parameter value and its standard error, it’s possible to build the interval.</p>
<p><span class="math inline">\(\hat{\beta_2} = 0.003308\)</span></p>
<p><span class="math inline">\(ep(\hat{\beta_2}) = 0.0003117\)</span></p>
<p>Critical t value = 2.009575</p>
<p><span class="math display">\[\hat{\beta_2} +/- t_\frac{\alpha}{2} \times ep(\hat{\beta_2})\]</span></p>
<p><span class="math display">\[0.003308 +/- 2.009575 \times 0.0003117\]</span></p>
<p>Then, the interval is (0.00268 e 0.00393). Allowing the rejection of the Null hypothesis because it doesn’t contain the requested value of 3.</p>
<p><strong>Ps:</strong> If I would make this test using test of hypothesis and p-value for the decision, I would generate a **t-statistics* of:</p>
<p><span class="math display">\[|\frac{\hat{\beta_2} - \beta_2}{ep(\hat{\beta_2})}|\]</span>
<span class="math display">\[|\frac{0.003308 - 3}{0.0003117}| = 9.614,026\]</span>
The p-value for this <strong>t-statistics</strong> is almost 0, reforcing the rejection of the Null Hypothesis. But note that this p-value is different from the p-value generated from R of <strong>2.707e-14</strong>. This p-value considers a Null Hypothesis of <span class="math inline">\(\hat{\beta_2} = 0\)</span>, instead of 3. Thus, R makes the following calculation to get this result:</p>
<p><span class="math display">\[|\frac{0.003308 - 0}{0.0003117}| = 10,61277\]</span></p>
<pre class="r"><code>(1-pt(0.003308/0.0003117, 49))*2</code></pre>
<pre><code>## [1] 2.708944e-14</code></pre>
</div>
<div id="return-to-the-exercise-3.22-to-use-the-dataset." class="section level3">
<h3><strong>5.13) Return to the exercise 3.22 to use the dataset.</strong></h3>
<pre class="r"><code># Dataset sample
tbl_3.7[sample(nrow(tbl_1.3),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 4
##    year  gold  nyse   cpi
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  1976  125.  576.  56.9
## 2  1994  384  2687. 148. 
## 3  1992  344. 2422. 140. 
## 4  1983  424.  980.  99.6
## 5  1979  307.  617.  72.6</code></pre>
</div>
<div id="b-the-regressions-residuals-are-normally-distributed" class="section level3">
<h3><strong>b) The regression’s residuals are normally distributed?</strong></h3>
<p><strong>A:</strong> Yes. The following graph will present the histogram of their values.</p>
<pre class="r"><code># Extracting the residuals from the model
tbl_3.7 %&gt;%
  lm(formula = cpi ~ gold) %&gt;%
  residuals() %&gt;% 
  data.frame(a = .) -&gt; res_1
  
res_1 %&gt;%
  ggplot()+
  geom_histogram(mapping = aes(x = a), fill = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Regression model&#39;s residuals distribution&quot;,
       subtitle = &quot;IPC and gold price relation&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre class="r"><code># Extracting the residuals from the model
tbl_3.7 %&gt;%
  lm(formula = cpi ~ nyse) %&gt;%
  residuals() %&gt;% 
  data.frame(a = .) -&gt; res_2
  
res_2 %&gt;%
  ggplot()+
  geom_histogram(mapping = aes(x = a), fill = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Regression model&#39;s residuals distribution&quot;,
       subtitle = &quot;IPC and NYSE index relation&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
<div id="c-in-the-gold-price-regression-model-test-the-hypothesis-that-the-second-parameter-is-1." class="section level3">
<h3><strong>c) In the gold price regression model, test the hypothesis that the second parameter is 1.</strong></h3>
<p><strong>A:</strong> The regression model generated a <span class="math inline">\(\beta_2\)</span> of 0.1693 with a standard error of 0.06624. These values generated a t statistics of 2.556 and a p-value of 0.0157, but considering the Null hypothesis of H0: <span class="math inline">\(\beta_2 = 0\)</span>. To answer the question (H0: <span class="math inline">\(\beta_2 = 1\)</span>), it’s necessary to put these values on the equation below to recalculate the t statistics and the p-value:</p>
<p><span class="math display">\[t.statistics = \frac{|\hat{\beta_2} - \beta_2|}{SEM}\]</span></p>
<p><span class="math display">\[t.statistics = \frac{|0.1693 - 1|}{0.06624} = 12.54076\]</span></p>
<p>This value of t will generate a lower p-value, as seging below.</p>
<pre class="r"><code>(1-pt(12.54076, 31))*2</code></pre>
<pre><code>## [1] 1.112443e-13</code></pre>
<p>Then, the Null hypothesis H0: <span class="math inline">\(\beta_2 = 1\)</span> can be rejected, allowing the conclusion that Gold Price is not a perfect edge asset for the inflation rate.</p>
</div>
</div>
<div id="chapter-6---extensions-for-the-linear-regression-models-of-two-variables" class="section level2">
<h2><strong>Chapter 6 - Extensions for the Linear Regression Models of two Variables</strong></h2>
<p>In this chapter the authors present some additional regression models that will be more appropriated in some circunstances. These models are different, but the premise of linearity of parameters is unbroken.</p>
<p>The first model is the <strong>regression model that pass by the origin</strong> that has the form:</p>
<p><span class="math display">\[Y_i = \beta_2 X_i + \mu_i\]</span>
This model is recommended when the theory behind it indicates that there’s no need for a intercept. Without the theory to indicate it, the authors recommend to always maintain the intercept and let the statistics inform if it will be relevant.</p>
<p>Continuing, the authors explain about scales and unities on the models, regarding the difference that can exist in scales between the dependent variables and Y. They show that the conclusion won’t change because these differences, but some algebric manipulations will be need. Also, they presented the standard variables on the regression model, used to analyze the impact of each independent variable on Y and obviously indicated to regression models with multiple variables. The standard variable has the form:</p>
<p><span class="math display">\[Y* = \frac{Y_j - \bar{Y}}{S_Y}\]</span></p>
<p><span class="math display">\[X* = \frac{X_j - \bar{X}}{S_X}\]</span></p>
<p>The next model presented is the logarithmic regression models, like log-log, lin-log and log-lin. And the last model presented is the reciprocal model, that has the following form:</p>
<p><span class="math display">\[Y = \beta_1 + \beta_2 (\frac{1}{X}) + \mu\]</span></p>
<div id="exercises-4" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="the-following-regressions-were-obtained-from-a-dataset-that-has-y-as-texacos-monthly-returns-and-x-as-markets-monthly-return-both-in-percentual." class="section level3">
<h3><strong>6.2 The following regressions were obtained from a dataset that has Y as Texaco’s monthly returns and X as market’s monthly return, both in percentual.</strong></h3>
<p><strong>(i)</strong> <span class="math inline">\(\hat{Y} = 0.00681 + 0.75815 X_t\)</span></p>
<p><span class="math inline">\(ep(\hat{\beta_0}) = 0.02596\)</span>; <span class="math inline">\(ep(\hat{\beta_1}) = 0.27009\)</span></p>
<p>p-value: <span class="math inline">\(\hat{\beta_0} = 0.7984\)</span>; <span class="math inline">\(\hat{\beta_1} = 0.0186\)</span></p>
<p>r² = 44.06%</p>
<p><strong>(ii)</strong> <span class="math inline">\(\hat{Y} = 0.76214 X_t\)</span></p>
<p><span class="math inline">\(ep(\hat{\beta_1}) = 0.265799\)</span></p>
<p>p-value: <span class="math inline">\(\hat{\beta_1} = 0.0131\)</span></p>
<p>r² = 43.68%</p>
</div>
<div id="f-the-normality-jarque-beta-statistics-is-1.1167-for-the-first-model-and-1.1170-for-the-second-model.-what-conclusions-can-be-made-from-this-values" class="section level3">
<h3><strong>f) The normality Jarque-Beta statistics is 1.1167 for the first model and 1.1170 for the second model. What conclusions can be made from this values?</strong></h3>
<p><strong>A:</strong> When the result of this test is far from zero, it indicates that the data isn’t normally distributed. Then, the first model with lower jarque-bera value has it’s residuals closer to a normal distribution than the second model. The Null hypothesis of this test is that residuals are normally distributed, thus <strong>H0: JB = 0</strong> and <strong>H1: JB &gt; 0</strong>.</p>
</div>
<div id="the-table-6.8-presents-data-about-investment-and-saving-for-21-countries.-this-values-are-an-average-obtained-from-1960-to-1974." class="section level3">
<h3><strong>6.15) The table 6.8 presents data about investment and saving for 21 countries. This values are an average obtained from 1960 to 1974.</strong></h3>
<pre class="r"><code>tbl_6.8 &lt;- read.table(file = 
                      paste(path,&quot;tabela_6.8_investment_and_saving.txt&quot;,sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;.&quot;)

# Dataset sample
tbl_6.8[sample(nrow(tbl_6.8),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 3
##   country    investment saving
##   &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;
## 1 norway          0.278  0.299
## 2 finland         0.288  0.305
## 3 greece          0.219  0.248
## 4 new_zeland      0.232  0.249
## 5 france          0.254  0.26</code></pre>
</div>
<div id="a-graphically-represent-the-relation-between-investment-and-saving" class="section level3">
<h3><strong>a) Graphically represent the relation between investment and saving</strong></h3>
<pre class="r"><code>tbl_6.8 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = saving, y = investment), shape = 20)+
  scale_y_continuous(labels = scales::percent)+
  scale_x_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;Investment and Saving relation&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
<div id="c-develop-a-regression-model-using-a-lin-lin-and-a-log-log-model-approach." class="section level3">
<h3><strong>c) Develop a regression model using a lin-lin and a log-log model approach.</strong></h3>
<p><span class="math display">\[Y_i = \beta_1 + \beta_2 X_i + \mu_i\]</span>
Where:</p>
<p><span class="math inline">\(Y_i = investment\)</span></p>
<p><span class="math inline">\(X_i = saving\)</span></p>
<pre class="r"><code># Lin-lin model
tbl_6.8 %&gt;% 
  lm(formula = investment ~ saving) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = investment ~ saving, data = .)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.023496 -0.014216  0.000176  0.008463  0.040120 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.01734    0.02218  -0.782    0.444    
## saving       1.04772    0.08572  12.222  1.9e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01595 on 19 degrees of freedom
## Multiple R-squared:  0.8872, Adjusted R-squared:  0.8812 
## F-statistic: 149.4 on 1 and 19 DF,  p-value: 1.899e-10</code></pre>
<pre class="r"><code># Log-log model
tbl_6.8 %&gt;% 
  lm(formula = log(investment) ~ log(saving)) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(investment) ~ log(saving), data = .)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.104399 -0.046031  0.001533  0.043086  0.140147 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.06302    0.12411   0.508    0.617    
## log(saving)  1.06308    0.08960  11.865 3.13e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.06391 on 19 degrees of freedom
## Multiple R-squared:  0.8811, Adjusted R-squared:  0.8748 
## F-statistic: 140.8 on 1 and 19 DF,  p-value: 3.134e-10</code></pre>
</div>
<div id="d-how-would-you-interpret-the-slope-coeficient-in-the-lin-lin-model-and-on-the-log-log-model-therere-any-differences-in-the-interpretation" class="section level3">
<h3><strong>d) How would you interpret the slope coeficient in the lin-lin model? And on the log-log model? There’re any differences in the interpretation?</strong></h3>
<p><strong>A:</strong> The lin-lin model has the coeficient of 1.04772, meaning that in increase in 1 pp in saving would generate an increase of 1.04772 pp in investment. On the other hand, in the log-log model, the coeficient is 1.06308, meaning that an 1% increase in saving would cause and 1.06308% increase in investment. There’s a difference in interpretation, because the first model is about changing percentual points and the second is about percentual variation.</p>
</div>
<div id="the-table-6.10-presents-data-about-total-consumption-and-advertising-expendes-for-29-product-categories-in-uk." class="section level3">
<h3><strong>6.19) The table 6.10 presents data about total consumption and advertising expendes for 29 product categories in UK.</strong></h3>
<pre class="r"><code>tbl_6.10 &lt;- read.table(file = 
                      paste(path,&quot;tabela_6.10_advertising_and_total_expenditures.txt&quot;,sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;.&quot;)</code></pre>
</div>
<div id="a-which-regression-model-would-better-represent-the-data" class="section level3">
<h3><strong>a) Which regression model would better represent the data?</strong></h3>
<p><strong>A:</strong> In order to answer this question, it’s better to try different regression models and visualize graphically which of them fit better on the data, taking care to not overfit.</p>
<pre class="r"><code>tbl_6.10 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = advertising, y = consumo))+
  geom_smooth(mapping = aes(x = advertising, y = consumo),
              formula = y ~ x,
              method = &quot;lm&quot;,
              se = F)+
  geom_smooth(mapping = aes(x = advertising, y = consumo),
            formula = &quot;y ~ I(x^0.5)&quot;,
            method = &quot;lm&quot;,
            se = F,
            color = &quot;red&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code>tbl_6.10 %&gt;% 
  lm(formula = &quot;consumo ~ I(advertising^.5)&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;consumo ~ I(advertising^.5)&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4133.5 -1497.1  -779.5   318.6  8730.6 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)        -106.556   1045.327  -0.102   0.9196  
## I(advertising^0.5)   16.775      7.023   2.388   0.0242 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2928 on 27 degrees of freedom
## Multiple R-squared:  0.1744, Adjusted R-squared:  0.1439 
## F-statistic: 5.705 on 1 and 27 DF,  p-value: 0.02417</code></pre>
<pre class="r"><code>tbl_6.10 %&gt;% 
  lm(formula = &quot;consumo ~ advertising&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;consumo ~ advertising&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5056.7 -1280.6  -831.6   -14.2  8100.8 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 857.03870  720.14179   1.190   0.2444  
## advertising   0.05277    0.02146   2.458   0.0207 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2913 on 27 degrees of freedom
## Multiple R-squared:  0.1829, Adjusted R-squared:  0.1526 
## F-statistic: 6.044 on 1 and 27 DF,  p-value: 0.02066</code></pre>
<p>It seems that the linear regression model fits better the data, because it generated a higher R² and a lower p-value for the slope.</p>
</div>
<div id="recall-the-3.3-example-from-chapter-3-to-answer-the-following-questions" class="section level3">
<h3><strong>6.20) Recall the 3.3 example from chapter 3 to answer the following questions:</strong></h3>
<pre class="r"><code>tbl_3.3 &lt;- read.table(file = 
                      paste(path,&quot;tabela_3.3_telefonia_movel_e_computadores.txt&quot;,sep = &quot;&quot;),
                      sep = &quot;;&quot;,
                      header = T, 
                      dec = &quot;,&quot;)

# Dataset sample
tbl_3.3[sample(nrow(tbl_3.3),10), ]</code></pre>
<pre><code>##       county cellphone    pc per_capita_income
## 12   germany     78.52 48.47             27610
## 9    equator     18.92  3.24              3940
## 17 indonesia      8.74  1.19              3210
## 10     egypt     54.80 19.20              3940
## 26     spain     16.19  6.91             22150
## 23    poland     45.09 14.20             11210
## 3    belgium     79.28 31.81             28920
## 20    mexico     29.47  8.30              8980
## 1  argentina     17.76  8.20             11410
## 15   hungary     76.88 10.84             13840</code></pre>
</div>
<div id="a-graphically-represent-cellphone-demand-in-relation-to-per-capita-income-adjusted-by-ppp." class="section level3">
<h3><strong>a) Graphically represent cellphone demand in relation to per capita income adjusted by PPP.</strong></h3>
<pre class="r"><code>tbl_3.3 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = cellphone, y = per_capita_income))+
  theme_graph()+
  labs(title = &quot;Cellphone vs Per capital income&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
</div>
<div id="b-graphically-represent-cellphone-demand-in-relation-to-per-capita-income-both-adjusted-by-logarithms." class="section level3">
<h3><strong>b) Graphically represent cellphone demand in relation to per capita income, both adjusted by logarithms.</strong></h3>
<pre class="r"><code>tbl_3.3 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = cellphone, y = per_capita_income))+
  scale_y_log10()+
  scale_x_log10()+
  theme_graph()+
  labs(title = &quot;Cellphone vs Per capita income, log adjusted&quot;)</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
</div>
<div id="d-analysing-these-two-graphs-do-you-believe-that-a-log-log-model-can-fit-better-the-data-develop-the-regression-model-for-a-log-log-method." class="section level3">
<h3><strong>d) Analysing these two graphs, do you believe that a log-log model can fit better the data? Develop the regression model for a log-log method.</strong></h3>
<pre class="r"><code>tbl_3.3 %&gt;% 
  lm(formula = &quot;log(cellphone) ~ log(per_capita_income)&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;log(cellphone) ~ log(per_capita_income)&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.0882 -0.3814  0.3393  0.5389  0.9273 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)              1.6758     1.5428   1.086    0.286
## log(per_capita_income)   0.2149     0.1636   1.313    0.199
## 
## Residual standard error: 0.7919 on 29 degrees of freedom
## Multiple R-squared:  0.05611,    Adjusted R-squared:  0.02357 
## F-statistic: 1.724 on 1 and 29 DF,  p-value: 0.1995</code></pre>
<pre class="r"><code>tbl_3.3 %&gt;% 
  lm(formula = &quot;cellphone ~ per_capita_income&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;cellphone ~ per_capita_income&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -56.172 -21.258   5.764  17.062  44.253 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       3.497e+01  8.636e+00   4.050 0.000349 ***
## per_capita_income 9.826e-04  4.344e-04   2.262 0.031377 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 27.1 on 29 degrees of freedom
## Multiple R-squared:   0.15,  Adjusted R-squared:  0.1206 
## F-statistic: 5.116 on 1 and 29 DF,  p-value: 0.03138</code></pre>
<p><strong>A:</strong> The lin-lin model generated a slope with a lower p-value and a higher r². Thus I would prefer this model instead of the log-log model.</p>
</div>
</div>
<div id="chapter-7---multiple-regression-analysis-the-estimation-problem" class="section level2">
<h2><strong>Chapter 7 - Multiple Regression Analysis: The Estimation Problem</strong></h2>
<p>The authors in this chapter explain how multiple regression have to be analyzed. They start presenting the hypothesis of this model, that are the same of simple regression models and then they explain that the coeficients of the model will measure the effect on the mean value of Y considering the other variables constant.</p>
<p>This consideration is made by removing the effect of the variation in one independent variable on the other (X1 and X2). Multiple regression models will generate the R² metric, that is different from the r², because it’ll represent the proportion of Y that is explained by all other independente variables of the model (X1, X2, X3…).</p>
<p>Then, the author continue explaining the impact thar multiple regression models can suffer by specification errors in the design of the model. In this case, the error would be a missing independent variable.</p>
<p>Also, the author shows that in a multiple regression model, you’ll have an Adjusted R² that will inform the same information of R² but considering the amount of variables including in the model. The Adjusted R² in this case, penalizes the inclusion of two many independente variables. At the end, they explain that in regression models, it’s not relevant to seek a high Adjusted R², because the goal is to make reliable estimation of the populational parameters <span class="math inline">\(\beta\)</span> in order to make predictions.</p>
<div id="exercises-5" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="consider-the-table-7.5-below" class="section level3">
<h3><strong>7.1 Consider the table 7.5 below:</strong></h3>
<pre><code>##   y x_2 x_3
## 1 1   1   2
## 2 2   2   1
## 3 8   3  -3</code></pre>
<p>Build the following models:</p>
<p><span class="math inline">\(Y = \alpha_1 + \alpha_2 X_2 + \mu\)</span></p>
<p><span class="math inline">\(Y = \lambda_1 + \lambda_3 X_3 + \mu\)</span></p>
<p><span class="math inline">\(Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \mu\)</span></p>
<p>Verify if <span class="math inline">\(\alpha_2 = \beta_2\)</span>, <span class="math inline">\(\lambda_3 = \beta_3\)</span>.</p>
<p><strong>A:</strong> Building the models.</p>
<pre class="r"><code>df &lt;- data.frame(y = c(1,3,8),
                 x_2 = c(1,2,3),
                 x_3 = c(2,1,-3))

# First model
df %&gt;% 
  lm(formula = y ~ x_2) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x_2, data = .)
## 
## Residuals:
##    1    2    3 
##  0.5 -1.0  0.5 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   -3.000      1.871  -1.604    0.355
## x_2            3.500      0.866   4.041    0.154
## 
## Residual standard error: 1.225 on 1 degrees of freedom
## Multiple R-squared:  0.9423, Adjusted R-squared:  0.8846 
## F-statistic: 16.33 on 1 and 1 DF,  p-value: 0.1544</code></pre>
<pre class="r"><code># Second model
df %&gt;% 
  lm(formula = y ~ x_3) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x_3, data = .)
## 
## Residuals:
##        1        2        3 
## -0.28571  0.35714 -0.07143 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)   4.0000     0.2673   14.97   0.0425 *
## x_3          -1.3571     0.1237  -10.97   0.0579 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4629 on 1 degrees of freedom
## Multiple R-squared:  0.9918, Adjusted R-squared:  0.9835 
## F-statistic: 120.3 on 1 and 1 DF,  p-value: 0.05787</code></pre>
<pre class="r"><code># Third model
df %&gt;% 
  lm(formula = y ~ x_2 + x_3) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x_2 + x_3, data = .)
## 
## Residuals:
## ALL 3 residuals are 0: no residual degrees of freedom!
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)        2         NA      NA       NA
## x_2                1         NA      NA       NA
## x_3               -1         NA      NA       NA
## 
## Residual standard error: NaN on 0 degrees of freedom
## Multiple R-squared:      1,  Adjusted R-squared:    NaN 
## F-statistic:   NaN on 2 and 0 DF,  p-value: NA</code></pre>
<p>Then, after running the models in R, we have:</p>
<p><span class="math inline">\(Y = -3 + 3.5 X_2 + \mu\)</span></p>
<p><span class="math inline">\(Y = 4 - 1.3571X_3 + \mu\)</span></p>
<p><span class="math inline">\(Y = 2 + 1 X_2 - 1 X_3 + \mu\)</span></p>
<p>With <span class="math inline">\(\alpha_2 = 3.5\)</span> and <span class="math inline">\(\beta_2 = 1\)</span>; <span class="math inline">\(\lambda_3 = -1.3571\)</span> and <span class="math inline">\(\beta_3 = -1\)</span>. These differences indicate that the independent variables have correlation between then, thus the coeficients diverge when putting together and when putther separatelly on the models.</p>
</div>
<div id="suposing-that-you-have-the-following-equations" class="section level3">
<h3><strong>7.13) Suposing that you have the following equations:</strong></h3>
<p><span class="math display">\[Y = \alpha + \alpha_2 X + \mu\]</span>
<span class="math display">\[Z = \beta + \beta_2 X + \mu\]</span>
Onde Y = consumo, Z = poupança e X = (Y + Z).</p>
</div>
<div id="a-what-is-the-relation-between-alpha-2-and-beta-2-show-your-calculus." class="section level3">
<h3><strong>a) What is the relation between alpha 2 and beta 2? Show your calculus.</strong></h3>
<p><strong>A:</strong> The slope of the models summed is 1 and their standard error is equal. Following is a random dataset to exemplify this situation.</p>
<pre class="r"><code># Random dataset
df &lt;- data.frame(y = rnorm(100, 10, 3),
           z = rnorm(100, 25, 5))

g1 &lt;- df %&gt;% 
  mutate(x = y + z) %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = x, y = y))+
  geom_smooth(mapping = aes(x = x, y = y),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()

g2 &lt;- df %&gt;% 
  mutate(x = y + z) %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = x, y = z))+
  geom_smooth(mapping = aes(x = x, y = z),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()

g1 + g2</code></pre>
<p><img src="/posts/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>Creating the regression model for the two variables (y and z).</p>
<pre class="r"><code>df %&gt;% 
  mutate(x = y + z) %&gt;% 
  lm(formula = &quot;y ~ x&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;y ~ x&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.0051 -1.6340  0.2393  1.6858  5.3138 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.89237    1.60497   1.179    0.241    
## x            0.23212    0.04506   5.151 1.34e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.548 on 98 degrees of freedom
## Multiple R-squared:  0.2131, Adjusted R-squared:  0.205 
## F-statistic: 26.53 on 1 and 98 DF,  p-value: 1.341e-06</code></pre>
<pre class="r"><code>df %&gt;% 
  mutate(x = y + z) %&gt;% 
  lm(formula = &quot;z ~ x&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;z ~ x&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.3138 -1.6858 -0.2393  1.6340  6.0051 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.89237    1.60497  -1.179    0.241    
## x            0.76788    0.04506  17.040   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.548 on 98 degrees of freedom
## Multiple R-squared:  0.7477, Adjusted R-squared:  0.7451 
## F-statistic: 290.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="c-its-possible-to-compare-r²-of-both-models" class="section level3">
<h3><strong>c) It’s possible to compare R² of both models?</strong></h3>
<p><strong>A:</strong> No, because Y and Z can have different weights on X, thus the R² would include this difference and preclude this comparisson.</p>
</div>
<div id="the-table-7.6-presents-roses-demand-by-quarter." class="section level3">
<h3><strong>7.6) The table 7.6 presents roses demand by quarter.</strong></h3>
<pre class="r"><code>tbl_7.6 &lt;- read.table(file = 
                      paste(path,&quot;tabela_7.6_flower_demand_in_detroit.txt&quot;,sep = &quot;&quot;),
                      sep = &quot;;&quot;,
                      header = T, 
                      dec = &quot;.&quot;)

# Dataset sample
tbl_7.6[sample(nrow(tbl_7.6),5), ]</code></pre>
<pre><code>##    year     y   x2   x3     x4 x5
## 11   11  5911 3.77 3.65 181.87 11
## 14   14  5868 2.96 3.12 188.20 14
## 3     3  8429 3.07 4.06 165.26  3
## 4     4 10079 2.91 3.64 172.92  4
## 7     7  6216 3.59 3.76 186.28  7</code></pre>
<p>In this table we have <strong>y = sold roses</strong>, <strong>x2 = average rose price</strong>, <strong>x3 = average clove price</strong>, <strong>x4 = average family income available</strong> and <strong>x5 = trend variable</strong>.</p>
<p>Consider the following demand functions:</p>
<p><span class="math display">\[Y = \alpha_1 + \alpha_2 X_2 + \alpha_3 X_3 + \alpha_4 X_4 + \alpha_5 X_5 + \mu\]</span></p>
<p><span class="math display">\[ln(Y) = \beta_1 + \beta_2 ln(X_2) + \beta_3 ln(X_3) + \beta_4 ln(X_4) + \beta_5 ln(X_5) + \mu\]</span></p>
</div>
<div id="a-estimate-the-parameters-of-the-linear-model-and-interpret-the-results" class="section level3">
<h3><strong>a) Estimate the parameters of the linear model and interpret the results</strong></h3>
<pre class="r"><code>tbl_7.6 %&gt;% 
  lm(formula = &quot;y ~ x2 + x3 + x4 + x5&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;y ~ x2 + x3 + x4 + x5&quot;, data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1095.47  -670.22   -76.67   569.51  1945.14 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 10816.043   5988.348   1.806   0.0983 .
## x2          -2227.704    920.466  -2.420   0.0340 *
## x3           1251.141   1157.021   1.081   0.3027  
## x4              6.283     30.622   0.205   0.8412  
## x5           -197.400    101.561  -1.944   0.0780 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 969.9 on 11 degrees of freedom
## Multiple R-squared:  0.8347, Adjusted R-squared:  0.7746 
## F-statistic: 13.89 on 4 and 11 DF,  p-value: 0.0002805</code></pre>
<p><strong>A:</strong> With R output, we have the following linear regression model:</p>
<p><span class="math display">\[\hat{Y} = 10.816 - 2.227,7 X_2 + 1.251,1 X_3 + 6,3 X_4 - 197,4 X_5\]</span>
With this regression model, <span class="math inline">\(\alpha_2 = -2.227,7\)</span> indicates that rose demand will be reduced in 2.227 if rose price increases, <span class="math inline">\(\alpha_3 = 1.251,1\)</span> indicates that rose demand will increase if clove price increase, in this case they are substitutes, <span class="math inline">\(\alpha_4 = 6,3\)</span> indicates that rose demnad will increase if average income availability increase and <span class="math inline">\(\alpha_5 = -197,4\)</span> indicate that rose demand is in a fall trend.</p>
</div>
<div id="b-estimate-the-parameters-of-the-log-log-model-and-interpret-the-results" class="section level3">
<h3><strong>b) Estimate the parameters of the log-log model and interpret the results</strong></h3>
<pre class="r"><code>tbl_7.6 %&gt;% 
  lm(formula = &quot;log(y) ~ log(x2) + log(x3) + log(x4) + log(x5)&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;log(y) ~ log(x2) + log(x3) + log(x4) + log(x5)&quot;, 
##     data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.28610 -0.08998  0.01395  0.07445  0.30779 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)   0.6268     6.1483   0.102   0.9206  
## log(x2)      -1.2736     0.5266  -2.418   0.0341 *
## log(x3)       0.9373     0.6592   1.422   0.1828  
## log(x4)       1.7130     1.2008   1.426   0.1815  
## log(x5)      -0.1816     0.1279  -1.420   0.1833  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1689 on 11 degrees of freedom
## Multiple R-squared:  0.778,  Adjusted R-squared:  0.6972 
## F-statistic: 9.635 on 4 and 11 DF,  p-value: 0.001343</code></pre>
<p><strong>A:</strong> With R output, we have the following log-log regression model:</p>
<p><span class="math display">\[\hat{Y} = 0.63 - 1.27 X_2 + 0.94 X_3 + 1.71 X4 - 0.18 X_5\]</span>
Given this regression model, <span class="math inline">\(\beta_2 = -1.27\)</span> indicates that rose demand will fall in 1.27% if rose price increase in 1%, <span class="math inline">\(\beta_3 = 0.93\)</span> indicates that rose demand will increase in 0.93% if clove price increase in 1%, <span class="math inline">\(\beta_4 = 1.71\)</span> indicates that rose demand will increase in 1.71% if average family income availability increase in 1% and <span class="math inline">\(\beta_5 = -0.18\)</span> indicates that rose demand is falling in a trend of -0.18% per quarter.</p>
</div>
<div id="c-the-parameters-beta-2-3-and-4-represent-price-elasticity-cross-price-elasticity-and-income-elasticity-for-rose-demand" class="section level3">
<h3><strong>C) The parameters beta 2, 3 and 4 represent price elasticity, cross price elasticity and income elasticity for rose demand?</strong></h3>
<p><strong>A:</strong> Yes, because they’re on transformed by natural log, then small variations can be interpreted as percentual variations. But this model generated parameters with high p-value, then these parameters aren’t strong predictors for the variable being studied.</p>
</div>
</div>
<div id="chapter-8---multple-regression-analysis-the-inference-problem" class="section level2">
<h2><strong>Chapter 8 - Multple Regression Analysis: The Inference Problem</strong></h2>
<p>In this chapter the author record the reader that OLS method to develop regression models doen’t make any assumption about the error distribution <span class="math inline">\(\mu\)</span> and the OLS would be fine if the goal is to make point estimation of the parameters of the model. But when the goal is to make estimations about the populational parameters, the normality of <span class="math inline">\(\mu\)</span> assumption will be required.</p>
<p>After, they present multiple hypothesis that can be tested in a multiple regression model, like:</p>
<ol style="list-style-type: decimal">
<li><p>Test hypothesis about one coeficient of the model (similar to Simple Regression Models).</p></li>
<li><p>Test the general significance of the model. In other words, test all the parameters (<span class="math inline">\(\beta\)</span>) of the model.</p></li>
<li><p>Test if two or more parameters are equal.</p></li>
<li><p>Test if parameters make sense given a particular theory behind the model.</p></li>
<li><p>Test the model stability given different data samples or different periods.</p></li>
</ol>
<p>In this chapter the author gave special attention to the F statistics. That is a metric that indicates the relevance of the paramters choosen for the regression model. Also, they presented the relation between F statistics and R².</p>
<p>Then, to end the chapter, they presented some discussion about when it is necessary to add new variables on a multiple regression model.</p>
</div>
]]></content>
        </item>
        
        <item>
            <title>Global Immigration Outlook using wbstats R package</title>
            <link>/posts/2020/01/global-immigration-outlook-using-wbstats-r-package/</link>
            <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
            
            <guid>/posts/2020/01/global-immigration-outlook-using-wbstats-r-package/</guid>
            <description>Imigration is a central theme on political debates for presidential elections nowadays, creating a lot of controversies and also hope for many people. This social event is increasingly calling more attention, both Syria conflict and the weaknesses of the Venezuelan economy that sent millions to seek other place to live surpreended politicians, that didn’t get ready earlier to obtain the benefits that migration flow could generate.
Because of the theme’s relevance, in November-2019 The Economist made a special report about immigration, aiming to clarify the current situation, answer some questions about the impact of immigrants on host countries and also present some success cases of countries that are obtaining advantages by the immigration flow.</description>
            <content type="html"><![CDATA[


<p>Imigration is a central theme on political debates for presidential elections nowadays, creating a lot of controversies and also hope for many people. This social event is increasingly calling more attention, both Syria conflict and the weaknesses of the Venezuelan economy that sent millions to seek other place to live surpreended politicians, that didn’t get ready earlier to obtain the benefits that migration flow could generate.</p>
<p>Because of the theme’s relevance, in November-2019 <a href="https://www.economist.com/printedition/2019-11-16">The Economist</a> made a special report about immigration, aiming to clarify the current situation, answer some questions about the impact of immigrants on host countries and also present some success cases of countries that are obtaining advantages by the immigration flow.</p>
<p>I’ve really enjoyed the theme and the researches that were writen by The Economist’s correspondents, thus I decided to write a post to present the main conclusions of the studies and also complement them with additional data about immigration around the world. For this task I’ll access World Bank’s database using the R package wbstats, an API that allows you to connect to the institution’s database.</p>
<p>The following image is from one of the magazine’s reports.</p>
<p><img src="/images/2020-01-09_imigration_post.png" /></p>
<div id="main-conclusions-of-the-report" class="section level2">
<h2><strong>Main conclusions of the report</strong></h2>
<ol style="list-style-type: decimal">
<li><p>People that successfully migrate from a poor country to a rich one increase their income between 3 to 6 times, because rich countries have better institutions, capital alocation and modern companies.</p></li>
<li><p>If everone that wanted immigration were allowed to, World’s GDP could increase in 100%.</p></li>
<li><p>Immigrants are more likely to open their own business because they percept unatended demands easier than local residents and also they find alternative solutions to existent problems on the country that they enter.</p></li>
<li><p>The main “problem” with immigration is the cultural change, that occurs speedly in specific places because immigrants tend to cluster in certain locals. In addition of being the main problem, it’s a tough condition to measure, then it’s difficult to be contested.</p></li>
<li><p>People are more tolerant to immigration flow in their countries when they feel that their government are in charge of the frontier, allowing people qualified that will add to local society. In other words, people are more receptive if they feel that the process is under control.</p></li>
<li><p>Imigratns often share their income with their families that have stayed on the country and this shared income is called <a href="https://www.ted.com/talks/dilip_ratha_the_hidden_force_in_global_economics_sending_money_home">remitances</a>. Remitances already is the main source of external investment in certain poor countries. This flow of money has high potential of impacts, because it goes straight to the pocket of people that are in need and also this investment doesn’t carry the risk of being misplaced by corruption. Remitances represent more than 10% of GDP for 28 countries.</p></li>
</ol>
</div>
<div id="immigration-outlook" class="section level2">
<h2><strong>Immigration Outlook</strong></h2>
<p>After understanding some aspects of immigration, we can analyze some data from World Bank in order to dive a little deep on this theme. For this task I’ll use the following packages:</p>
<pre class="r"><code>library(tidyverse)      # data science and data manipulation packages
library(wbstats)        # World Bank API

theme_graph &lt;- function(){
  theme(
    plot.title = element_text(size = 16),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(face = &quot;italic&quot;, size = 9),
    axis.text = element_text(size = 9),
    axis.title = element_text(face = &quot;italic&quot;, size = 9),
    strip.background = element_rect(fill = &quot;grey&quot;),
    strip.text = element_text(face = &quot;bold&quot;),
    legend.title = element_blank(),
    legend.position = &quot;bottom&quot;
  )
}</code></pre>
<p>The first thing the I would like to know about this theme is the population considered immigrant by the institution, as well as the evolution of this metric during the years. In order to access this data, we just need to call the wbstats package with the code that represent the desired metric.</p>
<pre class="r"><code># Creating a variable with the data.frame of the requested data
migrants_pop &lt;- wbstats::wb(indicator = &quot;SM.POP.TOTL&quot;,
                              country = &quot;countries_only&quot;,
                              startdate = 1980,
                              enddate = 2019)</code></pre>
<p>After requesting the data, it’s possible to analyze the immigration population during the years.</p>
<pre class="r"><code>migrants_pop %&gt;% 
  group_by(date) %&gt;% 
  summarise(immigrants = sum(value)) %&gt;% 
  ggplot()+
  geom_line(mapping = aes(x = date, y = immigrants, group = 1), 
            size = .5, 
            color = &quot;dark orange&quot;,
            alpha = .6)+
  geom_point(mapping = aes(x = date, y = immigrants),
             color = &quot;dark orange&quot;)+
  scale_y_continuous(labels = scales::comma)+
  theme_graph()+
  labs(title = &quot;Immigrant&#39;s population during the years&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/posts/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The dataset goes untill 2015 with a population of 250 millions. That’s more than Brazil’s population. Moving forward, let’s see which countries have the highest share of immigrants living on their territory. The next visualization will present the 20 countries with the highest rate of immigrants on their population. Probably Canada and Australia will be well positioned.</p>
<p>For this visualization, we’ll need bring data about country population in order to join with the immigrant population. This metric will consider the share in 2015 only.</p>
<pre class="r"><code># Bringing population data from world bank
population &lt;- wbstats::wb(indicator = &quot;SP.POP.TOTL&quot;,
                          country = &quot;countries_only&quot;)</code></pre>
<pre class="r"><code># Joining both datasets
migrants_pop %&gt;% 
  filter(date == &quot;2015&quot;) %&gt;% 
  select(country, date, value) %&gt;% 
  inner_join(population, by = c(&quot;country&quot; = &quot;country&quot;, &quot;date&quot; = &quot;date&quot;)) %&gt;%
  mutate(share_immigrants = round(value.x/value.y, digits = 4)) %&gt;% 
  filter(value.y &gt;= 10000000) %&gt;% 
  arrange(desc(share_immigrants)) %&gt;% 
  head(20) %&gt;% 
  mutate(country = fct_reorder(country, share_immigrants)) %&gt;% 
  ggplot()+
  geom_col(mapping = aes(x = country, y = share_immigrants),
           fill = &quot;dark orange&quot;,
           alpha = .6)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;County rank by imigration share over population&quot;,
       x = &quot;&quot;,
       y = &quot;% of imigrants over the population&quot;)+
  coord_flip()</code></pre>
<p><img src="/posts/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>I didn’t know Saudi Arabia would be the first. This <a href="https://www.thestreet.com/personal-finance/countries-with-most-immigrants">article</a> enforce this number. As I removed countries with less than 10 million hbitants, I’ve probably removed UAE that was the first positioned in the mentioned article.</p>
<p>Now that we know the countries that have been receiving many immigrants, let’s see some countries that have been loosing residents during the past years. The World Bank has an indicator that shows the net output of people for a given country. The math is straightforward, it’s immigrants minus migrants. Thus, if I classify countries from the lowest value to the highest and select the first 20, I’ll end up with the 20 countries that have lost more local residents.</p>
<p>For this analysis I’ll take the mentioned metric from 2010 to 2017, in order to consider the outflow of residents throughout some years. First, let’s take the Net Immigration Flow metric.</p>
<pre class="r"><code>net_immigration &lt;- wbstats::wb(indicator = &quot;SM.POP.NETM&quot;,
                         country = &quot;countries_only&quot;,
                         startdate = 2010,
                         enddate = 2019) %&gt;% 
  select(country, value, date)

# Dataset sample
net_immigration[sample(nrow(net_immigration),5), ]</code></pre>
<pre><code>##                       country   value date
## 1113                St. Lucia       0 2017
## 293                    Bhutan    1600 2017
## 908                   Ireland -111819 2012
## 2113                    Samoa  -14013 2017
## 313  Central African Republic -200000 2017</code></pre>
<p>Now we can adjust the dataset to sum the years gathered and classify the countries in order to take the 20 that have lost more residents during the period and plot the graph.</p>
<pre class="r"><code>net_immigration %&gt;% 
  group_by(country) %&gt;% 
  summarise(net_immigration = sum(value)) %&gt;% 
  arrange(net_immigration) %&gt;% 
  head(20) %&gt;% 
  mutate(net_immigration = net_immigration * -1,
         country = fct_reorder(country, net_immigration)) %&gt;% 
  ggplot()+
  geom_col(mapping = aes(x = country, y = net_immigration),
           fill = &quot;dark orange&quot;,
           alpha = .6)+
  scale_y_continuous(labels = scales::comma)+
  theme_graph()+
  coord_flip()+
  labs(title = &quot;Countries that had lost more residents since 2010&quot;,
       x = &quot;&quot;,
       y = &quot;Net immigration&quot;)</code></pre>
<p><img src="/posts/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>I was expecting to see Syria and Venezuela in this plot. India and China isn’t in the same scenario of those two countries, but with their large population, any movement in theses countries are a big thing when we don’t normalize the data. In order to remove these big countries, let’s remake this analysis but now classifing countries considering the share of the population that migrated from it. For this, we’ll need to bring the same Net Immigration Flow metric and join with poplational data in the last year of analysis.</p>
<pre class="r"><code># Net immigration summed during the period
net_immigration %&gt;% 
  group_by(country) %&gt;% 
  summarise(net_immigration = sum(value)) -&gt; net_immigration

# Joinning the population and net_immigration data
population %&gt;% 
  filter(date == 2015) %&gt;%
  mutate(population = value) %&gt;% 
  select(country, population) %&gt;% 
  inner_join(net_immigration, by = c(&quot;country&quot; = &quot;country&quot;)) %&gt;% 
  mutate(net_immigration_share = round(net_immigration/population, digits = 4)) %&gt;% 
  arrange(net_immigration_share) %&gt;% 
  head(20) %&gt;% 
  mutate(net_immigration_share = net_immigration_share * -1,
         country = fct_reorder(country, net_immigration_share)) %&gt;% 
  ggplot()+
  geom_col(mapping = aes(x = country, y = net_immigration_share),
           fill = &quot;dark orange&quot;,
           alpha = .6)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  coord_flip()+
  labs(title = &quot;Share of population that left the country in the period&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;,
       caption = &quot;From 2010 to 2017&quot;)</code></pre>
<p><img src="/posts/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>This analysis seems pretty realistic, having Syria in the first place and Puerto Rico with its social problems with gangs and organized crime.</p>
<p>Moving forward, know that we know what countries are loosing their residents, let’s analyze how remitances are impacting these countries. As noted at The Economist’s reports, remitance is a great source of investment for poor countries, because it goes straight to the hands of the family that are in need of it and is less likely to be drained by corruption.</p>
<p>Two findings in the research made by the magazine called my attention. The first is that remitance represents more thant 10% of GDP in 29 countries (that’s a huge contribution). The second discovery is that remitance is acyclic with Per Capita GDP. Thus it moves in opposite direction of this indicator and this behavior has a theory to justify it.</p>
<p>When immigrants realize that their home country are in trouble, like an economic crises or a war, they sent more money to their families in order to support them in a new tough situation. This seems well justified, but we can verify this hypothesis (i.e. if remitance is acyclic when compared with Per Capita GDP). Fortunately, Word Bank has the necessary data to analyze this hypothesis.</p>
<p>Let’s first visualize the countries that have more than 10% of their GDP comming from remitances. Luckly, World Bank has this specific metric ready to be requested by the API. The following list represent the countries and their share of GDP comming from remitances.</p>
<pre class="r"><code>remitances_over_gdp &lt;- wbstats::wb(indicator = &quot;BX.TRF.PWKR.DT.GD.ZS&quot;,
                         country = &quot;countries_only&quot;)

remitances_over_gdp %&gt;% 
  filter(date == &quot;2018&quot;,
         country != &quot;Lesotho&quot;,
         value &gt; 10) %&gt;%
  mutate(remitance_over_gdp = paste(round(value,digits = 0),&quot;%&quot;,sep = &quot;&quot;)) %&gt;% 
  arrange(desc(value)) %&gt;% 
  select(date, country, remitance_over_gdp)</code></pre>
<pre><code>##    date                country remitance_over_gdp
## 1  2018                  Tonga                41%
## 2  2018        Kyrgyz Republic                33%
## 3  2018                  Haiti                33%
## 4  2018             Tajikistan                29%
## 5  2018                  Nepal                29%
## 6  2018            El Salvador                21%
## 7  2018               Honduras                20%
## 8  2018                  Samoa                18%
## 9  2018     West Bank and Gaza                17%
## 10 2018                Moldova                16%
## 11 2018                Jamaica                16%
## 12 2018                 Kosovo                16%
## 13 2018             Uzbekistan                15%
## 14 2018       Marshall Islands                14%
## 15 2018                Liberia                14%
## 16 2018                Comoros                14%
## 17 2018            Gambia, The                12%
## 18 2018            Yemen, Rep.                12%
## 19 2018                Lebanon                12%
## 20 2018              Guatemala                12%
## 21 2018                Armenia                12%
## 22 2018             Cabo Verde                12%
## 23 2018                Georgia                12%
## 24 2018              Nicaragua                11%
## 25 2018                Ukraine                11%
## 26 2018             Montenegro                11%
## 27 2018                 Jordan                11%
## 28 2018 Bosnia and Herzegovina                11%
## 29 2018            Philippines                10%
## 30 2018       Egypt, Arab Rep.                10%
## 31 2018                Senegal                10%</code></pre>
<p>Now, let’s analyze the hypothesis that the remitance is acyclic when compared with Per Capita GDP. For this test, I’ll select the following countries: Suriname, Somalia, Sierra Leone, Senegal, Sudan, Rwanda, Nepal, Niger, Malawy, Mali, Lybia, Kenya, Ethiopia, Eritrea and Congo.</p>
<p>Now I’ll request the metric “Remitances received” and “Per Capita GDP”.</p>
<p>Know I have remitances received and Per Capita GDP. In order to compare, it’ll be necessary to normalize remitances by country population. The following code chunk will do this task.</p>
<pre class="r"><code>remitances_received %&gt;% 
  mutate(remitances = value) %&gt;% 
  select(date, country, remitances) %&gt;% 
  filter(country %in% country_view,
         date &gt; 2010) %&gt;% 
  inner_join(population, by = c(&quot;date&quot; = &quot;date&quot;, &quot;country&quot; = &quot;country&quot;)) %&gt;% 
  select(date, country, remitances, value) %&gt;% 
  mutate(remitances_adj = round(remitances/value, digits = 4)) %&gt;% 
  select(date, country, remitances_adj) -&gt; remitances

# Sample from dataset
remitances[sample(nrow(remitances),5), ]</code></pre>
<pre><code>##     date              country remitances_adj
## 46  2011              Nigeria       126.6354
## 58  2015               Rwanda        14.0029
## 1   2016          Congo, Rep.         1.5468
## 103 2018             Tanzania         7.3317
## 93  2012 Syrian Arab Republic        79.3708</code></pre>
<p>Know, I can join the two datasets and generate the scaterplot. The remitances dataset created before is already filtered with the selected countries.</p>
<pre class="r"><code>gdp_pc %&gt;% 
  mutate(gdp_pc = value) %&gt;% 
  select(date, country, gdp_pc) %&gt;% 
  inner_join(remitances, by = c(&quot;date&quot; = &quot;date&quot;, &quot;country&quot; = &quot;country&quot;)) %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = remitances_adj, y = gdp_pc))+
  geom_smooth(mapping = aes(x = remitances_adj, y = gdp_pc),
              method = &quot;lm&quot;,
              formula = y ~ x,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Per Capita GDP and Remitances correlation&quot;,
       y = &quot;Per Capita GDP&quot;,
       x = &quot;Per Capita Remitances&quot;)</code></pre>
<p><img src="/posts/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>I was expecting to see a negative correlation between the variables, because the lower Per Capita GDP, highet should be Per Capita Remitances. Let’s see the regression model for this two variables. The model that I’ll use will be:</p>
<p><span class="math display">\[R = \beta_0 + \beta_1 pc.gdp + \mu\]</span>
In other words, I’ll analyze the impact that per capita GDP causes in the flow of per capita remitances sent by immigrants to their families or friends. Unfortunately <span class="math inline">\(\beta_2\)</span> will be positive (contraty to what I was expecting), but eventhouth I’ll keep the model development.</p>
<pre class="r"><code>gdp_pc %&gt;% 
  mutate(gdp_pc = value) %&gt;% 
  select(date, country, gdp_pc) %&gt;% 
  inner_join(remitances, by = c(&quot;date&quot; = &quot;date&quot;, &quot;country&quot; = &quot;country&quot;)) %&gt;%
  lm(formula = remitances_adj ~ gdp_pc) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = remitances_adj ~ gdp_pc, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -109.41  -33.61  -25.80   35.76  240.26 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 23.911203  10.341743   2.312   0.0225 *  
## gdp_pc       0.030089   0.006127   4.911    3e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 66.44 on 116 degrees of freedom
## Multiple R-squared:  0.1721, Adjusted R-squared:  0.165 
## F-statistic: 24.11 on 1 and 116 DF,  p-value: 3.002e-06</code></pre>
<p>With the output of the model we can reformulate the regression equation:</p>
<p><span class="math display">\[R = 23.91 + 0.03 pc.gdp + \mu\]</span></p>
<p>Both intercept and slope are statistically significant with a p-value lower than 5%. Let’s summarize the both metric and see a time series of them in order to identify the mentioned pattern.</p>
<pre class="r"><code>gdp_pc %&gt;% 
  mutate(gdp_pc = value) %&gt;% 
  select(date, country, gdp_pc) %&gt;% 
  inner_join(remitances, by = c(&quot;date&quot; = &quot;date&quot;, &quot;country&quot; = &quot;country&quot;)) %&gt;%
  group_by(date) %&gt;% 
  summarise(gdp = sum(gdp_pc),
            remitance = sum(remitances_adj)) %&gt;% 
  ggplot()+
  geom_line(mapping = aes(x = date, y = gdp, group = 1),
            color = &quot;dark orange&quot;)+
  scale_y_continuous(labels = scales::comma)+
  theme_graph()+
  labs(title = &quot;Per Capita GDP time series&quot;)</code></pre>
<p><img src="/posts/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>gdp_pc %&gt;% 
  mutate(gdp_pc = value) %&gt;% 
  select(date, country, gdp_pc) %&gt;% 
  inner_join(remitances, by = c(&quot;date&quot; = &quot;date&quot;, &quot;country&quot; = &quot;country&quot;)) %&gt;%
  group_by(date) %&gt;% 
  summarise(gdp = sum(gdp_pc),
            remitance = sum(remitances_adj)) %&gt;% 
  ggplot()+
  geom_line(mapping = aes(x = date, y = remitance, group = 1),
            color = &quot;dark orange&quot;)+
  scale_y_continuous(labels = scales::comma)+
  theme_graph()+
  labs(title = &quot;Per Capita Remitances time series&quot;)</code></pre>
<p><img src="/posts/2020-01-20-imigration-data-analysis_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
<p>When the data is agregated, the behavior is perceived. I won’t go deep in this topic because it would miss the point.</p>
</div>
<div id="conclusions" class="section level2">
<h2><strong>Conclusions</strong></h2>
<p>In this post I wanted analyze some data about immigration, share the main findings of the research made by the magazine and show how to request some data from World Bank using the <strong>wbstats</strong> package. I hope it was interesting for you.</p>
</div>
]]></content>
        </item>
        
        <item>
            <title>Connecting R on Amazon Redshift Database</title>
            <link>/posts/2019/10/connecting-r-on-amazon-redshift-database/</link>
            <pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate>
            
            <guid>/posts/2019/10/connecting-r-on-amazon-redshift-database/</guid>
            <description>IntroductionIn this tutorial I’ll show a step by step on how to access Amazon Redshift Database throughout R. This task will be very useful if you prefer to use R intead of the traditional Excel to execute exploratory data analysis on your company. The goal of this tutorial is to access the database from R, bring a data.frame as a result set and then create your analysis.
Step 1Dowload the JDBC driver from Amazon.</description>
            <content type="html"><![CDATA[


<div id="introduction" class="section level2">
<h2><strong>Introduction</strong></h2>
<p>In this tutorial I’ll show a step by step on how to access Amazon Redshift Database throughout R. This task will be very useful if you prefer to use R intead of the traditional Excel to execute exploratory data analysis on your company. The goal of this tutorial is to access the database from R, bring a data.frame as a result set and then create your analysis.</p>
</div>
<div id="step-1" class="section level2">
<h2><strong>Step 1</strong></h2>
<p>Dowload the JDBC driver from Amazon. This file will be needed to allow the connection of one tool (i.e. R Studio) to the Amazon cluster that you have access. In this <a href="https://docs.aws.amazon.com/pt_br/redshift/latest/mgmt/configure-jdbc-connection.html#obtain-jdbc-url">link</a> there’s the files indicated for each version of the JDBC.</p>
<p>Ps: If at the end of the role process the database access fail, take a look on the above file that you dowloaded to see if it came corrupted. This can be caused (not likely but it can) when the dowload is made directly from R.</p>
</div>
<div id="step-2" class="section level2">
<h2><strong>Step 2</strong></h2>
<p>Inside R Studio, install and call the following packages: “rJava” and “RJDBC”.</p>
<pre class="r"><code>install.packages(&quot;rJava&quot;)
install.packages(&quot;RJDBC&quot;)

library(rJava)
library(RJDBC)</code></pre>
</div>
<div id="step-3" class="section level2">
<h2><strong>Step 3</strong></h2>
<p>Create a variable (i.e. “driver”) that will contain the JDBC file that was dowloaded on the Step 1. Use the following code for this step:</p>
<pre class="r"><code>driver &lt;- JDBC(driverClass = &quot;com.amazon.redshift.jdbc41.Driver&quot;, 
               classPath = Sys.glob(&quot;caminho do arquivo/RedshiftJDBC41-1.1.9.1009.jar&quot;), 
               identifier.quote=&quot;`&quot;)</code></pre>
</div>
<div id="step-4" class="section level2">
<h2><strong>Step 4</strong></h2>
<p>Create a variable (i.e. “db_connection”) that will indicate the information of the database to be accessed by R Studio. Use the following code for this step:</p>
<pre class="r"><code>db_connection &lt;- sprintf(&quot;jdbc:redshift://%s:%s/%s?tcpKeepAlive=true&amp;ssl=true&amp;sslfactory=com.amazon.redshift.ssl.NonValidatingFactory&quot;, 
                         &quot;host of the database&quot;, 
                         &quot;port&quot;, 
                         &quot;database name&quot;)</code></pre>
</div>
<div id="step-5" class="section level2">
<h2><strong>Step 5</strong></h2>
<p>Create a variable (i.e. “access”) that will contain the information of the database and also the user that will access the database. Use the following code for this step:</p>
<pre class="r"><code>access &lt;- dbConnect(driver, db_end, &quot;user&quot;, &quot;password&quot;)</code></pre>
<p>Ps: Note that the funcion “dbConnect” used the database access file driver, the information of the database that will be accessed and the information of the user that will access the database.</p>
</div>
<div id="step-6" class="section level2">
<h2><strong>Step 6</strong></h2>
<p>After the creation of the necessary variables, to access the database it’ll be necessary the last one created (at step 5). To access the database, the package RJDBC will be used. Some examples can be seeing on the code chunck that follows:</p>
<p>Ps: Queries run on R Studio console must by with quotes.</p>
<pre class="r"><code>RJDBC::dbGetQuery(jconn,
                  &quot;select 
                     order
                   , client
                   , sale_date 
                  from orders_tb 
                  limit 10&quot;)</code></pre>
</div>
<div id="step-7" class="section level2">
<h2><strong>Step 7</strong></h2>
<p>To explore the feature in depth, it’s interesting that, intead of writing the query on the R Studio console, you save your query in a file (.sql) and make R access this file and save it as a variable (i.e. “sql_file”). To do this, it’s necessary to install and call the “readr” package.</p>
<pre class="r"><code>install.packages(&quot;readr&quot;)

library(readr)

-- Saving the .sql file as a variable
sql_file &lt;- readr::read_file(&quot;path_of_the_file/script.sql&quot;)

-- Using the .sql file to access Redshift database
query_redshift &lt;- RJDBC::dbGetQuery(jconn, sql_file)</code></pre>
</div>
<div id="final-step-plus" class="section level2">
<h2><strong>Final step (plus)</strong></h2>
<p>Sometimes you can have projects that demand more than one query, thus you will need many .sql files in order to run each query. But there’s a solution to make things simpler. You can save many queries on the same .sql file and separate each query by a string (i.e. –query_1_being, –query_1_end, –query_2_begin, –query_2_end, and so on). Then, you can use regexp to split your file and create variables for each query. The regexp will access the defined query and execute the split to create the variable with just one query code. For this step, you will need to install and call the “stringr” package.</p>
<pre class="r"><code>install.packages(&quot;stringr&quot;)

library(stringr)

-- saving the .sql file as a variable
sql_syntax &lt;- readr::read_file(&quot;.sql_file_path.all_queries.sql&quot;)

-- creating one variable with the first query of the .sql file
query_1 &lt;- 
  RJDBC::dbGetQuery(jconn, 
                    substr(sql_syntax, 
                           min(stringr::str_locate(string = sql_syntax, &#39;-- query.1.begin&#39;)), 
                           max(stringr::str_locate(string = sql_syntax, &#39;-- query.1.end&#39;))))

query_2 &lt;- 
  RJDBC::dbGetQuery(jconn, 
                    substr(sql_syntax, 
                           min(stringr::str_locate(string = sql_syntax, &#39;-- query.2.begin&#39;)), 
                           max(stringr::str_locate(string = sql_syntax, &#39;-- query.2.end&#39;))))</code></pre>
</div>
<div id="conclusion" class="section level2">
<h2><strong>Conclusion</strong></h2>
<p>This tutorial was aimed to show how simple it is to access Amazon Redshift database throughout R Studio. It’s an excellent alternative when you want to perform robust exploratory data analysis (where Excel will let you down).</p>
</div>
<div id="more-tutorials-on-the-subject" class="section level2">
<h2><strong>More tutorials on the subject</strong></h2>
<p><a href="https://www.progress.com/tutorials/jdbc/connecting-to-amazon-redshift-from-r-via-jdbc-driver">Connecting to Amazon Redshift from R</a></p>
<p><a href="https://www.r-bloggers.com/a-comprehensive-guide-to-connect-r-to-amazon-redshift/">A comprehensive guide to connect R to Amazon Redshift</a></p>
</div>
]]></content>
        </item>
        
        <item>
            <title>Residual analysis in econometric models</title>
            <link>/posts/2019/09/residual-analysis-in-econometric-models/</link>
            <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
            
            <guid>/posts/2019/09/residual-analysis-in-econometric-models/</guid>
            <description>The linear regression model is highly used on the prediction of continuous variables, where is one or more independent variables and one dependent one (the one that it seeks to test an hypothesis about its behavior). This is a pretty simple model to by executed and one of the firsts to be taught in econometric classes. Although its simplicity, in order to reach reliable results, some assumptions are needed, like the following:</description>
            <content type="html"><![CDATA[


<p>The linear regression model is highly used on the prediction of continuous variables, where is one or more independent variables and one dependent one (the one that it seeks to test an hypothesis about its behavior). This is a pretty simple model to by executed and one of the firsts to be taught in econometric classes. Although its simplicity, in order to reach reliable results, some assumptions are needed, like the following:</p>
<ol style="list-style-type: lower-roman">
<li><p>Absence of multicolineatiry between independent variables.</p></li>
<li><p>Absence of autocorrelation on the dependent variable.</p></li>
<li><p>Absence of pattern on the behavior of the model residuals, in other words, absence of heteroscedasticity.</p></li>
<li><p>Normal distribution of residuals.</p></li>
</ol>
<p>Having this assumptions satisfied, the model can be executed and the generated conclusions will be reliable to allow decision making. In this post, I’ll analyse the behavior of the residuals for a linear regression model in order to validate the assumptions (iii) and (iv). For this, I’ll use a data set that presents the price of suggarcane as independent variable and the planted area of this product, that will be the dependent variable.</p>
<p>The goal of this example is to analyse, through a simple linear regression model, the suggarcane supply elasticity as a function of the suggarcane price. The hipothesis is that there’s elasticity on the supply, thus the planted area increases in responde to a price increase. But, in order to validade this existence, it’s necessary to check, using a regression model, that these two variables have correlation. Then, the assumptions (iii) and (iv) will support this conclusion.</p>
<p>For this example, I’ll utilize the following R packages:</p>
<pre class="r"><code>library(tidyverse)
library(lmtest)
library(corrplot)
library(readxl)

theme_graph &lt;- function(){
  theme(
    plot.title = element_text(size = 16),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(face = &quot;italic&quot;, size = 9),
    axis.text = element_text(size = 9),
    axis.title = element_text(face = &quot;italic&quot;, size = 9),
    strip.background = element_rect(fill = &quot;grey&quot;),
    strip.text = element_text(face = &quot;bold&quot;),
    legend.title = element_blank(),
    legend.position = &quot;bottom&quot;
  )
}
  
graph_color &lt;- &quot;#006666&quot;</code></pre>
<p>The data set for this example has 34 rows, with planted area and suggarcane price fields.</p>
<pre class="r"><code># Getting the dataset
dados &lt;- readxl::read_excel(&quot;C:/Users/francisco.piccolo/Desktop/R/franciscopiccolo.github.io/datasets/Econometria_exercicios/autocorrelacao_cana_de_acucar.xlsx&quot;)

# Chaging the name of the fields to remove blank spaces
dados %&gt;% 
  transmute(period = Período,
            area = Área,
            price = `Preço da Cana de Açúcar`*100) -&gt; df</code></pre>
<p>Lets see a sample of this dataset.</p>
<pre class="r"><code>df[sample(nrow(df),5), ] %&gt;%
    as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 3
##   period  area price
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1      1    29  7.53
## 2     16    99 22.1 
## 3     25    87 30.1 
## 4      2    71 11.5 
## 5      9    60 18.8</code></pre>
<p>The linear regression model proposed for this problem is defined by the following equation:</p>
<p><span class="math display">\[ lnY_t = \beta_0+\beta_1 (lnX_t) + \mu_t \]</span></p>
<p>Where:</p>
<p><span class="math inline">\(Y_t\)</span> = planted area, natural log transformed</p>
<p><span class="math inline">\(X_t\)</span> = suggarcane price, natural log transformed</p>
<p><span class="math inline">\(\beta_0\)</span> = intercept</p>
<p><span class="math inline">\(\beta_1\)</span> = slope</p>
<p><span class="math inline">\(\mu_t\)</span> = residuals</p>
<p>The natural log is used to in order to have variations between periods interpreted as percentual variation. This assumption is made possible only for natural log transformations and this adjustment is needed in problems invonving elasticity, because elasticity is interpreted as a percentual variation in one variable given a percentual variation in another variable.</p>
<p>The following graph shows the behavior of the studied variables and the regression line.</p>
<pre class="r"><code>df %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = price, y = area), shape = 20)+
  geom_smooth(mapping = aes(x = price, y = area), 
              method = &quot;lm&quot;, 
              formula = y ~ x, 
              se = F, 
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Dispersion graph for the model&quot;)</code></pre>
<p><img src="/posts/2019-09-05-residual-analysis-in-econometric-models_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>In order to develop the model using the log transformation, there’re two options:</p>
<ol style="list-style-type: lower-roman">
<li><p>Adjust the variables on the dataset and build the normal using the new variables generated by the log scale calculation.</p></li>
<li><p>Use the original dataset and inform adjust the model to apply the transformation on the variables before calculating the parameters.</p></li>
</ol>
<p>Lets see each one of theses methods to confirm they’re equal:</p>
<p><strong>(i) </strong></p>
<pre class="r"><code>df %&gt;% 
  mutate(area_log = log(area),
         price_log = log(price)) %&gt;% 
  lm(formula = area_log ~ price_log) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = area_log ~ price_log, data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.65076 -0.18823 -0.03096  0.24914  0.60492 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.6416     0.3534   4.645 5.56e-05 ***
## price_log     0.9706     0.1106   8.773 5.03e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3088 on 32 degrees of freedom
## Multiple R-squared:  0.7063, Adjusted R-squared:  0.6972 
## F-statistic: 76.97 on 1 and 32 DF,  p-value: 5.031e-10</code></pre>
<p><strong>(ii)</strong></p>
<pre class="r"><code>df %&gt;% 
  lm(formula = log(area) ~ log(price)) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(area) ~ log(price), data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.65076 -0.18823 -0.03096  0.24914  0.60492 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.6416     0.3534   4.645 5.56e-05 ***
## log(price)    0.9706     0.1106   8.773 5.03e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3088 on 32 degrees of freedom
## Multiple R-squared:  0.7063, Adjusted R-squared:  0.6972 
## F-statistic: 76.97 on 1 and 32 DF,  p-value: 5.031e-10</code></pre>
<p>As indicated, the two alternatives generate the same result. It’s up to the researcher to choose the method. The regression model generated is:</p>
<p><span class="math display">\[\hat{Y} = 1.6416 + 0.9706X_1 + \mu\]</span>
With both intercept and slope statistically significant given their low p-value.</p>
<p>The r² reach 70.6%, but this isn’t enough to make conclusions about the model. Its necessary to analyse the model hypothesis about the residuals to ensure that the model is reliable. The following code return the residuals from a ‘lm’ model.</p>
<pre class="r"><code>model_residuals &lt;- 
  data.frame(values = df %&gt;% 
                        lm(formula = log(area) ~ log(price)) %&gt;% 
                        residuals())</code></pre>
<p>The next graph shows the residuals distribution in order to validate if they’re normally distributed. Both histogram and quantil-quantil plot are good choices to validate this assumption.</p>
<pre class="r"><code># Histogram
model_residuals %&gt;% 
  ggplot()+
  geom_histogram(mapping = aes(x = values), fill = &quot;dark orange&quot;, alpha = .5)+
  theme_graph()+
  labs(title = &quot;Residuals Distribution&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/posts/2019-09-05-residual-analysis-in-econometric-models_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># q-q plot
model_residuals %&gt;% 
  ggplot()+
  geom_qq(mapping = aes(sample = values))+
  theme_graph()+
  labs(title = &quot;Residuals&#39; Q-Q plot&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/posts/2019-09-05-residual-analysis-in-econometric-models_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Both graphs enforce the ideia that the model’s residuals are normally distributed. But in order to make a conclusion, sometimes a formal test will be necessary. For this problem, we’ll use the Durbin Watson test. This test can be performed using the following code.</p>
<pre class="r"><code>lmtest::dwtest(df %&gt;% 
                 lm(formula = log(area) ~ log(price)))</code></pre>
<pre><code>## 
##  Durbin-Watson test
## 
## data:  df %&gt;% lm(formula = log(area) ~ log(price))
## DW = 1.2912, p-value = 0.009801
## alternative hypothesis: true autocorrelation is greater than 0</code></pre>
<p>The test generate the value of 1.2912, but only this value isn’t enough to make the conclusion. As well as the DW value, DL and DU intervals are necessary. These values can be found at this <a href="http://www.portalaction.com.br/analise-de-regressao/33-diagnostico-de-independencia">table</a>. In order to find them, take the number of observations of the dataset (n = 33), the significance level of your test (i.e. 0.05) and the degress of freedom (i.e. 1). Thus we have:</p>
<p><strong>DL</strong> = 1.35</p>
<p><strong>DU</strong> = 1.49</p>
<p>With a DW of 1.2912, above 0 and under the DL value, we can conclude that the residuals are independent.</p>
<p>To conclude, both graphs and formal tests confirmed that the residuals are independent and then we have the hypothesis of the model satisfied, allowing inferences about the problem being studies, in this case, the elasticity of the suggarcane supply given it’s price variability.</p>
]]></content>
        </item>
        
    </channel>
</rss>
