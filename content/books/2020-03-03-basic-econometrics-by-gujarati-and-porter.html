---
title: "Basic Econometrics by Gujarati and Porter"
date: 2020-03-03
output:
  html_document:
    fig_width: 8.5
    fig_height: 4
---



<div id="overview" class="section level2">
<h2><strong>Overview</strong></h2>
<p>This book is the tipical book reference for a given subject, and when the theme is Econometrics, this material is highly recommended for economy grad students. For its importance, in this post I’ll present a brief summary of each chapter and solve some exercises (because the book doesn’t present the answer for the problems). Also, I’ll release the datasets used on the problems on my <a href="https://github.com/FranciscoPiccolo/franciscopiccolo.github.io/tree/master/datasets/livro-econometria-basica-gujarati">github</a> account. To beggin this summary, lets see what the authors say about the subject:</p>
<blockquote>
<p>“Econometry is the application of statistics in economic data”.</p>
</blockquote>
<blockquote>
<p>“Econometry is the quantiative analysis of economic phenomena”.</p>
</blockquote>
<div class="figure">
<img src="/images/2020-03-03_econometria_basica.png" alt="Book cover" />
<p class="caption">Book cover</p>
</div>
</div>
<div id="cap.1-the-nature-of-regression-analysis" class="section level2">
<h2><strong>Cap.1 The nature of regression analysis</strong></h2>
<p>This chapter presents some foundations of regression analysis. First it’s defined the concept and then its goals. Accordingly with the authors, regression is the study of the dependence of one variable in relation to one or more independent variables.</p>
<p>Following, the authors indicate the difference between statistical relations and deterministic relations, presenting some scenarios for each one. Also, it’s explained the difference between regression and causality and between regression and correlation.</p>
<p>Now let’s see some problems presented in this chapter.</p>
<div id="exercises" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="bring-the-dataset-from-table-1.3-related-to-consumer-price-index-to-answer-the-following-questions." class="section level3">
<h3><strong>1.1) Bring the dataset from table 1.3 related to consumer price index to answer the following questions.</strong></h3>
<pre class="r"><code>tbl_1.3 &lt;- read.table(file = paste(path,&quot;tabela_1.3_IPC.txt&quot;,
                                   sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;,&quot;)

# Random sample of the dataset
tbl_1.3[sample(nrow(tbl_1.3),5), ] %&gt;% as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 8
##    year   usa canada japan france germany italy    uk
##   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  1993 144.   148.  118.   143.     128. 188.  165. 
## 2  1982  96.5   94.9  98.1   91.7     97   87.8  95.4
## 3  1998 163    158.  122.   154.     139. 222.  191. 
## 4  1999 167.   160.  122.   155      140  226.  194. 
## 5  1991 136.   143.  115    137.     116. 170.  157.</code></pre>
</div>
<div id="b-graphically-represent-the-inflation-rate-of-each-country." class="section level3">
<h3><strong>b) Graphically represent the inflation rate of each country.</strong></h3>
<pre class="r"><code>tbl_1.3 %&gt;%
  tidyr::gather(&quot;campo&quot;, &quot;valor&quot;, 2:8) %&gt;% 
  ggplot()+
  geom_line(mapping = aes(x = year, y = valor, group = campo, color = campo),
            size = 1)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  labs(title = &quot;Anual inflation for selected countries&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="the-table-1.6-represents-the-budget-used-with-advertising-and-the-retention-of-impressions-from-customers-based-in-a-survey-with-4.000-people." class="section level3">
<h3><strong>1.7) The table 1.6 represents the budget used with advertising and the retention of impressions from customers, based in a survey with 4.000 people.</strong></h3>
<pre class="r"><code>tbl_1.6 &lt;- read.table(file = paste(path,
                                   &quot;tabela_1.6_advertising.txt&quot;,
                                   sep = &quot;&quot;),
                      sep = &quot;;&quot;,
                      header = T, 
                      dec = &quot;,&quot;
                      )

# Sample from the dataset
tbl_1.6[sample(nrow(tbl_1.6),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 3
##   company      impressions investment
##   &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt;
## 1 Calvin_Klein        12          5  
## 2 McDonald            92.4      186. 
## 3 Ford                40.1      166. 
## 4 Meow_Mix            12.3        7.6
## 5 Oscar_Meyer         23.4        9.2</code></pre>
</div>
<div id="a-present-the-relation-between-impressions-and-advertising-expenditures." class="section level3">
<h3><strong>a) Present the relation between impressions and advertising expenditures.</strong></h3>
<pre class="r"><code>tbl_1.6 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = investment, y = impressions))+
  geom_smooth(mapping = aes(x = investment, y = impressions),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Investment and Impressions relation&quot;,
       x = &quot;Investment&quot;,
       y = &quot;Impressions&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
</div>
<div id="cap.2-regression-analysis-with-two-variables-some-basic-ideas" class="section level2">
<h2><strong>Cap.2 Regression analysis with two variables: Some basic ideas</strong></h2>
<p>In this chapter the authors go a little deep on simple regression models, introducing the concept of <strong>expected value of Y</strong>. One of the key findings of this chapter is…</p>
<blockquote>
<p>“The essence of regression models is to supply conditions to explain the average value of the Y variable throughout one equation.”</p>
</blockquote>
<p>The authors also affirm that the linear regression model will try to estimate the <strong>population regression function</strong> using economic theories and sample date of the population being studied. With the sample data occur the creation of the regression model for this sample that will be a proxy for the population regression function. Thus, regression models have to deal with inferences and uncertainty.</p>
<p>Also, in this chapter the authors explain that the models of the book are linear on the independent variables (x1, x2, etc), but not on the parameters (<span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, etc). At the end of the chapter, the authors explain about the stochastic erros of the model, that represent all other variables that weren’t included in the developed model.</p>
<div id="exercises-1" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="use-the-table-2.8-with-data-about-food-expenses-and-total-expenses-from-55-houses-in-india-to-answer-the-following-questions." class="section level3">
<h3><strong>2.15) Use the table 2.8 with data about food expenses and total expenses from 55 houses in India to answer the following questions.</strong></h3>
<pre class="r"><code>tbl_2.8 &lt;- read.table(file = paste(path,
                                   &quot;tabela_2.8_expenses_in_india.txt&quot;,
                                   sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T,
                      dec = &quot;,&quot;)

# Random sample
tbl_2.8[sample(nrow(tbl_2.8),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 2
##    food total
##   &lt;int&gt; &lt;int&gt;
## 1   395   745
## 2   315   618
## 3   315   575
## 4   300   630
## 5   295   695</code></pre>
</div>
<div id="a-represent-the-data-drawing-the-food-expenses-in-the-y-axis-and-total-expenses-in-the-x-axis-and-then-plot-the-regression-line." class="section level3">
<h3><strong>a) Represent the data drawing the food expenses in the Y axis and total expenses in the X axis and then plot the regression line.</strong></h3>
<pre class="r"><code>tbl_2.8 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = total, y = food))+
  geom_smooth(mapping = aes(x = total, y = food),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Food expenditures and total expenditures&quot;,
       x = &quot;Total expenditures&quot;,
       y = &quot;Food expenditures&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="cap.3-regression-model-with-two-variables-the-estimation-problem" class="section level2">
<h2><strong>Cap.3 Regression model with two variables: The estimation problem</strong></h2>
<p>The goal of this chapter will be the estimation of the populational regression function using a regression model created from a sample data of the population. The authors indicate two possible methods for this task, (i) the ordinary least squares (OLS) and (ii) the maximum likelihood. Both methods lead to the same results but the first one is easier to apply.</p>
<p>As the focus of the chapter is about estimation, the authors explain the hipothesis that support the estimation when using the OLS method. They are summed up below:</p>
<ul>
<li><p>Independency of the X variables and the error term (<span class="math inline">\(\mu\)</span>).</p></li>
<li><p>The mean value of <span class="math inline">\(\mu\)</span> should be zero. This hipothesis subjectively afirm that there’s not specification mistakes on the model.</p></li>
<li><p>Homoscedasticity of <span class="math inline">\(\mu\)</span>. In other words, there’s constant variation of the <span class="math inline">\(\mu\)</span> term.</p></li>
<li><p>There’s no correlation between <span class="math inline">\(\mu\)</span> values of the model.</p></li>
</ul>
<div id="exercises-2" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="the-following-equation-shows-the-relation-between-exchange-rate-and-inflation-rate-for-canada-and-us-in-the-period-of-1985-to-2005." class="section level3">
<h3><strong>3.19) The following equation shows the relation between exchange rate and inflation rate for Canada and US in the period of 1985 to 2005.</strong></h3>
<p><span class="math display">\[\widehat{Y_t} = 0.912 + 2.25X_t\]</span></p>
<p>Where:</p>
<p><span class="math inline">\(\widehat{Y_t}\)</span> = Canadian Exchange Rate (DC/US$)</p>
<p><span class="math inline">\(X_t\)</span> = US Inflation Rate/Canada Inflation Rate</p>
</div>
<div id="a-write-the-interpretation-for-this-regression-model." class="section level3">
<h3><strong>a) Write the interpretation for this regression model.</strong></h3>
<p>This regression model indicates that as US inflation rate is higher than Canadian inflation rate, the Canadian currency will appreciate, thus the denominator of the fraction <span class="math inline">\(DC/US\)</span> will be lower because inflation erodes the purchasing power of the currency. The r² of 44% indicates that ther’re other variables that could support the model.</p>
</div>
<div id="b-the-positive-value-of-x-has-economic-foundations" class="section level3">
<h3><strong>b) The positive value of X has economic foundations?</strong></h3>
<p>Yes, it does. As the inflation rate is higher in the US, the Canadian dolar will be traded by more US dolars.</p>
</div>
<div id="the-next-table-presents-a-dataset-about-hourly-production-index-x-and-hourly-real-wages-y-for-the-industrial-sector-and-non-agricultural-sector-between-1960-and-2005" class="section level3">
<h3><strong>3.20) The next table presents a dataset about hourly production index (X) and hourly real wages (Y) for the industrial sector and non agricultural sector, between 1960 and 2005</strong></h3>
<pre class="r"><code>tbl_3.6 &lt;- read.table(file = paste(path,&quot;tabela_3.6_wage_and_productivity.txt&quot;,sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;,&quot;)

# Random sample
tbl_3.6[sample(nrow(tbl_3.6),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 5
##    year corporate non_agricultural corporate_wages non_agricultural_wages
##   &lt;int&gt;     &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;                  &lt;dbl&gt;
## 1  2004     133.             132.            119                    118. 
## 2  1982      80.1             80.8            90.4                   90.8
## 3  1993     100.             100.             99.7                   99.5
## 4  1995     102.             102              98.7                   98.8
## 5  1963      55               57.8            66.1                   68.1</code></pre>
</div>
<div id="a-graphically-represent-separetely-y-versus-x-for-the-two-sectors-of-the-economy." class="section level3">
<h3><strong>a) Graphically represent separetely Y versus X for the two sectors of the economy.</strong></h3>
<pre class="r"><code>tbl_3.6 %&gt;% 
  tidyr::gather(&quot;production&quot;,&quot;prod&quot;,2:3) %&gt;% 
  tidyr::gather(&quot;wage&quot;,&quot;wag&quot;,2:3) %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = prod, y = wag, color = production))+
  scale_color_brewer(type = &quot;qual&quot;)+
  geom_smooth(mapping = aes(x = prod, y = wag),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F, 
              lty = 2, 
              color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Production index and hourly wages&quot;,
       x = &quot;Production&quot;,
       y = &quot;Wage&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="c-create-a-regression-model-usgin-ols-method." class="section level3">
<h3><strong>c) Create a regression model usgin OLS method.</strong></h3>
<pre class="r"><code>tbl_3.6 %&gt;% 
  tidyr::gather(&quot;production&quot;,&quot;X&quot;,2:3) %&gt;% 
  tidyr::gather(&quot;wage&quot;,&quot;Y&quot;,2:3) -&gt; df_tbl_3.6

summary(lm(Y~X, data = df_tbl_3.6))</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y ~ X, data = df_tbl_3.6)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.6041 -1.5583  0.3798  1.8433  4.5035 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 32.633974   0.712458   45.80   &lt;2e-16 ***
## X            0.669944   0.007976   83.99   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.37 on 182 degrees of freedom
## Multiple R-squared:  0.9749, Adjusted R-squared:  0.9747 
## F-statistic:  7055 on 1 and 182 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="the-following-dataset-presents-gold-price-nyse-index-and-cpi-index-between-1974-to-2006.-use-this-dataset-to-answer-the-next-questions." class="section level3">
<h3><strong>3.22) The following dataset presents gold price, NYSE index and CPI index between 1974 to 2006. Use this dataset to answer the next questions.</strong></h3>
<pre class="r"><code>tbl_3.7 &lt;- read.table(file = 
                      paste(path,&quot;tabela_3.7_ouro_nyse_IPC.txt&quot;,sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;,&quot;)

# Sample from dataset
tbl_3.7[sample(nrow(tbl_3.7),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 4
##     ano gold_price nyse_index ipc_index
##   &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;
## 1  2003       363.      5447.     184  
## 2  1978       193.       568.      65.2
## 3  1997       331.      4827.     160. 
## 4  2006       603.      8358.     202. 
## 5  1987       446.      1710.     114.</code></pre>
</div>
<div id="a-plot-the-gold-price-nyse-index-and-cpi-index-in-a-dispersion-graph." class="section level3">
<h3><strong>a) Plot the gold price, NYSE index and CPI index in a dispersion graph.</strong></h3>
<pre class="r"><code>tbl_3.7 %&gt;% 
  transmute(x = gold_price,
            y = nyse_index,
            z = ipc_index) -&gt; df_test

x &lt;- df_test$x
y &lt;- df_test$y
z &lt;- df_test$z
  
plot3D::scatter3D(x, y, z, 
                  colvar = NULL, 
                  add = FALSE, 
                  col = &quot;blue&quot;, 
                  pch = 19, 
                  phi = 20, 
                  theta = 20, 
                  cex = .9, 
                  bty = &quot;b2&quot;, 
                  expand = .9, 
                  col.grid = &quot;darkblue&quot;, 
                  col.panel = &quot;steelblue&quot;, 
                  colkey = T, 
                  main = &quot;Table 3.7 data&quot;, 
                  xlab = &quot;gold_price&quot;, 
                  ylab = &quot;nyse_index&quot;, 
                  zlab = &quot;ipc_index&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="b-suposing-that-an-investment-works-as-an-inflation-edge-if-its-profitability-follows-the-inflation-rate-test-the-following-regression-models-using-the-variables-of-the-dataset." class="section level3">
<h3><strong>b) Suposing that an investment works as an inflation edge if its profitability follows the inflation rate, test the following regression models using the variables of the dataset.</strong></h3>
<p><span class="math display">\[gold.price = \beta_1 + \beta_2 + CPI + \mu\]</span>
<span class="math display">\[NYSE.index = \beta_1 + \beta_2 + CPI + \mu\]</span></p>
<pre class="r"><code># Adjusting the dataset to take the profitability of each asset
tbl_3.7 %&gt;% 
  mutate(gold_price_var = round(gold_price/lag(gold_price, 1, default = NA), digits = 8)-1,
         nyse_index_var = round(nyse_index/lag(nyse_index, 1, default = NA), digits = 8)-1,
         ipc_index_var = round(ipc_index/lag(ipc_index, 1, default = NA), digits = 8)-1) %&gt;% 
  filter(!is.na(gold_price_var)) -&gt; tbl_3.7_df_adj</code></pre>
<pre class="r"><code>g1 &lt;- 
  tbl_3.7_df_adj %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = ipc_index_var, y = nyse_index_var), color = &quot;dark orange&quot;)+
  geom_smooth(mapping = aes(x = ipc_index_var, y = nyse_index_var),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  expand_limits(x = 0, y = 0)+
  scale_x_continuous(labels = scales::percent)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  theme(title = element_text(size = 8))+
  labs(title = &quot;NYSE vs CPI&quot;,
       x = &quot;CPI index variation&quot;,
       y = &quot;NYSE index variation&quot;)

g2 &lt;- 
  tbl_3.7_df_adj %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = ipc_index_var, y = gold_price_var), color = &quot;dark orange&quot;)+
  geom_smooth(mapping = aes(x = ipc_index_var, y = gold_price_var),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;,
              se = F,
              lty = 2,
              color = &quot;dark orange&quot;)+
  expand_limits(x = 0, y = 0)+
  scale_x_continuous(labels = scales::percent)+
  scale_y_continuous(labels = scales::percent)+
  theme_graph()+
  theme(title = element_text(size = 8))+
  labs(title = &quot;Gold Price vs CPI&quot;,
       x = &quot;CPI index variation&quot;,
       y = &quot;Gold price variation&quot;)

gridExtra::grid.arrange(g1,g2,ncol = 2)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code># Modelo 1, gold_price ~ ipc_index
tbl_3.7_df_adj %&gt;% 
  lm(formula = gold_price_var ~ ipc_index_var) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = gold_price_var ~ ipc_index_var, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.5428 -0.1114 -0.0066  0.1246  0.5776 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   -0.11471    0.07197  -1.594  0.12148   
## ipc_index_var  3.95969    1.33692   2.962  0.00593 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.219 on 30 degrees of freedom
## Multiple R-squared:  0.2263, Adjusted R-squared:  0.2005 
## F-statistic: 8.772 on 1 and 30 DF,  p-value: 0.005932</code></pre>
<pre class="r"><code># Modelo 2, nyse_index ~ ipc_index
tbl_3.7_df_adj %&gt;% 
  lm(formula = nyse_index_var ~ ipc_index_var) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = nyse_index_var ~ ipc_index_var, data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.24198 -0.08735  0.01355  0.09243  0.23748 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)    0.12125    0.03777   3.210  0.00316 **
## ipc_index_var -0.46036    0.70166  -0.656  0.51676   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1149 on 30 degrees of freedom
## Multiple R-squared:  0.01415,    Adjusted R-squared:  -0.01872 
## F-statistic: 0.4305 on 1 and 30 DF,  p-value: 0.5168</code></pre>
<p>Apparently, gold price is a better asset to edge inflation rate than the NYSE index. Althought there’re two outliers on the gold price data that are impacting the model.</p>
</div>
</div>
<div id="cap.4-classical-normal-linear-regression-model" class="section level2">
<h2><strong>Cap.4 Classical Normal Linear Regression Model</strong></h2>
<p>In this chapter the authors gave attention to the classical normal linear regression model that introduces the concept of normal distrubutions of residuals. As was informed in previus chapter, the goal of the regression model is to estimate parameters (<span class="math inline">\(\hat{\beta}\)</span>) for the populational parameter (<span class="math inline">\(\beta\)</span>) and because it’s an estimation based on samples, the result will vary on each model and the variation of (<span class="math inline">\(\hat{\beta}\)</span>) will follow a particular distribution.</p>
<p>Knowing that <span class="math inline">\(\hat{\beta}\)</span> should follow a particular distribution, the most desired one is the normal distribution. To afirm that the parameter follows the normal distribution, the author prove algebrically that <span class="math inline">\(\hat{\beta}\)</span> is a linear function of the residuals <span class="math inline">\(\mu\)</span>. Then, having the premise that <span class="math inline">\(\mu\)</span> is normally distributed, the premise that <span class="math inline">\(\hat{\beta}\)</span> is normally distributed will be attended. The premise of <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\mu\)</span> being normally distributed will allow inferences about the population’s parameters using confident intervals and test of hypothesis, two techniques developed by classical statisticians.</p>
</div>
<div id="cap.5-two-variables-regression-model-the-confidence-interval-and-hypothesis-test" class="section level2">
<h2><strong>Cap.5 Two variables regression model: The confidence interval and hypothesis test</strong></h2>
<p>After explaining the premises of the classical linear regression model on the last chapter, the authors develop in this chapter the hole regression model to draw inferences about the population parameters being studied. Before they start the explanation, they indicate that the expected value of the sample parameter will be the true value of the populational parameter. Mathematically it means that <span class="math inline">\(e(\hat{\beta}) = \beta\)</span>. Also, they indicate that the reliability of the parameters created by the regression model will be measured by the <strong>standard error</strong>, and this metric will support the development of the confidence interval and test of hypothesis.</p>
<p>After explaining this initial assumptions, the authors explain how to develop confidence intervals for the parameters and how to conduct the test of hypothesis.</p>
<p><strong>Confidence Intervals</strong></p>
<p>In this method, intervals are constructed by two distinct methods. The first is when the populational variance is known and the second is when it isn’t known. For the first case, the <strong>t distribution</strong> is used, for the second, the <strong>normal distribution</strong> is used. In both cases, the interval range will be proportional to the standard error.</p>
<p>The following equation present this method:</p>
<p><span class="math display">\[\hat{\beta} +/- (z|t_\frac{\alpha}{2}  ep(\hat{\beta}))\]</span></p>
<p><strong>Hypothesis Test</strong></p>
<p>In this method, the researcher can use the confidence interval explained before or the test of significance that will create the p-value to allow the conclusion about the hypothesis.</p>
<div id="exercises-3" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="consult-the-regression-model-of-the-equation-3.7.3-to-answer-the-following-questions." class="section level3">
<h3><strong>5.3) Consult the regression model of the equation (3.7.3) to answer the following questions.</strong></h3>
<p><span class="math display">\[\hat{Y} = 14.4773 + 0.0022X_i\]</span></p>
<p><span class="math display">\[ep(\hat{\beta_1}) = 6.1523; ep(\hat{\beta_2}) = 0.00032; r² = 0.6023\]</span></p>
</div>
<div id="a-the-estimated-coeficient-is-statistically-significant-at-5-what-is-the-null-hypothesis-for-this-scenario" class="section level3">
<h3><strong>a) The estimated coeficient is statistically significant at 5%? What is the Null hypothesis for this scenario?</strong></h3>
<p>The significance of the intercept <span class="math inline">\(\beta_1\)</span> will be validated by testing the hypothesis that it is zero. Then we have H0: <span class="math inline">\(\beta_1 = 0\)</span> and H1: <span class="math inline">\(\beta_1 \neq 0\)</span>. For this test we’ll generate the <strong>t-statistics</strong> (from the student distribution, because the population variation is unknown) in order to calculate the p-value and make the conclusion about the Null hypothesis (reject H0 or fail to reject it).</p>
<p><strong>t-statistics</strong></p>
<p><span class="math display">\[\frac{\hat{\beta_1}-\beta_1}{ep(\hat{\beta_1})}\]</span></p>
<p><span class="math display">\[\frac{14,47-0}{6.1523} = 2.35\]</span></p>
<p>Having the <strong>t-statistics</strong>, it’s possible to calculate the <strong>p-value</strong>. For this, we have to use the distribution function for the t distribution, that will ask for the t-statistics and degrees of freedom. This function will show the area under de curve that would contain this t-statistics given this degree of freedom and when we make 1 minus this area under de curve and multiply the result by 2 (because it’s a bicaudal test), we generate the p-value.</p>
<p>The area under the curve for the t distribution with a t-statistics of 2.35 and 34 df is 0.9876. In other words, 98.76% probability of having a value inside the range (-2.35 to 2.35) given 34 df.</p>
<pre class="r"><code>pt(2.35, 34)</code></pre>
<pre><code>## [1] 0.9876413</code></pre>
<p>The p-value will be the probability of having a value outside this range, in this case 1-0.9876. Also, as is a bicaudal test (because H1 is <span class="math inline">\(\beta_1\neq 0\)</span>) it’s necessary to multiply the value by 2. Then, we have:</p>
<pre class="r"><code># p-value calculation in R
(1-pt(2.35, 34))*2</code></pre>
<pre><code>## [1] 0.02471741</code></pre>
<p>Concluding, with a p-value of 2.437% it’s possible to reject H0 because the significance level as of 5%. In other words the intercept of the model is statistically significant at 5% of significance level.</p>
</div>
<div id="b-the-slope-is-statistically-significant-at-5-of-confidence-level-what-is-the-null-hypothesis-for-this-test" class="section level3">
<h3><strong>b) The slope is statistically significant at 5% of confidence level? What is the Null hypothesis for this test?</strong></h3>
<p>The same approach can be used for this question. The hypothesis are H0: <span class="math inline">\(\beta_2 = 0\)</span> e H1: <span class="math inline">\(\beta_2 \neq 0\)</span>. The same equations can be used, adjusting the values of the variables.</p>
<p><strong>t-statistics</strong></p>
<p><span class="math display">\[\frac{0.0022}{0.00032} = 6.88\]</span></p>
<p>This <strong>t-statistics</strong> will generate a pretty low <strong>p-value</strong>, thus the Null hypothesis can be rejected without concern and the conclusion that the parameter of the slope is statistically significant at 5% of confidence can be made.</p>
<pre class="r"><code># p-value using the t-statistics calculated before
(1-pt(6.88,36))*2</code></pre>
<pre><code>## [1] 4.719422e-08</code></pre>
</div>
<div id="the-table-5.5-present-data-about-teachers-earnings-and-students-expenditures-in-1985-for-50-us-states.-in-order-to-verify-the-relation-between-earnings-and-expenditures-the-following-model-was-sugested." class="section level3">
<h3><strong>5.9) The table 5.5 present data about teachers’ earnings and students’ expenditures in 1985 for 50 US states. In order to verify the relation between earnings and expenditures, the following model was sugested.</strong></h3>
<p><span class="math display">\[r = \beta_1+\beta_2\times g + \mu\]</span></p>
<p>Where:</p>
<p>r = teachers’ earnings</p>
<p>g = students’ expenditures</p>
<pre class="r"><code>tbl_5.5 &lt;- 
  read.table(file = paste(path,&quot;tabela_5.5_wage_and_college_expenditures.txt&quot;, sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;.&quot;)

# Dataset sample
tbl_5.5[sample(nrow(tbl_5.5),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 3
##     obs  wage expense
##   &lt;int&gt; &lt;dbl&gt;   &lt;int&gt;
## 1    40  21.0    2509
## 2     9  25.9    4168
## 3    46  25.6    2932
## 4    47  26.0    3705
## 5    26  20.6    2821</code></pre>
</div>
<div id="b-create-a-regression-model-to-analyze-the-standard-error-and-r²." class="section level3">
<h3><strong>b) Create a regression model to analyze the standard error and r².</strong></h3>
<pre class="r"><code>tbl_5.5 %&gt;%
  lm(formula = wage~expense) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = wage ~ expense, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8480 -1.8446 -0.2175  1.6600  5.5293 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.213e+01  1.197e+00   10.13 1.31e-13 ***
## expense     3.308e-03  3.117e-04   10.61 2.71e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.325 on 49 degrees of freedom
## Multiple R-squared:  0.6968, Adjusted R-squared:  0.6906 
## F-statistic: 112.6 on 1 and 49 DF,  p-value: 2.707e-14</code></pre>
<p>With the output, the following equations is generated:</p>
<p><span class="math display">\[\hat{r} = \hat{\beta_1}+\hat{\beta_2}\times g+\mu\]</span></p>
<p><span class="math display">\[\hat{r} = 12.13+0.003308\times g+\mu\]</span></p>
</div>
<div id="d-test-if-the-true-parameter-of-beta-2-is-3.-use-a-confidence-interval-for-this-task." class="section level3">
<h3><strong>d) Test if the true parameter of beta 2 is 3. Use a confidence interval for this task.</strong></h3>
<p>For this scenario, we have: H0: <span class="math inline">\(\beta_2 = 3\)</span> and H1: <span class="math inline">\(\beta_2 \neq 3\)</span>. The equation to build the limits is:</p>
<p><span class="math display">\[\hat{\beta_2} +/- t_\frac{\alpha}{2} \times ep(\hat{\beta_2})\]</span>
Using the <strong>t distribution</strong> because population variance is unknown and <span class="math inline">\(\frac{\alpha}{2}\)</span> because is a two side test (i.e. H1: <span class="math inline">\(\beta_2 \neq 3\)</span>).</p>
<p>To solve this equation, it’s necessary to get the <strong>critical t value</strong> (not <strong>t-statistics</strong> in this case). This value is calculated with the significance level of [1-(5%/2)] and degrees of freedom of the model (49).</p>
<pre class="r"><code># Critical t value
qt(0.975, df = 49)</code></pre>
<pre><code>## [1] 2.009575</code></pre>
<p>Using this value with the parameter value and its standard error, it’s possible to build the interval.</p>
<p><span class="math inline">\(\hat{\beta_2} = 0.003308\)</span></p>
<p><span class="math inline">\(ep(\hat{\beta_2}) = 0.0003117\)</span></p>
<p>Critical t value = 2.009575</p>
<p><span class="math display">\[\hat{\beta_2} +/- t_\frac{\alpha}{2} \times ep(\hat{\beta_2})\]</span></p>
<p><span class="math display">\[0.003308 +/- 2.009575 \times 0.0003117\]</span></p>
<p>Then, the interval is (0.00268 e 0.00393). Allowing the rejection of the Null hypothesis.</p>
</div>
<div id="return-to-the-exercise-3.22-to-use-the-dataset." class="section level3">
<h3><strong>5.13) Return to the exercise 3.22 to use the dataset.</strong></h3>
<pre class="r"><code># Dataset sample
tbl_3.7[sample(nrow(tbl_1.3),10), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 10 x 4
##      ano gold_price nyse_index ipc_index
##    &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;
##  1  1994       384       2687.     148. 
##  2  1978       193.       568.      65.2
##  3  1981       460.       783.      90.9
##  4  1974       159.       464.      49.3
##  5  1989       381.      1903.     124  
##  6  1975       161.       484.      53.8
##  7  1995       384.      3079.     152. 
##  8  1991       362.      2182.     136. 
##  9  1982       376.       729.      96.5
## 10  1987       446.      1710.     114.</code></pre>
</div>
<div id="b-the-regressions-residuals-are-normally-distributed" class="section level3">
<h3><strong>b) The regression’s residuals are normally distributed?</strong></h3>
<p>Yes. The following graph will present the histogram of their values.</p>
<pre class="r"><code># Extracting the residuals from the model
tbl_3.7 %&gt;%
  lm(formula = ipc_index~gold_price) %&gt;%
  residuals() %&gt;% 
  data.frame(a = .) -&gt; res_1
  
res_1 %&gt;%
  ggplot()+
  geom_histogram(mapping = aes(x = a), fill = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Regression model&#39;s residuals distribution&quot;,
       subtitle = &quot;IPC and gold price relation&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code># Extracting the residuals from the model
tbl_3.7 %&gt;%
  lm(formula = ipc_index~nyse_index) %&gt;%
  residuals() %&gt;% 
  data.frame(a = .) -&gt; res_2
  
res_2 %&gt;%
  ggplot()+
  geom_histogram(mapping = aes(x = a), fill = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;Regression model&#39;s residuals distribution&quot;,
       subtitle = &quot;IPC and NYSE index relation&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
<div id="c-in-the-gold-price-regression-model-test-the-hypothesis-that-the-second-parameter-is-1." class="section level3">
<h3><strong>c) In the gold price regression model, test the hypothesis that the second parameter is 1.</strong></h3>
<p>The regression model generated a <span class="math inline">\(\beta_2\)</span> of 0.1693 with a standard error of 0.06624. These values generated a t statistics of 2.556 and a p-value of 0.0157, but considering the Null hypothesis of H0: <span class="math inline">\(\beta_2 = 0\)</span>. To answer the question (H0: <span class="math inline">\(\beta_2 = 1\)</span>), it’s necessary to put these values on the equation below to recalculate the t statistics and the p-value:</p>
<p><span class="math display">\[t.value = \frac{|\hat{\beta_2} - \beta_2|}{SEM}\]</span></p>
<p><span class="math display">\[t.value = \frac{|0.1693 - 1|}{0.06624} = 12.54076\]</span></p>
<p>This value of t will generate a lower p-value, as seging below.</p>
<pre class="r"><code>(1-pt(12.54076, 31))*2</code></pre>
<pre><code>## [1] 1.112443e-13</code></pre>
<p>Then, the Null hypothesis H0: <span class="math inline">\(\beta_2 = 1\)</span> can be rejected, allowing the conclusion that Gold Price is not a perfect edge asset for the inflation rate.</p>
</div>
</div>
<div id="cap.6-extensions-for-the-linear-regression-models-of-two-variables" class="section level2">
<h2><strong>Cap.6 Extensions for the Linear Regression Models of two Variables</strong></h2>
<p>In this chapter the authors present some additional regression models that will be more appropriated in some circunstances. These models are different, but the premise of linearity of parameters is unbroken.</p>
<p>The first model is the <strong>regression model that pass by the origin</strong> that has the form:</p>
<p><span class="math display">\[Y_i = \beta_2 X_i + \mu_i\]</span>
This model is recommended when the theory behind it indicates that there’s no need for a intercept. Without the theory to indicate it, the authors recommend to always maintain the intercept and let the statistics inform if it will be relevant.</p>
<p>Continuing, the authors explain about scales and unities on the models, regarding the difference that can exist in scales between the dependent variables and Y. They show that the conclusion won’t change because these differences, but some algebric manipulations will be need. Also, they presented the standard variables on the regression model, used to analyze the impact of each independent variable on Y and obviously indicated to regression models with multiple variables. The standard variable has the form:</p>
<p><span class="math display">\[Y* = \frac{Y_j - \bar{Y}}{S_Y}\]</span></p>
<p><span class="math display">\[X* = \frac{X_j - \bar{X}}{S_X}\]</span></p>
<p>The next model presented is the logarithmic regression models, like log-log, lin-log and log-lin. And the last model presented is the reciprocal model, that has the following form:</p>
<p><span class="math display">\[Y = \beta_1 + \beta_2 (\frac{1}{X}) + \mu\]</span></p>
<div id="exercises-4" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="the-following-regressions-were-obtained-from-a-dataset-that-has-y-as-texacos-monthly-returns-and-x-as-markets-monthly-return-both-in-percentual." class="section level3">
<h3><strong>6.2 The following regressions were obtained from a dataset that has Y as Texaco’s monthly returns and X as market’s monthly return, both in percentual.</strong></h3>
<p><strong>(i)</strong> <span class="math inline">\(\hat{Y} = 0.00681 + 0.75815 X_t\)</span></p>
<p><span class="math inline">\(ep(\hat{\beta_0}) = 0.02596\)</span>; <span class="math inline">\(ep(\hat{\beta_1}) = 0.27009\)</span></p>
<p>p-value: <span class="math inline">\(\hat{\beta_0} = 0.7984\)</span>; <span class="math inline">\(\hat{\beta_1} = 0.0186\)</span></p>
<p>r² = 44.06%</p>
<p><strong>(ii)</strong> <span class="math inline">\(\hat{Y} = 0.76214 X_t\)</span></p>
<p><span class="math inline">\(ep(\hat{\beta_1}) = 0.265799\)</span></p>
<p>p-value: <span class="math inline">\(\hat{\beta_1} = 0.0131\)</span></p>
<p>r² = 43.68%</p>
</div>
<div id="f-the-normality-jarque-beta-statistics-is-1.1167-for-the-first-model-and-1.1170-for-the-second-model.-what-conclusions-can-be-made-from-this-values" class="section level3">
<h3><strong>f) The normality Jarque-Beta statistics is 1.1167 for the first model and 1.1170 for the second model. What conclusions can be made from this values?</strong></h3>
<p><strong>A:</strong> When the result of this test is far from zero, it indicates that the data isn’t normally distributed. Then, the first model with lower jarque-bera value has it’s residuals closer to a normal distribution than the second model. The Null hypothesis of this test is that residuals are normally distributed, thus <strong>H0: JB = 0</strong> and <strong>H1: JB &gt; 0</strong>.</p>
</div>
<div id="using-the-table-6.7-adjust-the-following-model-and-obtain-the-regression-statistics-of-it." class="section level3">
<h3><strong>6.14 Using the table 6.7, adjust the following model and obtain the regression statistics of it.</strong></h3>
<p><span class="math display">\[\frac{100}{100 - Y_i} = \beta_1 + \beta_2(\frac{1}{X_i})\]</span></p>
<pre class="r"><code>df &lt;- 
  data.frame(y = c(86,79,76,69,65,62,52,51,51,48),
             x = c(3,7,12,17,25,35,45,55,70,120))

df %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = x, y = y))+
  theme_graph()+
  labs(title = &quot;Reciprocal regression model&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre class="r"><code># Gerando o modelo de regressão recíproco
df %&gt;% 
  lm(formula = &quot;100/(100-y) ~ I(1/x)&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;100/(100-y) ~ I(1/x)&quot;, data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.34675 -0.31185 -0.07989  0.18582  0.74362 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   2.0675     0.1596   12.96 1.19e-06 ***
## I(1/x)       16.2662     1.3232   12.29 1.78e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3952 on 8 degrees of freedom
## Multiple R-squared:  0.9497, Adjusted R-squared:  0.9434 
## F-statistic: 151.1 on 1 and 8 DF,  p-value: 1.783e-06</code></pre>
<p><strong>A:</strong> Given the regression output, Y will approach the value of 51.64 as the X value increases.</p>
</div>
<div id="the-table-6.8-presents-data-about-investment-and-saving-for-21-countries.-this-values-are-an-average-obtained-from-1960-to-1974." class="section level3">
<h3><strong>6.15) The table 6.8 presents data about investment and saving for 21 countries. This values are an average obtained from 1960 to 1974.</strong></h3>
<pre class="r"><code>tbl_6.8 &lt;- read.table(file = 
                      paste(path,&quot;tabela_6.8_investment_and_saving.txt&quot;,sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;.&quot;)

# Dataset sample
tbl_6.8[sample(nrow(tbl_6.8),5), ] %&gt;% 
  as.tibble()</code></pre>
<pre><code>## # A tibble: 5 x 3
##   country   investment saving
##   &lt;fct&gt;          &lt;dbl&gt;  &lt;dbl&gt;
## 1 ireland        0.19   0.218
## 2 australia      0.25   0.27 
## 3 autria         0.285  0.282
## 4 italy          0.235  0.224
## 5 canada         0.219  0.231</code></pre>
</div>
<div id="a-graphically-represent-the-relation-between-investment-and-saving" class="section level3">
<h3><strong>a) Graphically represent the relation between investment and saving</strong></h3>
<pre class="r"><code>tbl_6.8 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = saving, y = investment), shape = 20)+
  scale_y_continuous(labels = scales::percent)+
  scale_x_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;Investment and Saving relation&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
<div id="c-develop-a-regression-model-using-a-lin-lin-and-a-log-log-model-approach." class="section level3">
<h3><strong>c) Develop a regression model using a lin-lin and a log-log model approach.</strong></h3>
<p><span class="math display">\[Y_i = \beta_1 + \beta_2 X_i + \mu_i\]</span>
Where:</p>
<p><span class="math inline">\(Y_i = investment\)</span></p>
<p><span class="math inline">\(X_i = saving\)</span></p>
<pre class="r"><code># Lin-lin model
tbl_6.8 %&gt;% 
  lm(formula = investment ~ saving) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = investment ~ saving, data = .)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.023496 -0.014216  0.000176  0.008463  0.040120 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.01734    0.02218  -0.782    0.444    
## saving       1.04772    0.08572  12.222  1.9e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01595 on 19 degrees of freedom
## Multiple R-squared:  0.8872, Adjusted R-squared:  0.8812 
## F-statistic: 149.4 on 1 and 19 DF,  p-value: 1.899e-10</code></pre>
<pre class="r"><code># Log-log model
tbl_6.8 %&gt;% 
  lm(formula = log(investment) ~ log(saving)) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(investment) ~ log(saving), data = .)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.104399 -0.046031  0.001533  0.043086  0.140147 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.06302    0.12411   0.508    0.617    
## log(saving)  1.06308    0.08960  11.865 3.13e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.06391 on 19 degrees of freedom
## Multiple R-squared:  0.8811, Adjusted R-squared:  0.8748 
## F-statistic: 140.8 on 1 and 19 DF,  p-value: 3.134e-10</code></pre>
</div>
<div id="d-how-would-you-interpret-the-slope-coeficient-in-the-lin-lin-model-and-on-the-log-log-model-therere-any-differences-in-the-interpretation" class="section level3">
<h3><strong>d) How would you interpret the slope coeficient in the lin-lin model? And on the log-log model? There’re any differences in the interpretation?</strong></h3>
<p><strong>A:</strong> The lin-lin model has the coeficient of 1.04772, meaning that in increase in 1 pp in saving would generate an increase of 1.04772 pp in investment. On the other hand, in the log-log model, the coeficient is 1.06308, meaning that an 1% increase in saving would cause and 1.06308% increase in investment. There’s a difference in interpretation, because the first model is about changing percentual points and the second is about percentual variation.</p>
</div>
<div id="the-table-6.10-presents-data-about-total-consumption-and-advertising-expendes-for-29-product-categories-in-uk." class="section level3">
<h3><strong>6.19) The table 6.10 presents data about total consumption and advertising expendes for 29 product categories in UK.</strong></h3>
<pre class="r"><code>tbl_6.10 &lt;- read.table(file = 
                      paste(path,&quot;tabela_6.10_advertising_and_total_expenditures.txt&quot;,sep = &quot;&quot;),
                      sep = &quot; &quot;,
                      header = T, 
                      dec = &quot;.&quot;)</code></pre>
</div>
<div id="a-which-regression-model-would-better-represent-the-data" class="section level3">
<h3><strong>a) Which regression model would better represent the data?</strong></h3>
<p><strong>A:</strong> In order to answer this question, it’s better to try different regression models and visualize graphically which of them fit better on the data, taking care to not overfit.</p>
<pre class="r"><code>tbl_6.10 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = advertising, y = consumo))+
  geom_smooth(mapping = aes(x = advertising, y = consumo),
              formula = y ~ x,
              method = &quot;lm&quot;,
              se = F)+
  geom_smooth(mapping = aes(x = advertising, y = consumo),
            formula = &quot;y ~ I(x^0.5)&quot;,
            method = &quot;lm&quot;,
            se = F,
            color = &quot;red&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code>tbl_6.10 %&gt;% 
  lm(formula = &quot;consumo ~ I(advertising^.5)&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;consumo ~ I(advertising^.5)&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4133.5 -1497.1  -779.5   318.6  8730.6 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)        -106.556   1045.327  -0.102   0.9196  
## I(advertising^0.5)   16.775      7.023   2.388   0.0242 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2928 on 27 degrees of freedom
## Multiple R-squared:  0.1744, Adjusted R-squared:  0.1439 
## F-statistic: 5.705 on 1 and 27 DF,  p-value: 0.02417</code></pre>
<pre class="r"><code>tbl_6.10 %&gt;% 
  lm(formula = &quot;consumo ~ advertising&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;consumo ~ advertising&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5056.7 -1280.6  -831.6   -14.2  8100.8 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 857.03870  720.14179   1.190   0.2444  
## advertising   0.05277    0.02146   2.458   0.0207 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2913 on 27 degrees of freedom
## Multiple R-squared:  0.1829, Adjusted R-squared:  0.1526 
## F-statistic: 6.044 on 1 and 27 DF,  p-value: 0.02066</code></pre>
<p>It seems that the linear regression model fits better the data, because it generated a higher R² and a lower p-value for the slope.</p>
</div>
<div id="recall-the-3.3-example-from-chapter-3-to-answer-the-following-questions" class="section level3">
<h3><strong>6.20) Recall the 3.3 example from chapter 3 to answer the following questions:</strong></h3>
<pre class="r"><code>tbl_3.3 &lt;- read.table(file = 
                      paste(path,&quot;tabela_3.3_telefonia_movel_e_computadores.txt&quot;,sep = &quot;&quot;),
                      sep = &quot;;&quot;,
                      header = T, 
                      dec = &quot;,&quot;)

# Dataset sample
tbl_3.3[sample(nrow(tbl_3.3),10), ]</code></pre>
<pre><code>##         county cellphone    pc per_capita_income
## 15     hungary     76.88 10.84             13840
## 8     colombia     14.13  4.93              6410
## 19       japan      9.76 22.83             28450
## 28 switzerland     84.34 70.87             32220
## 14   guatemala     13.15  1.44              4090
## 6       canada     41.90 48.70             30040
## 9      equator     18.92  3.24              3940
## 4       brazil     63.62 84.70              7510
## 11      france      5.96 17.43             27640
## 10       egypt     54.80 19.20              3940</code></pre>
</div>
<div id="a-graphically-represent-cellphone-demand-in-relation-to-per-capita-income-adjusted-by-ppp." class="section level3">
<h3><strong>a) Graphically represent cellphone demand in relation to per capita income adjusted by PPP.</strong></h3>
<pre class="r"><code>tbl_3.3 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = cellphone, y = per_capita_income))+
  theme_graph()+
  labs(title = &quot;Cellphone vs Per capital income&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
</div>
<div id="b-graphically-represent-cellphone-demand-in-relation-to-per-capita-income-both-adjusted-by-logarithms." class="section level3">
<h3><strong>b) Graphically represent cellphone demand in relation to per capita income, both adjusted by logarithms.</strong></h3>
<pre class="r"><code>tbl_3.3 %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = cellphone, y = per_capita_income))+
  scale_y_log10()+
  scale_x_log10()+
  theme_graph()+
  labs(title = &quot;Cellphone vs Per capita income, log adjusted&quot;)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
</div>
<div id="d-analysing-these-two-graphs-do-you-believe-that-a-log-log-model-can-fit-better-the-data-develop-the-regression-model-for-a-log-log-method." class="section level3">
<h3><strong>d) Analysing these two graphs, do you believe that a log-log model can fit better the data? Develop the regression model for a log-log method.</strong></h3>
<pre class="r"><code>tbl_3.3 %&gt;% 
  lm(formula = &quot;log(cellphone) ~ log(per_capita_income)&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;log(cellphone) ~ log(per_capita_income)&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.0882 -0.3814  0.3393  0.5389  0.9273 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)              1.6758     1.5428   1.086    0.286
## log(per_capita_income)   0.2149     0.1636   1.313    0.199
## 
## Residual standard error: 0.7919 on 29 degrees of freedom
## Multiple R-squared:  0.05611,    Adjusted R-squared:  0.02357 
## F-statistic: 1.724 on 1 and 29 DF,  p-value: 0.1995</code></pre>
<pre class="r"><code>tbl_3.3 %&gt;% 
  lm(formula = &quot;cellphone ~ per_capita_income&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;cellphone ~ per_capita_income&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -56.172 -21.258   5.764  17.062  44.253 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       3.497e+01  8.636e+00   4.050 0.000349 ***
## per_capita_income 9.826e-04  4.344e-04   2.262 0.031377 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 27.1 on 29 degrees of freedom
## Multiple R-squared:   0.15,  Adjusted R-squared:  0.1206 
## F-statistic: 5.116 on 1 and 29 DF,  p-value: 0.03138</code></pre>
<p><strong>A:</strong> The lin-lin model generated a slope with a lower p-value and a higher r². Thus I would prefer this model instead of the log-log model.</p>
</div>
<div id="cap.7-multiple-regression-analysis-the-estimation-problem" class="section level3">
<h3><strong>Cap.7 Multiple Regression Analysis: The Estimation Problem</strong></h3>
<p>The authors in this chapter explain how multiple regression have to be analyzed. They start presenting the hypothesis of this model, that are the same of simple regression models and then they explain that the coeficients of the model will measure the effect on the mean value of Y considering the other variables constant.</p>
<p>This consideration is made by removing the effect of the variation in one independent variable on the other (X1 and X2). Multiple regression models will generate the R² metric, that is different from the r², because it’ll represent the proportion of Y that is explained by all other independente variables of the model (X1, X2, X3…).</p>
<p>Then, the author continue explaining the impact thar multiple regression models can suffer by specification errors in the design of the model. In this case, the error would be a missing independent variable.</p>
<p>Also, the author shows that in a multiple regression model, you’ll have an Adjusted R² that will inform the same information of R² but considering the amount of variables including in the model. The Adjusted R² in this case, penalizes the inclusion of two many independente variables. At the end, they explain that in regression models, it’s not relevant to seek a high Adjusted R², because the goal is to make reliable estimation of the populational parameters <span class="math inline">\(\beta\)</span> in order to make predictions.</p>
</div>
<div id="exercises-5" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="consider-the-table-7.5-below" class="section level3">
<h3><strong>7.1 Consider the table 7.5 below:</strong></h3>
<pre><code>##   y x_2 x_3
## 1 1   1   2
## 2 2   2   1
## 3 8   3  -3</code></pre>
<p>Build the following models:</p>
<p><span class="math inline">\(Y = \alpha_1 + \alpha_2 X_2 + \mu\)</span></p>
<p><span class="math inline">\(Y = \lambda_1 + \lambda_3 X_3 + \mu\)</span></p>
<p><span class="math inline">\(Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \mu\)</span></p>
<p>Verify if <span class="math inline">\(\alpha_2 = \beta_2\)</span>, <span class="math inline">\(\lambda_3 = \beta_3\)</span>.</p>
<p><strong>A:</strong> Building the models.</p>
<pre class="r"><code>df &lt;- data.frame(y = c(1,3,8),
                 x_2 = c(1,2,3),
                 x_3 = c(2,1,-3))

# First model
df %&gt;% 
  lm(formula = y ~ x_2) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x_2, data = .)
## 
## Residuals:
##    1    2    3 
##  0.5 -1.0  0.5 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   -3.000      1.871  -1.604    0.355
## x_2            3.500      0.866   4.041    0.154
## 
## Residual standard error: 1.225 on 1 degrees of freedom
## Multiple R-squared:  0.9423, Adjusted R-squared:  0.8846 
## F-statistic: 16.33 on 1 and 1 DF,  p-value: 0.1544</code></pre>
<pre class="r"><code># Second model
df %&gt;% 
  lm(formula = y ~ x_3) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x_3, data = .)
## 
## Residuals:
##        1        2        3 
## -0.28571  0.35714 -0.07143 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)   4.0000     0.2673   14.97   0.0425 *
## x_3          -1.3571     0.1237  -10.97   0.0579 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4629 on 1 degrees of freedom
## Multiple R-squared:  0.9918, Adjusted R-squared:  0.9835 
## F-statistic: 120.3 on 1 and 1 DF,  p-value: 0.05787</code></pre>
<pre class="r"><code># Third model
df %&gt;% 
  lm(formula = y ~ x_2 + x_3) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x_2 + x_3, data = .)
## 
## Residuals:
## ALL 3 residuals are 0: no residual degrees of freedom!
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)        2         NA      NA       NA
## x_2                1         NA      NA       NA
## x_3               -1         NA      NA       NA
## 
## Residual standard error: NaN on 0 degrees of freedom
## Multiple R-squared:      1,  Adjusted R-squared:    NaN 
## F-statistic:   NaN on 2 and 0 DF,  p-value: NA</code></pre>
<p>Then, after running the models in R, we have:</p>
<p><span class="math inline">\(Y = -3 + 3.5 X_2 + \mu\)</span></p>
<p><span class="math inline">\(Y = 4 - 1.3571X_3 + \mu\)</span></p>
<p><span class="math inline">\(Y = 2 + 1 X_2 - 1 X_3 + \mu\)</span></p>
<p>With <span class="math inline">\(\alpha_2 = 3.5\)</span> and <span class="math inline">\(\beta_2 = 1\)</span>; <span class="math inline">\(\lambda_3 = -1.3571\)</span> and <span class="math inline">\(\beta_3 = -1\)</span>. These differences indicate that the independent variables have correlation between then, thus the coeficients diverge when putting together and when putther separatelly on the models.</p>
</div>
<div id="suposing-that-you-have-the-following-equations" class="section level3">
<h3><strong>7.13) Suposing that you have the following equations:</strong></h3>
<p><span class="math display">\[Y = \alpha + \alpha_2 X + \mu\]</span>
<span class="math display">\[Z = \beta + \beta_2 X + \mu\]</span>
Onde Y = consumo, Z = poupança e X = (Y + Z).</p>
</div>
<div id="a-what-is-the-relation-between-alpha-2-and-beta-2-show-your-calculus." class="section level3">
<h3><strong>a) What is the relation between alpha 2 and beta 2? Show your calculus.</strong></h3>
<p><strong>A:</strong> The slope of the models summed is 1 and their standard error is equal. Following is a random dataset to exemplify this situation.</p>
<pre class="r"><code># Random dataset
df &lt;- data.frame(y = rnorm(100, 10, 3),
           z = rnorm(100, 25, 5))

g1 &lt;- df %&gt;% 
  mutate(x = y + z) %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = x, y = y))+
  geom_smooth(mapping = aes(x = x, y = y),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;)+
  theme_graph()

g2 &lt;- df %&gt;% 
  mutate(x = y + z) %&gt;% 
  ggplot()+
  geom_point(mapping = aes(x = x, y = z))+
  geom_smooth(mapping = aes(x = x, y = z),
              formula = &quot;y ~ x&quot;,
              method = &quot;lm&quot;)+
  theme_graph()

gridExtra::grid.arrange(g1,g2, ncol = 2)</code></pre>
<p><img src="/books/2020-03-03-basic-econometrics-by-gujarati-and-porter_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>Creating the regression model for the two variables (y and z).</p>
<pre class="r"><code>df %&gt;% 
  mutate(x = y + z) %&gt;% 
  lm(formula = &quot;y ~ x&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;y ~ x&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2326 -1.9702  0.1131  1.9126  4.9464 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -1.7806     1.4646  -1.216    0.227    
## x             0.3478     0.0419   8.301 5.75e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.564 on 98 degrees of freedom
## Multiple R-squared:  0.4129, Adjusted R-squared:  0.4069 
## F-statistic: 68.91 on 1 and 98 DF,  p-value: 5.747e-13</code></pre>
<pre class="r"><code>df %&gt;% 
  mutate(x = y + z) %&gt;% 
  lm(formula = &quot;z ~ x&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;z ~ x&quot;, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.9464 -1.9126 -0.1131  1.9702  5.2326 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.7806     1.4646   1.216    0.227    
## x             0.6522     0.0419  15.565   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.564 on 98 degrees of freedom
## Multiple R-squared:  0.712,  Adjusted R-squared:  0.7091 
## F-statistic: 242.3 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="c-its-possible-to-compare-r²-of-both-models" class="section level3">
<h3><strong>c) It’s possible to compare R² of both models?</strong></h3>
<p><strong>A:</strong> No, because Y and Z can have different weights on X, thus the R² would include this difference and preclude this comparisson.</p>
</div>
<div id="the-table-7.6-presents-roses-demand-by-quarter." class="section level3">
<h3><strong>7.6) The table 7.6 presents roses demand by quarter.</strong></h3>
<pre class="r"><code>tbl_7.6 &lt;- read.table(file = 
                      paste(path,&quot;tabela_7.6_flower_demand_in_detroit.txt&quot;,sep = &quot;&quot;),
                      sep = &quot;;&quot;,
                      header = T, 
                      dec = &quot;.&quot;)

# Dataset sample
tbl_7.6[sample(nrow(tbl_7.6),5), ]</code></pre>
<pre><code>##    year     y   x2   x3     x4 x5
## 9     9  8038 2.60 3.13 180.49  9
## 4     4 10079 2.91 3.64 172.92  4
## 14   14  5868 2.96 3.12 188.20 14
## 6     6  8862 2.77 3.66 198.62  6
## 2     2  9348 2.54 2.85 173.36  2</code></pre>
<p>In this table we have <strong>y = sold roses</strong>, <strong>x2 = average rose price</strong>, <strong>x3 = average clove price</strong>, <strong>x4 = average family income available</strong> and <strong>x5 = trend variable</strong>.</p>
<p>Consider the following demand functions:</p>
<p><span class="math display">\[Y = \alpha_1 + \alpha_2 X_2 + \alpha_3 X_3 + \alpha_4 X_4 + \alpha_5 X_5 + \mu\]</span></p>
<p><span class="math display">\[ln(Y) = \beta_1 + \beta_2 ln(X_2) + \beta_3 ln(X_3) + \beta_4 ln(X_4) + \beta_5 ln(X_5) + \mu\]</span>
### <strong>a) Estimate the parameters of the linear model and interpret the results</strong></p>
<pre class="r"><code>tbl_7.6 %&gt;% 
  lm(formula = &quot;y ~ x2 + x3 + x4 + x5&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;y ~ x2 + x3 + x4 + x5&quot;, data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1095.47  -670.22   -76.67   569.51  1945.14 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 10816.043   5988.348   1.806   0.0983 .
## x2          -2227.704    920.466  -2.420   0.0340 *
## x3           1251.141   1157.021   1.081   0.3027  
## x4              6.283     30.622   0.205   0.8412  
## x5           -197.400    101.561  -1.944   0.0780 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 969.9 on 11 degrees of freedom
## Multiple R-squared:  0.8347, Adjusted R-squared:  0.7746 
## F-statistic: 13.89 on 4 and 11 DF,  p-value: 0.0002805</code></pre>
<p><strong>A:</strong> With R output, we have the following linear regression model:</p>
<p><span class="math display">\[\hat{Y} = 10.816 - 2.227,7 X_2 + 1.251,1 X_3 + 6,3 X_4 - 197,4 X_5\]</span>
With this regression model, <span class="math inline">\(\alpha_2 = -2.227,7\)</span> indicates that rose demand will be reduced in 2.227 if rose price increases, <span class="math inline">\(\alpha_3 = 1.251,1\)</span> indicates that rose demand will increase if clove price increase, in this case they are substitutes, <span class="math inline">\(\alpha_4 = 6,3\)</span> indicates that rose demnad will increase if average income availability increase and <span class="math inline">\(\alpha_5 = -197,4\)</span> indicate that rose demand is in a fall trend.</p>
</div>
<div id="b-estimate-the-parameters-of-the-log-log-model-and-interpret-the-results" class="section level3">
<h3><strong>b) Estimate the parameters of the log-log model and interpret the results</strong></h3>
<pre class="r"><code>tbl_7.6 %&gt;% 
  lm(formula = &quot;log(y) ~ log(x2) + log(x3) + log(x4) + log(x5)&quot;) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;log(y) ~ log(x2) + log(x3) + log(x4) + log(x5)&quot;, 
##     data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.28610 -0.08998  0.01395  0.07445  0.30779 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)   0.6268     6.1483   0.102   0.9206  
## log(x2)      -1.2736     0.5266  -2.418   0.0341 *
## log(x3)       0.9373     0.6592   1.422   0.1828  
## log(x4)       1.7130     1.2008   1.426   0.1815  
## log(x5)      -0.1816     0.1279  -1.420   0.1833  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1689 on 11 degrees of freedom
## Multiple R-squared:  0.778,  Adjusted R-squared:  0.6972 
## F-statistic: 9.635 on 4 and 11 DF,  p-value: 0.001343</code></pre>
<p><strong>A:</strong> With R output, we have the following log-log regression model:</p>
<p><span class="math display">\[\hat{Y} = 0.63 - 1.27 X_2 + 0.94 X_3 + 1.71 X4 - 0.18 X_5\]</span>
Given this regression model, <span class="math inline">\(\beta_2 = -1.27\)</span> indicates that rose demand will fall in 1.27% if rose price increase in 1%, <span class="math inline">\(\beta_3 = 0.93\)</span> indicates that rose demand will increase in 0.93% if clove price increase in 1%, <span class="math inline">\(\beta_4 = 1.71\)</span> indicates that rose demand will increase in 1.71% if average family income availability increase in 1% and <span class="math inline">\(\beta_5 = -0.18\)</span> indicates that rose demand is falling in a trend of -0.18% per quarter.</p>
</div>
<div id="c-the-parameters-beta-2-3-and-4-represent-price-elasticity-cross-price-elasticity-and-income-elasticity-for-rose-demand" class="section level3">
<h3><strong>C) The parameters beta 2, 3 and 4 represent price elasticity, cross price elasticity and income elasticity for rose demand?</strong></h3>
<p><strong>A:</strong> Yes, because they’re on transformed by natural log, then small variations can be interpreted as percentual variations. But this model generated parameters with high p-value, then these parameters aren’t strong predictors for the variable being studied.</p>
</div>
<div id="usas-defense-budget-between-1962-and-1981-is-represented-by-the-table-7.8.-to-explain-this-budget-use-the-following-model" class="section level3">
<h3><strong>7.18) USA’s defense budget between 1962 and 1981 is represented by the table 7.8. To explain this budget, use the following model:</strong></h3>
<p><span class="math display">\[Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 + \mu\]</span>
Where</p>
<p><span class="math inline">\(Y\)</span> = Anual expenditure with defense in billions</p>
<p><span class="math inline">\(\beta_2\)</span> = GNP in billions</p>
<p><span class="math inline">\(\beta_3\)</span> = Sales of militar assistance in billions</p>
<p><span class="math inline">\(\beta_4\)</span> = Sales from aerospace industry in billions</p>
<p><span class="math inline">\(\beta_5\)</span> = Militar conflicts with more than 100.000 soldiers. Binary variable (i.e. 1 or 0)</p>
</div>
<div id="cap.8-multple-regression-analysis-the-inference-problem" class="section level3">
<h3><strong>Cap.8 Multple Regression Analysis: The Inference Problem</strong></h3>
<p>In this chapter the author record the reader that OLS method to develop regression models doen’t make any assumption about the error distribution <span class="math inline">\(\mu\)</span> and the OLS would be fine if the goal is to make point estimation of the parameters of the model. But when the goal is to make estimations about the populational parameters, the normality of <span class="math inline">\(\mu\)</span> assumption will be required.</p>
<p>After, they present multiple hypothesis that can be tested in a multiple regression model, like:</p>
<ol style="list-style-type: decimal">
<li><p>Test hypothesis about one coeficient of the model (similar to Simple Regression Models).</p></li>
<li><p>Test the general significance of the model. In other words, test all the parameters (<span class="math inline">\(\beta\)</span>) of the model.</p></li>
<li><p>Test if two or more parameters are equal.</p></li>
<li><p>Test if parameters make sense given a particular theory behind the model.</p></li>
<li><p>Test the model stability given different data samples or different periods.</p></li>
</ol>
<p>In this chapter the author gave special attention to the F statistics. That is a metric that indicates the relevance of the paramters choosen for the regression model. Also, they presented the relation between F statistics and R².</p>
<p>Then, to end the chapter, they presented some discussion about when it is necessary to add new variables on a multiple regression model.</p>
</div>
</div>
