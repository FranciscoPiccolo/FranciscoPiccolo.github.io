---
title: "Data Analysis and Graphs with R by J.Maindonald and J.Braun"
date: 2020-05-14
output:
  html_document:
    fig_width: 8.5
    fig_height: 4
---



<div id="overview" class="section level2">
<h2><strong>Overview</strong></h2>
<p>This book is part of a project made by Cambrige University called <strong>Cambridge Series in Statistics and Probabilistic Mathematics</strong> aiming to spread knowledge in statistics. As well as this book, there’re other 19 that explore other areas of statistics.</p>
<p>The content of this book is more generic, where the authors explain a range of concepts, from exploratory data analysis, statistical tests, tree models and survival analysis. The book was published in 2006 (the second edition) and I think because of that the authors didn’t use libraries like ggplot or the tidyverse universe.</p>
<p>The explanations given by the authors are clear and easy to understand, also the utilization of data sets from R packages (e.g. DAAG) simplify reproducibility of examples and execution of models to solve the proposed exercises.</p>
<div class="figure">
<img src="/images/2020-02-07_data_analysis_with_r.png" alt="Book cover" />
<p class="caption">Book cover</p>
</div>
</div>
<div id="chapter-1---a-brief-introduction-to-r" class="section level2">
<h2><strong>Chapter 1 - A brief introduction to R</strong></h2>
<p>In this chapter the focus is to introduce the R enviroment to the reader. With this goal, the authors explain the types of data that R can handle, how to build functions and what kind of graphs are possible with this tool. Let’s see some exercises presented this chapter.</p>
<div id="exercises" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="the-following-table-gives-the-size-of-the-floor-area-ha-and-the-price-a000-for-15-houses-sold-in-the-canberra-australia-suburb-of-aranda-in-1999." class="section level3">
<h3><strong>1. The following table gives the size of the floor area (ha) and the price ($A000), for 15 houses sold in the Canberra (Australia) suburb of Aranda in 1999.</strong></h3>
<pre class="r"><code>df &lt;- data.frame(area = c(694,905,802,1366,716,963,821,714,1018,887,790,696,771,1006,1191),
                 sale.price = c(192,215,215,274,112.7,185,212,220,276,260,221,255,260,293,375))</code></pre>
</div>
<div id="a-plot-sale.price-versus-area." class="section level3">
<h3><strong>a) Plot sale.price versus area.</strong></h3>
<p><strong>A:</strong> For this, I’ll use ggplot library instead of base R plots, because it’s easier and prettier.</p>
<pre class="r"><code>df %&gt;% 
  ggplot2::ggplot()+
  geom_point(mapping = aes(x = area, y = sale.price), 
             color = &quot;darkred&quot;)+
  theme_graph()+
  labs(title = &quot;Sales Price vs Area&quot;)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="b-use-hist-command-to-plot-a-histogram-of-the-sales.price." class="section level3">
<h3><strong>b) Use hist() command to plot a histogram of the sales.price.</strong></h3>
<p><strong>A:</strong> In this alternative I won’t use the hist() command. Instead I’ll opt for the geom_histogram from ggplot.</p>
<pre class="r"><code>df %&gt;% 
  ggplot2::ggplot()+
  geom_histogram(mapping = aes(x = sale.price),
                 fill = &quot;darkblue&quot;,
                 alpha = .4)+
  theme_graph()+
  labs(title = &quot;Sales Price Histogram&quot;)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="c-repeat-a-and-b-after-taking-logarithms-of-sale-prices." class="section level3">
<h3><strong>c) Repeat (a) and (b) after taking logarithms of sale prices.</strong></h3>
<p><strong>A:</strong> It’s possible to reproduce the same graphs and put the log() formula on the variables inside aes() function.</p>
<pre class="r"><code>df %&gt;% 
  ggplot2::ggplot()+
  geom_point(mapping = aes(x = log(area), y = log(sale.price)),
             color = &quot;darkred&quot;)+
  theme_graph()+
  labs(title = &quot;Sales Price vs Area using log transformation&quot;)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The histogram will follow the same logic.</p>
<pre class="r"><code>df %&gt;% 
  ggplot2::ggplot()+
  geom_histogram(mapping = aes(x = log(sale.price)),
                 fill = &quot;darkblue&quot;,
                 alpha = .4)+
  theme_graph()+
  labs(title = &quot;Sale Price Histograma&quot;,
       subtitle = &quot;Log transformed&quot;)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-2---styles-of-data-analysis" class="section level2">
<h2><strong>Chapter 2 - Styles of data analysis</strong></h2>
<p>This chapter explains about exploratory data analysis, a technique developed by John Tukey to support researchers during their initial problem exploration. The aythors present the types of graphs available for researchers and when to use each of them. Also, they explain about outliers and data skewness, showing how to deal with each situation.</p>
<div id="exercises-1" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="use-the-lattice-function-bwplot-to-display-for-each-combination-of-site-and-sex-in-the-data-frame-possumdaag-package-the-distribution-of-ages.-show-the-different-sites-on-the-same-panel-with-different-panels-for-different-sexes." class="section level3">
<h3><strong>1) Use the lattice function bwplot() to display, for each combination of <em>site</em> and <em>sex</em> in the data frame <em>possum</em>(DAAG package), the distribution of ages. Show the different sites on the same panel, with different panels for different sexes.</strong></h3>
<p><strong>A:</strong> bwplot means box and whisker plot (a.k.a boxplot). It’s a good visualization to present distribution among different categories. Let’s create the boxplot using ggplot.</p>
<pre class="r"><code>DAAG::possum %&gt;%
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = age, group = site, fill = as.factor(site)),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  theme_graph()+
  theme(legend.title = element_text(size = 7))+
  scale_fill_viridis_d()+
  facet_grid(~sex)+
  labs(title = &quot;Age distribution for different sites&quot;)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="plot-a-histogram-of-the-earconch-measurements-for-the-possum-data.-the-distribution-should-appear-bimodal-two-peaks.-this-is-a-simple-indication-of-clustering-possibly-due-to-sex-differences.-obtain-side-by-side-boxplots-of-the-male-and-female-earconch-measurements.-how-do-these-measurement-distributions-differ-can-you-predict-what-the-corresponding-histograms-would-look-like-plot-them-to-check-your-answer." class="section level3">
<h3><strong>3) Plot a histogram of the earconch measurements for the possum data. The distribution should appear bimodal (two peaks). This is a simple indication of clustering, possibly due to sex differences. Obtain side-by-side boxplots of the male and female earconch measurements. How do these measurement distributions differ? Can you predict what the corresponding histograms would look like? Plot them to check your answer.</strong></h3>
<p><strong>A:</strong> If the data ser contains men and women in a similar proportion, the histogram will be bicaudal, because the measure differ for each gender.</p>
<pre class="r"><code>DAAG::possum %&gt;% 
  ggplot2::ggplot()+
  geom_histogram(mapping = aes(x = earconch),
                 fill = &quot;darkred&quot;,
                 alpha = .2,
                 color = &quot;darkred&quot;,
                 size = 1)+
  theme_graph()+
  labs(title = &quot;Earconch Distribution&quot;)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Now let’s see the boxblot for each gender.</p>
<pre class="r"><code>DAAG::possum %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = earconch, fill = sex),
               alpha = .2)+
  scale_fill_viridis_d()+
  theme_graph()+
  labs(title = &quot;Earconch box plot distribution&quot;,
       x = &quot;&quot;,
       y = &quot;&quot;)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-3---statistical-models" class="section level2">
<h2><strong>Chapter 3 - Statistical models</strong></h2>
<p>This chapter introduce the fundations of statistical models. First the authors explain the importance of statistical models, showing the difference between physical phenomenon and random events that has singnal and noise well established.</p>
<p>They affirm that models have to provide accurate inferences in order to allow predictions. Thus, for researchers build reliable models it’s necessary to set hypothesis about data distribution, sampling method and also assumptions about the parameters of the model.</p>
<p>As the assumption of the data distribution is an important part of model definition, the authors explain how to validate if the data is normally distributed, using visual displays (i.e. histograms on qq-plots) or formal tests.</p>
<div id="exercises-2" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="use-y---rnorm100-to-generate-a-random-sample-of-100-numbers-from-a-normal-distribution.-calculate-the-mean-and-standard-deviation-of-y.-now-put-the-calculation-in-a-loop-and-repeat-25-times.-store-the-25-means-in-a-vector-named-av.-calculate-the-standard-deviation-of-the-values-in-av." class="section level3">
<h3><strong>2) Use y &lt;- rnorm(100) to generate a random sample of 100 numbers from a normal distribution. Calculate the mean and standard deviation of y. Now put the calculation in a loop and repeat 25 times. Store the 25 means in a vector named av. Calculate the standard deviation of the values in av.</strong></h3>
<pre class="r"><code>y &lt;- rnorm(100)

tibble(mean = mean(y),
           sd = sd(y))</code></pre>
<pre><code>## # A tibble: 1 x 2
##      mean    sd
##     &lt;dbl&gt; &lt;dbl&gt;
## 1 -0.0429 0.962</code></pre>
<p><strong>A:</strong> In order to run this process, it’s necessary to create an empty variable with 25 slots available, then create a <em>for lopp</em> that will calculate the mean for a sample of 100 values taken from a normal distribution, 25 times.</p>
<pre class="r"><code># Creating an empty variable with 25 slots
output &lt;- rep(NA,25)

# Creating the for loop
for(i in 1:25){
  output[i] &lt;- data.frame(
    mean(rnorm(100))
  )
}

# Extracting the output and inserting it in a data.frame
df &lt;- data.frame(
  a = matrix(unlist(output),nrow = 25,byrow = F), stringsAsFactors = F)

# Calculating the mean and sd
tibble(mean = mean(df[,1]),
           sd = sd(df[,1]))</code></pre>
<pre><code>## # A tibble: 1 x 2
##     mean     sd
##    &lt;dbl&gt;  &lt;dbl&gt;
## 1 0.0243 0.0893</code></pre>
</div>
<div id="create-a-function-that-does-the-calculations-of-exercise-2.-run-the-function-several-times-showing-each-of-the-distributions-of-25-means-in-a-density-plot." class="section level3">
<h3><strong>3) Create a function that does the calculations of Exercise 2. Run the function several times, showing each of the distributions of 25 means in a density plot.</strong></h3>
<pre class="r"><code># Creating the function, x = repetitions, y = random samples
fun &lt;- function(x, y){
  output &lt;- rep(NA, x)
  for(i in 1:x){
    output[i] &lt;- data.frame(
      round(mean(rnorm(y)), digits = 2)
    )
  }
  df &lt;- data.frame(
    a = matrix(unlist(output), 
               nrow = x,
               byrow = T), 
    stringsAsFactors = F)
}

# Running the function and ploting the results
fun(25, 100) %&gt;% 
  ggplot2::ggplot()+
  geom_density(mapping = aes(x = a),
               fill = &quot;dark orange&quot;,
               alpha = .2)+
  theme_graph()+
  labs(title = &quot;Density plot for &quot;)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-4---an-introduction-to-formal-inference" class="section level2">
<h2><strong>Chapter 4 - An introduction to formal inference</strong></h2>
<p>This chapter introduces some basic concepts about statistical inference. The main concepts about this subject are <strong>Standard Error of the Mean</strong>; <strong>student distribution</strong>; <strong>degress of freedom</strong>; <strong>p-values</strong>; <strong>confidence intervals</strong>; <strong>test of hypothesis</strong>.</p>
<p>The authors explain the formulas for some of these variables, how to perform hypothesis test and build confidence intervals and also, why researchers are moving from hypothesis test to confidence intervals.</p>
<div id="exercises-3" class="section level3">
<h3><strong>Exercises</strong></h3>
</div>
<div id="in-the-data-set-nsw74demo-daag-determine-95-confidence-intervals-for-a-the-1974-incomes-of-each-of-the-treated-and-control-groups-b-the-1975-incomes-of-each-group-c-the-1978-incomes-of-each-group.-finally-calculate-a-95-confidence-interval-for-the-difference-in-income-between-treated-and-controls-in-1978." class="section level3">
<h3><strong>1) In the data set nsw74demo (DAAG) determine 95% confidence intervals for: (a) the 1974 incomes of each of the “treated” and “control” groups; (b) the 1975 incomes of each group; (c) the 1978 incomes of each group. Finally, calculate a 95% confidence interval for the difference in income between treated and controls in 1978.</strong></h3>
<p><strong>A:</strong> As explained in this chapter, confidence intervals are build using the following equation:</p>
<p><span class="math display">\[\mu - (t_\frac{\alpha}{2} \times SEM)\]</span>
Changing the signals from minus to plus in order to obtain the the upper limit instead of the lower.</p>
<p>Where:</p>
<p><span class="math inline">\(\mu\)</span> = the sample mean given in the data set.</p>
<p>$t_ = $ Critical t value obtained using the confidence level (<span class="math inline">\(\alpha\)</span>) and dividing by two because we’ll validate the two side of the distribution (bicaudal test).</p>
<p><span class="math inline">\(SEM = \frac{s}{\sqrt{n}}\)</span>.</p>
<p>$s = $ Sample standard deviation.</p>
<p>$n = $ Number of observations taken.</p>
<p>In the data set mentioned, we have one group being studied in different time and then being compared with a control group. The groups are identified by the field <strong>trt</strong> that will be equal 1 for treatment groups and 0 for control grups. Let’s develop a data.frame that will allow us to answer all three questions.</p>
<p>First I’ll group the data set and take the average income of each group in different columns. Then, I’ll calculate the critical t value from the student distribution and with this value I’ll create the standard error of the mean for each timeframe. With this values it will be possible to calculate the limits of the invervals.</p>
<pre class="r"><code>DAAG::nsw74demo %&gt;%
  mutate(group = ifelse(trt == 0, &quot;control&quot;,&quot;test&quot;)) %&gt;% 
  group_by(group) %&gt;% 
  summarise(n = n(),
            avg_income_74 = mean(re74),
            avg_income_75 = mean(re75),
            avg_income_78 = mean(re78),
            sem_74 = sd(re74)/sqrt(n),
            sem_75 = sd(re75)/sqrt(n),
            sem_78 = sd(re78)/sqrt(n)) %&gt;% 
  mutate(critical_t = qt(0.975, n),
         sem_74_times_critical_t = sem_74 * critical_t,
         sem_75_times_critical_t = sem_75 * critical_t,
         sem_78_times_critical_t = sem_78 * critical_t,
         inc74_lower = avg_income_74 - sem_74_times_critical_t,
         inc74_upper = avg_income_74 + sem_74_times_critical_t,
         inc75_lower = avg_income_75 - sem_74_times_critical_t,
         inc75_upper = avg_income_75 + sem_74_times_critical_t,
         inc78_lower = avg_income_78 - sem_74_times_critical_t,
         inc78_upper = avg_income_78 + sem_74_times_critical_t) -&gt; df</code></pre>
<pre class="r"><code>df %&gt;% 
  select(inc74_lower,
         inc74_upper)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   inc74_lower inc74_upper
##         &lt;dbl&gt;       &lt;dbl&gt;
## 1       1412.       2802.
## 2       1387.       2804.</code></pre>
<pre class="r"><code>df %&gt;% 
  select(inc75_lower,
         inc75_upper)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   inc75_lower inc75_upper
##         &lt;dbl&gt;       &lt;dbl&gt;
## 1        572.       1962.
## 2        823.       2241.</code></pre>
<pre class="r"><code>df %&gt;% 
  select(inc78_lower,
         inc78_upper)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   inc78_lower inc78_upper
##         &lt;dbl&gt;       &lt;dbl&gt;
## 1       3860.       5249.
## 2       5640.       7058.</code></pre>
<p>Now, the final question demand us to estimate the income difference between treated and control groups in 1978, with 95% of confidence. For this estimation, the book recommends the following equation:</p>
<p><span class="math inline">\(SED = \sqrt{SEM_1^2 + SEM_2^2}\)</span></p>
<p>Where:</p>
<p>SED = <em>standard error of the difference</em></p>
<p><span class="math inline">\(SEM_control\)</span> = <em>standard error of the mean</em></p>
<p><span class="math inline">\(SEM_test\)</span> = <em>standard error of the mean</em></p>
<p><span class="math inline">\(SED = \sqrt{415^2+680^2}\)</span></p>
<p><span class="math inline">\(SED = \sqrt{172.225+462.400}\)</span></p>
<p><span class="math inline">\(SED = 796,6\)</span></p>
<p><strong>A:</strong> The income difference between groups, with 95% of confidence is of 796,6.</p>
</div>
<div id="draw-graphs-that-show-for-degrees-of-freedom-between-1-and-100-the-change-in-the-5-critical-value-of-the-t-statistic.-compare-a-graph-on-which-neither-axis-is-transformed-with-a-graph-on-which-the-respective-axis-scales-are-proportional-to-logt-statistic-and-logdegrees-of-freedom.-which-graph-gives-the-more-useful-visual-indication-of-the-change-in-the-5-critical-value-of-the-t-statistic-with-increasing-degrees-of-freedom." class="section level3">
<h3><strong>2) Draw graphs that show, for degrees of freedom between 1 and 100, the change in the 5% critical value of the t-statistic. Compare a graph on which neither axis is transformed with a graph on which the respective axis scales are proportional to log(t-statistic) and log(degrees of freedom). Which graph gives the more useful visual indication of the change in the 5% critical value of the t-statistic with increasing degrees of freedom?.</strong></h3>
<pre class="r"><code>t_value &lt;- 
  data.frame(df = seq(1, 100, by = 1)) %&gt;% 
  mutate(t_value = qt(0.975, df = row_number()))

g1 &lt;- t_value %&gt;% 
  ggplot2::ggplot()+
  geom_line(mapping = aes(x = df, y = t_value))+
  scale_x_log10()+
  scale_y_log10()+
  theme_graph()+
  labs(title = &quot;Log transformation applied&quot;,
       x = &quot;log_10 of Degrees of freedom&quot;,
       y = &quot;log_10 of t-statistics&quot;)

g2 &lt;- t_value %&gt;% 
  ggplot2::ggplot()+
  geom_line(mapping = aes(x = df, y = t_value))+
  theme_graph()+
  labs(title = &quot;Log transformation not applied&quot;,
       x = &quot;Degrees of freedom&quot;,
       y = &quot;t-statistics&quot;)

g1 + g2</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="generate-a-random-sample-of-10-numbers-from-a-normal-distribution-with-mean-0-and-standard-deviation-2.-use-t.test-to-test-the-null-hypothesis-that-the-mean-is-0.-now-generate-a-random-sample-of-10-numbers-from-a-normal-distribution-with-mean-1.5-and-standard-deviation-2.-again-use-t.test-to-test-the-null-hypothesis-that-the-mean-is-0.-finally-write-a-function-that-generates-a-random-sample-of-n-numbers-from-a-normal-distribution-with-mean-and-standard-deviation-1-and-returns-the-p-value-for-the-test-that-the-mean-is-0.." class="section level3">
<h3><strong>3) Generate a random sample of 10 numbers from a normal distribution with mean 0 and standard deviation 2. Use t.test() to test the null hypothesis that the mean is 0. Now generate a random sample of 10 numbers from a normal distribution with mean 1.5 and standard deviation 2. Again use t.test() to test the null hypothesis that the mean is 0. Finally write a function that generates a random sample of n numbers from a normal distribution with mean and standard deviation 1, and returns the p-value for the test that the mean is 0..</strong></h3>
<p><strong>A:</strong> This question demands us to make a hypothesis test using a random data set. Let’s create this data set with using a normal distribution.</p>
<pre class="r"><code>df &lt;- rnorm(n = 10, mean = 0, sd = 2)

# Visualizing the data set
df</code></pre>
<pre><code>##  [1]  3.0300076  0.5495646 -2.1466435 -1.5087365  1.9965087 -1.3526119
##  [7] -0.5509253 -3.5739961  0.4305624  1.8354560</code></pre>
<p>Now, let’s make R calculate the t.test() and see the result and after this I’ll calculate the p-value by hand to reach the same result. I expect that the null hypothesis (<span class="math inline">\(H0: \mu = 0\)</span>) won’t be rejected, because the data set was generated from a population with mean 0.</p>
<pre class="r"><code>df %&gt;% 
  t.test(mu = 0, conf.level = 0.975)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  .
## t = -0.19698, df = 9, p-value = 0.8482
## alternative hypothesis: true mean is not equal to 0
## 97.5 percent confidence interval:
##  -1.888568  1.630405
## sample estimates:
##  mean of x 
## -0.1290814</code></pre>
<p>The t.test from R generated a p-value of 0.7261. In this case we can’t reject H0 as expected. Now let’s make the calculation step by step.</p>
<pre class="r"><code>mean &lt;- mean(df)
sd &lt;- sd(df)
n_sqrt &lt;- sqrt(length(df))

sem &lt;- sd/n_sqrt

(1-pt(mean/sem, 10-1))*2</code></pre>
<pre><code>## [1] 1.151781</code></pre>
<p>The same p-value was obtained. Thus, we can rely on our t.test() from R and understand what it is happening on backstage.</p>
<p>Now, the question ask us to perform the same method in a population with mean 1.5. In this case, let’s make the sample generation and statistical modeling all at once.</p>
<pre class="r"><code>data.frame(rnorm(n = 10, mean = 1.5, sd = 2)) %&gt;% 
  t.test(mu = 0, conf.level = 0.975)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  .
## t = 3.2093, df = 9, p-value = 0.01067
## alternative hypothesis: true mean is not equal to 0
## 97.5 percent confidence interval:
##  0.3703831 4.1640321
## sample estimates:
## mean of x 
##  2.267208</code></pre>
<p>In this test, the p-value obtained was lower, but not sufficient to make us reject H0: <span class="math inline">\(\mu = 0\)</span>. Maybe it’s because the sample size is small. Let’s remake the test with a larger sample.</p>
<pre class="r"><code>data.frame(rnorm(n = 100, mean = 1.5, sd = 2)) %&gt;% 
  t.test(mu = 0, conf.level = 0.975)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  .
## t = 6.9181, df = 99, p-value = 4.574e-10
## alternative hypothesis: true mean is not equal to 0
## 97.5 percent confidence interval:
##  0.9456001 1.8728564
## sample estimates:
## mean of x 
##  1.409228</code></pre>
<p>Now, the p-value is pretty lower and we can reject H0 with a 95% of confidence.</p>
</div>
<div id="use-the-function-that-was-created-in-exercise-3-to-generate-50-independent-p-values-all-with-a-sample-size-n-10-and-with-mean-0.-use-qqplot-with-the-parameter-setting-x-qunifppoints50-to-compare-the-distribution-of-the-p-values-with-that-of-a-uniform-random-variable-on-the-interval-0-1.-comment-on-the-plot." class="section level3">
<h3><strong>4) Use the function that was created in Exercise 3 to generate 50 independent p-values, all with a sample size n = 10 and with mean = 0. Use qqplot(), with the parameter setting x = qunif(ppoints(50)), to compare the distribution of the p-values with that of a uniform random variable, on the interval [0, 1]. Comment on the plot.</strong></h3>
<p><strong>A:</strong> In the last exercise I didn’t create the function, then I’ll create it now. The function will generates a random sample of <em>n</em> numbers from a normal distribution with mean <span class="math inline">\(\mu = 0\)</span> and standard deviation 1, and will return the <strong>p-value</strong> from a test considering <strong>H0: <span class="math inline">\(\mu = 0\)</span></strong>.</p>
<p>The first thing we have to make in order to build a function, is to create an empty vector that will receive the output. Then, we create the function and fill this vector with the output from the function. Let’s see how it works.</p>
<pre class="r"><code># Creating the empty vector
output &lt;- rep(NA,50)

# Creating the function with the p-value
for(i in 1:50){
  sample &lt;- rnorm(n = 10, mean = 0, sd = 1)
  mean &lt;- mean(sample)
  sd &lt;- sd(sample)
  sem &lt;- sd/sqrt(10)
  output[i] &lt;- data.frame(p_value = (1-pt((abs(mean)/sem),9))*2)
}

# Putting function output in a data.frame
df &lt;- data.frame(a = matrix(unlist(output), 
                            nrow = 50, 
                            byrow = T))</code></pre>
<p>Now we have the data.frame and we’re ready to plot the requested qq-plot. This plot is nice to visualize how normal is the data distribution. For this case, we have a sample obtained from a population with mean 0 and sd 1 and we’re testing the Null Hypothesis that population mean is 0. Then, the p-value has to by high (more than 0.05 at least) in order to preclude the rejection of H0. The qq-plot will show that p-value is normally distributed. I’ll also generate a histogram to show that p-value is high in most of the cases (the data.frame contains 50 p-values). Let’s see both graphs</p>
<pre class="r"><code># Generating the qq-plot
g1 &lt;- df %&gt;% 
  ggplot2::ggplot()+
  geom_qq(mapping = aes(sample = a),
          color = &quot;dark blue&quot;)+
  geom_qq_line(mapping = aes(sample = a),
               lty = 2,
               color = &quot;dark orange&quot;)+
  theme_graph()+
  labs(title = &quot;QQ plot for p-values&quot;)

# Generating the histogram
g2 &lt;- df %&gt;% 
  ggplot2::ggplot()+
  geom_histogram(mapping = aes(x = a),
                 fill = &quot;dark blue&quot;,
                 alpha = .3)+
  geom_vline(xintercept = 0.05,
             lty = 2,
             color = &quot;dark orange&quot;)+
  scale_x_continuous(labels = scales::percent)+
  theme_graph()+
  labs(title = &quot;Histogram of p-values&quot;)

g1+g2</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Look that most of the p-values generated are on the righ side of the histogram’s yellow dached line. This indicates that most of the Null Hypothesis wouldn’t be rejected considering a significance level of 5%.</p>
</div>
<div id="here-we-generate-random-normal-numbers-with-a-sequential-dependence-structure.-repeat-this-several-times.-there-should-be-no-consistent-pattern-in-the-acf-plot-for-different-random-samples-y1.-there-will-be-a-fairly-consistent-pattern-in-the-acf-plot-for-y-a-result-of-the-correlation-that-is-introduced-by-adding-to-each-value-the-next-value-in-the-sequence." class="section level3">
<h3><strong>6) Here we generate random normal numbers with a sequential dependence structure. Repeat this several times. There should be no consistent pattern in the ACF plot for different random samples y1. There will be a fairly consistent pattern in the ACF plot for y, a result of the correlation that is introduced by adding to each value the next value in the sequence.</strong></h3>
<pre class="r"><code># Random numbers mentioned in the exercise
y1 &lt;- rnorm(51)
y &lt;- y1[-1] + y1[-51]
acf(y1) # acf is ‘autocorrelation function’</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre class="r"><code>acf(y)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-25-2.png" width="672" /></p>
</div>
<div id="a-for-each-column-of-the-data-set-nsw74psid1-after-the-first-compare-the-control-group-trt0-with-the-treatment-group-trt1.-use-overlaid-density-plots-to-compare-the-continuous-variables-and-two-way-tables-to-compare-the-binary-01-variables.-where-are-the-greatest-differences." class="section level3">
<h3><strong>8) (a) For each column of the data set nsw74psid1 after the first, compare the control group (trt==0) with the treatment group (trt==1). Use overlaid density plots to compare the continuous variables, and two-way tables to compare the binary (0/1) variables. Where are the greatest differences?.</strong></h3>
<p><strong>A:</strong> In this exercise I’ll generate box plots for continuous variables in order to compare the control and treated groups.</p>
<pre class="r"><code>g1 &lt;- DAAG::nsw74psid1 %&gt;%
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = age, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme(axis.text.x = element_blank())+
  theme_graph()

g2 &lt;- DAAG::nsw74psid1 %&gt;% 
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = educ, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  theme(axis.text.x = element_blank())

g3 &lt;- DAAG::nsw74psid1 %&gt;% 
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = re74, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_y_log10(labels = scales::comma)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  theme(axis.text.x = element_blank())

g4 &lt;- DAAG::nsw74psid1 %&gt;% 
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = re75, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_y_log10(labels = scales::comma)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  theme(axis.text.x = element_blank())

g5 &lt;- DAAG::nsw74psid1 %&gt;% 
  mutate(trt = as.factor(trt)) %&gt;% 
  ggplot2::ggplot()+
  geom_boxplot(mapping = aes(y = re78, group = trt, color = trt),
               outlier.shape = 20,
               outlier.color = &quot;black&quot;)+
  scale_y_log10(labels = scales::comma)+
  scale_color_brewer(type = &quot;qual&quot;)+
  theme_graph()+
  theme(axis.text.x = element_blank())

# Putting all graphs together
plots &lt;- (g1+g2)/(g3|g4|g5)

plots +
  plot_layout(guides = &quot;collect&quot;)+
  plot_annotation(title = &quot;Continuous variables comparison&quot;,
                  subtitle = &quot;Control (0) and treated (1) groups&quot;)</code></pre>
<p><img src="/books/2020-05-14-data-analysis-and-graphs-with-r-john-maindonald-and-john-braun_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
</div>
