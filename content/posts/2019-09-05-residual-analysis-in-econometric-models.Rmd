---
title: "Residual analysis in econometric models"
date: 2019-09-05
output:
  html_document:
    fig_width: 8.5
    fig_height: 4
---

The linear regression model is highly used on the prediction of continuous variables, where is one or more independent variables and one dependent one (the one that it seeks to test an hipothesis about its behavior). This is a pretty simple model to by executed and one of the firsts to be taught in econometric classes. Although its simplicity, in order to reach reliable results, some assumptions are needed, like the following:

i) Absence of multicolineatiry between independent variables

ii) Absence of autocorrelation on the dependent variable

iii) Absence of pattern on the behavior of the model residuals, in other words, absence of heteroscedasticity

iv) Normal distribution of residuals

Having this assumptions satisfied, the model can be executed and the generated conclusions will be reliable to allow decision making. In this post, I'll analyse the behavior of the residuals from a linear regression model in order to validate the assumptions (iii) and (iv). For this, I'll use a data set that presents the price of suggarcane as independent variable and the planted area of this product, that will be the dependent variable.

The goal of this example is to analyse, through a simple linear regression model, the suggarcane supply elasticity as a function of the suggarcane price. The hipothesis is that there's elasticity on the supply, thus the planted area increases in responde to a price increase. But, in order to validade this existence, it's necessary to check, using a regression model, that these two variables have correlation. Then, the assumptions (iii) and (iv) will support this conclusion.

For this example, I'll utilize the following R packages:

```{r,eval=T,include=T,echo=T,warning=F,message=F}
library(tidyverse)
library(lmtest)
library(corrplot)
library(readxl)
library(gridExtra)
library(ggthemes)

theme_graph <- function(){
  theme(
    plot.title = element_text(size = 16),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(face = "italic", size = 9),
    axis.text = element_text(size = 9),
    axis.title = element_text(face = "italic", size = 9),
    text = element_text(family = "Times New Roman"),
    strip.background = element_rect(fill = "grey"),
    strip.text = element_text(face = "bold"),
    legend.title = element_blank(),
    legend.position = "bottom"
  )
}
  
graph_color <- "#006666"
```

```{r,eval=T,include=T,echo=T,warning=F,message=F}
dados <- readxl::read_excel("C:/Users/francisco.piccolo/Desktop/R/franciscopiccolo.github.io/datasets/Econometria_exercicios/autocorrelacao_cana_de_acucar.xlsx")

# Chaging the name of the field to remove blanck spaces
dados %>% 
  transmute(
    periodo = Período,
    area = Área,
    valor = `Preço da Cana de Açúcar`*100,
    log_area = log(area),
    log_valor = log(valor),
    delta_area = round((area/lag(area,1))-1,digits = 2),
    delta_log_area = round(log_area-lag(log_area,1),digits = 2),
    delta_valor = round((valor/lag(valor,1))-1,digits=2),
    delta_log_valor = round(log_valor-lag(log_valor),digits = 2),
    log_area_precicion = delta_area - delta_log_area,
    log_valor_precision = delta_valor - delta_log_valor
  ) -> dados_2
```
The data set of this example has 34 rows, with planted area and suggarcane price data. Both fields with log transformation (natural) included. Folliwing there's a sample of the data set:

```{r,eval=T,include=T,echo=T,warning=F,message=F}
dados_2[sample(nrow(dados_2),10), ] %>%
	as.tibble()
```

The linear regression model proposed for this problem is defined by the following equation:

$$ lnY_t = \beta_0+\beta_1 (lnX_t) + U_t $$

Where:

$Y_t$ = planted area on year t, natural log transformed

$X_t$ = suggarcane price on year t, natural log transformed

$\beta_0$ = Intercept

$\beta_1$ = slope

$U_t$ = Model residuals

The natural log is used to in order to have variations between periods interpreted as percentual variation. This assumption is made possible only for natural log transformations and this adjustment is needed in problems invonving elasticity, because elasticity is interpreted as a percentual variation in one variable given a percentual variation in another variable.

Following is an example of this property of natural log transformations. You can see that the total variation can be seen as percentual variation after the natural log transformation. For more explanations on this topic you can see [article 1](https://people.duke.edu/~rnau/411log.htm) [article 2](https://dev.to/rokaandy/logarithmic-transformation-in-linear-regression-models-why-when-3a7c).

```{r,eval=T,include=T,echo=T,warning=F,message=F}
data.frame(
    "Valor" = c(100,105,112,120),
    "Crescimento" = c("-","5.00%","6.67%","7.14%"),
    "Log natural" = c(log(100),log(105),log(112),log(120)),
    "Delta" = c("-",
                  paste(round(log(105)-log(100),4)*100,"%",sep = ""),
                  paste(round(log(112)-log(105),4)*100,"%",sep = ""),
                  paste(round(log(120)-log(112),4)*100,"%",sep = ""))) %>%
    as.tibble()
```
Veja como o crescimento percentual dos valores (5.00%, 6.67% e 7.14%) ficam bem próximos do respectivo delta, que é a diferença entre o log natural do número com relação ao log natural do número anterior (e.g. 4.654 - 4.605 = 0.0488). Desta forma, se a equação estiver em logarítmo natural em ambos os lados, pode-se interpretar que a variação percentual na variável explicativa irá gerar uma variação percentual de $\beta_1$ na variável dependente.

It becomes clear that the percentual values of 5.00%, 6.67% and 7.14% are pretty close to the value of the delta field, that represents the differente between the natural log of the number and the natural log of the predecessor value (e.g. 4.654 - 4.605 = 0.0488). 

Ao tentar criar esta mesma tabela com outros logarítmos (e.g. base 2 ou base 10), não se alcança os mesmos resultados. Portanto, esta interpretação e transformação só faz sentido com logaritmo natural. When we try to create this same view with others log transformations (e.g. base 2 or 10), we can't reach the same results. Therefore, this interpretation and transformation make sense only for natural log.

```{r,eval=T,include=T,echo=T,warning=F,message=F}
data.frame(
    "Valor" = c(100,105,112,120),
    "Crescimento" = c("-","5.00%","6.67%","7.14%"),
    "Log natural" = c(log(100,2),log(105,2),log(112,2),log(120,2)),
    "Delta" = c("-",
                  paste(round(log(105,2)-log(100,2),4)*100,"%",sep = ""),
                  paste(round(log(112,2)-log(105,2),4)*100,"%",sep = ""),
                  paste(round(log(120,2)-log(112,2),4)*100,"%",sep = ""))) %>% 
    as.tibble()
```

```{r,eval=T,include=T,echo=T,warning=F,message=F}
data.frame(
    "Valor" = c(100,105,112,120),
    "Crescimento" = c("-","5.00%","6.67%","7.14%"),
    "Log natural" = c(log(100,10),log(105,10),log(112,10),log(120,10)),
    "Delta" = c("-",
                  paste(round(log(105,10)-log(100,10),4)*100,"%",sep = ""),
                  paste(round(log(112,10)-log(105,10),4)*100,"%",sep = ""),
                  paste(round(log(120,10)-log(112,10),4)*100,"%",sep = ""))) %>% 
	as.tibble()
```

Now that we understand this property of natural log, it's possible to validate this property on the data set that will be used on the regression model proposed, for the suggarcane price and suggarcane area planted. The delta_area and delta_value fields presents the percentual variation of one period to the preceding. At delta_log_value and delta_log_area fields is calculated the difference between one period with the preceding and the fields log_area_precision and log_value_precision calculate the difference between delta_area with delta_log_area and delta_value with delta_log_value. The lower these two values, stronger is the property of the natural log transformation explained before. The following table will show the results:

```{r,eval=T,include=T,echo=T,warning=F,message=F}
dados_2
```
After this explanation, we can continue with the regression model development. The variables will always be presented with the natural log transformation applied. The following graph shows the behavior of the studied variables and the regression line.

```{r,eval=T,include=T,echo=T,warning=F,message=F}
dados_2 %>% 
  ggplot(aes(x=log_area,y=log_valor))+
  geom_point()+
  geom_smooth(method = "lm",se = F, lty = 2, color = "dark orange")+
  theme_graph()+
  labs(title = "Area planted and suggarcane price relation",
  	   subtitle= "There's an elasticity ",
  	   x = "Planted area (ln)",
  	   y = "Market price (ln)")
```

The variable shows linear correlation, meaning that there's supply elasticity. Following is the summary of the regression model:

```{r,eval=T,include=T,echo=T,warning=F,message=F}
model <- lm(log_area~log_valor,data = dados_2)
```

```{r,eval=T,include=T,echo=T,warning=F,message=F}
summary(model)
```

The regression equation generated by the model is defined as: $lnY_t = 1.64 + 0.97 (lnX_t) + U_t$

This equation indicates that the planted area of suggarcane would be of $e^{1.64} = 5.15$ (ha) if the price was 0. As the price increase in 1%, the impact on the planted area will be of 0.97%.

The r² reach 70.6%, but this isn't enough to make conclusions about the model. Its necessary to analyse the model premissed about the residuals to ensure that the model is reliable.

```{r,eval=T,include=T,echo=T,warning=F,message=F}
residual <- residuals(model)
```

```{r,eval=T,include=T,echo=T,warning=F,message=F}
df <- cbind(dados_2, residual)
```

The next graph shows the residuals distribution in order to validate if they're normally distributed. Both histogram and quantil-quantil plot are good choices to validate this assumption.

```{r,eval=T,include=T,echo=T,warning=F,message=F}
df %>% 
  ggplot(aes(x=residual))+
  geom_histogram(binwidth = .05,alpha=.4,fill="dark orange")+
  theme_graph()+
  labs(title = "Histograma dos resíduos",
  	   subtitle = "Aparente normalidade dos dados",
  	   x = "",
  	   y = "")
```

```{r,eval=T,include=T,echo=T,warning=F,message=F}
df %>% 
  ggplot(aes(sample=residual))+
  geom_qq()+
  theme_graph()+
  labs(title = "QQ-Plot dos resíduos",
       subtitle = "Corrobora a suposta normalidade dos dados",
       x = "",
       y = "")
```

Despite the graph visualization being a good bet to analyse departure from normality, it's also important to perform formal tests like Durbin-Watson, Breusch-Godfrey and Ljung-Box Qtest. The Durbin-Watson (DW) will be the first performed:

```{r,eval=T,include=T,echo=T,warning=F,message=F}
lmtest::dwtest(model)
```

The DW value was of 1.2912 and this value will be used to generate conclusions for the test. As well as the DW value, its necessary to have the DL and DU intervals that will be compared with the DW value. The DL and DU values can be found at this [table](http://www.portalaction.com.br/analise-de-regressao/33-diagnostico-de-independencia). To find DL and DU values at this table, take the number of points of your data set (n = 33), the significance level (i.e. 0.05) and the degrees of freedom (i.e. 1), then we have:

**DL** = 1.35

**DU** = 1.49

With a DW of 1.2912, above 0 and under the DL value, we can conclude that the residuals are independent. Thus, we can ensure that residuals are both normally distributed and independent, then the model is reliable on what it shows (there's elasticity on the suggarcane supply given a price variation).
